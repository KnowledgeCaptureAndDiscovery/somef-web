{"description": [{"excerpt": "An Open Source Machine Learning Framework for Everyone", "confidence": [1.0]}], "citation": [{"excerpt": "Raspberry Pi 0 and 1 |   | Py2 Py3 \nRaspberry Pi 2 and 3 |   | Py2 Py3 \n", "confidence": [0.8131107928270905, 0.8131107928270905]}], "installation": [[{"excerpt": "See the [TensorFlow install guide](https://www.tensorflow.org/install) for the\n[pip package](https://www.tensorflow.org/install/pip), to\n[enable GPU support](https://www.tensorflow.org/install/gpu), use a\n[Docker container](https://www.tensorflow.org/install/docker), and\n[build from source](https://www.tensorflow.org/install/source).\n\nTo install the current release, which includes support for\n[CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and\nWindows)*:\n\n```\n$ pip install tensorflow\n```\n\nA smaller CPU-only package is also available:\n\n```\n$ pip install tensorflow-cpu\n```\n\nTo update TensorFlow to the latest version, add `--upgrade` flag to the above\ncommands.\n\n*Nightly binaries are available for testing using the\n[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPi.*\n\n", "confidence": [1]}], {"excerpt": "$ python \n", "confidence": [0.8416634629897486]}, {"excerpt": "Linux CPU            |                                                                                                                                                                         | PyPI \nLinux GPU            |                                                                                                                                                               | PyPI \nLinux XLA            |                                                                                                                                                                       | TBA \n", "confidence": [0.8498125932570172, 0.9050461549372528, 0.8060695731063168]}, {"excerpt": "Windows CPU          |                                                                                                                                                                     | PyPI \nWindows GPU          |                                                                                                                                                                     | PyPI \n", "confidence": [0.8333339278375657, 0.8934333724063521]}, {"excerpt": "Red Hat\u00ae Enterprise Linux\u00ae 7.6 CPU & GPU <br> Python 2.7, 3.6 |  | 1.13.1 PyPI \n", "confidence": [0.8434773201812285]}], "invocation": [{"excerpt": "import tensorflow as tf \n", "confidence": [0.8296426571746836]}], "contribution": [[{"excerpt": "**If you want to contribute to TensorFlow, be sure to review the\n[contribution guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.**\n\n**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, please see\n[TensorFlow Discuss](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)\nfor general questions and discussion, and please direct specific questions to\n[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**\n\nThe TensorFlow project strives to abide by generally accepted best practices in\nopen-source software development:\n\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)\n\n", "confidence": [1]}]], "license": {"excerpt": {"name": "Apache License 2.0", "url": "https://api.github.com/licenses/apache-2.0"}, "confidence": [1.0]}, "name": {"excerpt": "tensorflow", "confidence": [1.0]}, "owner": {"excerpt": "tensorflow", "confidence": [1.0]}, "forks_url": {"excerpt": "https://api.github.com/repos/tensorflow/tensorflow/forks", "confidence": [1.0]}, "topics": {"excerpt": ["tensorflow", "machine-learning", "python", "deep-learning", "deep-neural-networks", "neural-network", "ml", "distributed"], "confidence": [1.0]}, "languages": {"excerpt": ["C++", "Python", "HTML", "Starlark", "MLIR", "Go", "Java", "C", "Shell", "Jupyter Notebook", "Objective-C++", "Objective-C", "Dockerfile", "Makefile", "Swift", "Batchfile", "Smarty", "Assembly", "Pawn", "PHP", "C#", "Perl", "Ruby", "CMake", "LLVM", "Roff", "RobotFramework", "SWIG", "Pascal", "Vim Snippet"], "confidence": [1.0]}, "readme_url": {"excerpt": "https://github.com/tensorflow/tensorflow/blob/master/README.md", "confidence": [1.0]}, "releases": {"excerpt": [{"tag_name": "v2.2.0-rc2", "name": "TensorFlow Release 2.2.0-rc2", "author_name": "goldiegadde", "body": "# Release 2.2.0\r\n\r\n## Major Features and Improvements\r\n\r\n* Replaced the scalar type for string tensors from `std::string` to `tensorflow::tstring` which is now ABI stable.\r\n* A new Profiler for TF 2 for CPU/GPU/TPU. It offers both device and host performance analysis, including input pipeline and TF Ops. Optimization advisory is provided whenever possible. Please see [this tutorial](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) and [guide](https://www.tensorflow.org/guide/profiler) for usage guidelines.\r\n* Export C++ functions to Python using `pybind11` as opposed to `SWIG` as a part of our [deprecation of swig efforts](https://github.com/tensorflow/community/blob/master/rfcs/20190208-pybind11.md).\r\n* `tf.distribute`:\r\n  * Support added for global sync `BatchNormalization` by using the newly added `tf.keras.layers.experimental.SyncBatchNormalization` layer. This layer will sync `BatchNormalization` statistics every step across all replicas taking part in sync training.\r\n  * Performance improvements for GPU multi-worker distributed training using `tf.distribute.experimental.MultiWorkerMirroredStrategy`\r\n    * Update NVIDIA `NCCL` to `2.5.7-1` for better performance and performance tuning. Please see [nccl developer guide](https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html) for more information on this.\r\n    * Support gradient `allreduce` in `float16`. See this [example](https://github.com/tensorflow/models/blob/master/official/staging/training/grad_utils.py) usage.\r\n    * Experimental support of [all reduce gradient packing](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CollectiveHints) to allow overlapping gradient aggregation with backward path computation. \r\n    * Deprecated `experimental_run_v2` method for distribution strategies and renamed the method `run` as it is no longer experimental.\r\n* `tf.keras`:\r\n  * `Model.fit` major improvements:\r\n     * You can now use custom training logic with `Model.fit` by overriding `Model.train_step`.\r\n     * Easily write state-of-the-art training loops without worrying about all of the features `Model.fit` handles for you (distribution strategies, callbacks, data formats, looping logic, etc)\r\n     * See the default [`Model.train_step`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L433) for an example of what this function should look like\r\n     * Same applies for validation and inference via `Model.test_step` and `Model.predict_step`\r\n  * The SavedModel format now supports all Keras built-in layers (including metrics, preprocessing layers, and stateful RNN layers)\r\n* `tf.lite`:\r\n  * Enable TFLite experimental new converter by default.\r\n* XLA\r\n  * XLA now builds and works on windows. All prebuilt packages come with XLA available.\r\n  * XLA can be [enabled for a `tf.function`](https://www.tensorflow.org/xla#explicit_compilation_with_tffunction\r\n) with \u201ccompile or throw exception\u201d semantics on CPU and GPU.\r\n\r\n## Breaking Changes\r\n* `tf.keras`: \r\n  * In `tf.keras.applications` the name of the \"top\" layer has been standardized to \"predictions\". This is only a problem if your code relies on the exact name of the layer.\r\n  * Huber loss function has been updated to be consistent with other Keras losses. It now computes mean over the last axis of per-sample losses before applying the reduction function.\r\n* AutoGraph no longer converts functions passed to `tf.py_function`, `tf.py_func` and `tf.numpy_function`.\r\n* Deprecating `XLA_CPU` and `XLA_GPU` devices with this release.\r\n* Increasing the minimum bazel version to build TF to 2.0.0 to use Bazel's `cc_experimental_shared_library`.\r\n\r\n## Known Caveats\r\n* Due to certain unforeseen circumstances, we are unable to release MacOS py3.8 binaries, but Windows/Linux binaries for py3.8 are available.\r\n* The current TensorFlow release now **requires** gast version 0.3.3. \r\n\r\n## Bug Fixes and Other Changes\r\n* `tf.data`:\r\n  * Removed `autotune_algorithm` from experimental optimization options.\r\n* TF Core:\r\n  * `tf.constant` always creates CPU tensors irrespective of the current device context.  \r\n  * Eager `TensorHandles` maintain a list of mirrors for any copies to local or remote devices. This avoids any redundant copies due to op execution.\r\n  * For `tf.Tensor` & `tf.Variable`, `.experimental_ref()` is no longer experimental and is available as simply `.ref()`.\r\n  * `pfor/vectorized_map`: Added support for vectorizing 56 more ops. Vectorizing `tf.cond` is also supported now.\r\n  * Set as much partial shape as we can infer statically within the gradient impl of the gather op.\r\n  * Gradient of `tf.while_loop` emits `StatelessWhile` op if `cond` and body functions are stateless. This allows multiple gradients while ops to run in parallel under distribution strategy.\r\n  * Speed up `GradientTape` in eager mode by auto-generating list of op inputs/outputs which are unused and hence not cached for gradient functions.\r\n  * Support `back_prop=False` in `while_v2` but mark it as deprecated.\r\n  * Improve error message when attempting to use `None` in data-dependent control flow.\r\n  * Add `RaggedTensor.numpy()`.\r\n  * Update `RaggedTensor.__getitem__` to preserve uniform dimensions & allow indexing into uniform dimensions.\r\n  * Update `tf.expand_dims` to always insert the new dimension as a non-ragged dimension.\r\n  * Update `tf.embedding_lookup` to use `partition_strategy` and `max_norm` when `ids` is ragged.\r\n  * Allow `batch_dims==rank(indices)` in `tf.gather`.\r\n  * Add support for bfloat16 in `tf.print`.  \r\n* `tf.distribute`: \r\n  * Support `embedding_column` with variable-length input features for `MultiWorkerMirroredStrategy`.\r\n* `tf.keras`:\r\n  * Added `all_reduce_sum_gradients` argument to `tf.keras.optimizer.Optimizer.apply_gradients`. This allows custom gradient aggregation and processing aggregated gradients in custom training loop.\r\n  * Allow `pathlib.Path` paths for loading models via Keras API.\r\n* `tf.function`/AutoGraph:\r\n  * AutoGraph is now available in `ReplicaContext.merge_call`, `Strategy.extended.update` and `Strategy.extended.update_non_slot`.\r\n  * Experimental support for shape invariants has been enabled in `tf.function`. See the API docs for `tf.autograph.experimental.set_loop_options` for additonal info.\r\n  * AutoGraph error messages now exclude frames corresponding to APIs internal to AutoGraph.\r\n  * Improve shape inference for `tf.function` input arguments to unlock more Grappler optimizations in TensorFlow 2.x.\r\n  * Improve automatic control dependency management of resources by allowing resource reads to occur in parallel and synchronizing only on writes.\r\n  * Fix execution order of multiple stateful calls to `experimental_run_v2` in `tf.function`.\r\n  * You can now iterate over `RaggedTensors` using a for loop inside `tf.function`.\r\n* `tf.lite`:\r\n  *  Migrated the `tf.lite` C inference API out of experimental into lite/c.\r\n  * Add an option to disallow `NNAPI` CPU / partial acceleration on Android 10\r\n  * TFLite Android AARs now include the C headers and APIs are required to use TFLite from native code.\r\n  * Refactors the delegate and delegate kernel sources to allow usage in the linter.\r\n  * Limit delegated ops to actually supported ones if a device name is specified or `NNAPI` CPU Fallback is disabled.\r\n  * TFLite now supports `tf.math.reciprocal1` op by lowering to `tf.div op`.\r\n  * TFLite's unpack op now supports boolean tensor inputs.\r\n  * Microcontroller and embedded code moved from experimental to main TensorFlow Lite folder\r\n  * Check for large TFLite tensors.\r\n  * Fix GPU delegate crash with C++17.\r\n  * Add 5D support to TFLite `strided_slice`.\r\n  * Fix error in delegation of `DEPTH_TO_SPACE` to `NNAPI` causing op not to be accelerated.\r\n  * Fix segmentation fault when running a model with LSTM nodes using `NNAPI` Delegate\r\n  * Fix `NNAPI` delegate failure when an operand for Maximum/Minimum operation is a scalar.\r\n  * Fix `NNAPI` delegate failure when Axis input for reduce operation is a scalar.\r\n  * Expose option to limit the number of partitions that will be delegated to `NNAPI`.\r\n  * If a target accelerator is specified, use its feature level to determine operations to delegate instead of SDK version.\r\n* `tf.random`:\r\n  * Various random number generation improvements: \r\n    * Add a fast path for default `random_uniform`\r\n    * `random_seed` documentation improvement.\r\n    * `RandomBinomial` broadcasts and appends the sample shape to the left rather than the right. \r\n  * Added `tf.random.stateless_binomial`, `tf.random.stateless_gamma`, `tf.random.stateless_poisson`\r\n  * `tf.random.stateless_uniform` now supports unbounded sampling of `int` types.\r\n* Math and Linear Algebra:\r\n  * Add `tf.linalg.LinearOperatorTridiag`.\r\n  * Add `LinearOperatorBlockLowerTriangular`\r\n  * Add broadcasting support to tf.linalg.triangular_solve[#26204](https://github.com/tensorflow/tensorflow/issues/26204), tf.math.invert_permutation.\r\n  * Add `tf.math.sobol_sample` op.\r\n  * Add `tf.math.xlog1py`.\r\n  * Add `tf.math.special.{dawsn,expi,fresnel_cos,fresnel_sin,spence}`.\r\n  * Add a Modified Discrete Cosine Transform (MDCT) and its inverse to `tf.signal`.\r\n* TPU Enhancements:\r\n  * Refactor `TpuClusterResolver` to move shared logic to a separate pip package.\r\n  * Support configuring TPU software version from cloud tpu client.\r\n  * Allowed TPU embedding weight decay factor to be multiplied by learning rate.\r\n* XLA Support:\r\n  * Add standalone XLA AOT runtime target + relevant .cc sources to pip package.\r\n  * Add check for memory alignment to MemoryAllocation::MemoryAllocation() on 32-bit ARM. This ensures a deterministic early exit instead of a hard to debug bus error later.\r\n  * `saved_model_cli aot_compile_cpu` allows you to compile saved models to XLA header+object files and include them in your C++ programs.\r\n  * Enable `Igamma`, `Igammac` for XLA.\r\n  * XLA reduction emitter is deterministic when the environment variable `TF_DETERMINISTIC_OPS` is set.\r\n* Tracing and Debugging:\r\n  * Add source, destination name to `_send` traceme to allow easier debugging.\r\n  * Add traceme event to `fastpathexecute`.\r\n* Other:\r\n  * Fix an issue with AUC.reset_states for multi-label AUC [#35852](https://github.com/tensorflow/tensorflow/issues/35852)\r\n  * Fix the TF upgrade script to not delete files when there is a parsing error and the output mode is `in-place`.\r\n  * Move `tensorflow/core:framework/*_pyclif` rules to `tensorflow/core/framework:*_pyclif`.\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n372046933, 8bitmp3, aaronhma, Abin Shahab, Aditya Patwardhan, Agoniii, Ahti Kitsik, Alan Yee, Albin Joy, Alex Hoffman, Alexander Grund, Alexandre E. Eichenberger, Amit Kumar Jaiswal, amoitra, Andrew Anderson, Angus-Luo, Anthony Barbier, Anton Kachatkou, Anuj Rawat, archis, Arpan-Dhatt, Arvind Sundararajan, Ashutosh Hathidara, autoih, Bairen Yi, Balint Cristian, Bas Aarts, BashirSbaiti, Basit Ayantunde, Ben Barsdell, Benjamin Gaillard, boron, Brett Koonce, Bryan Cutler, Christian Goll, Christian Sachs, Clayne Robison, comet, Daniel Falbel, Daria Zhuravleva, darsh8200, David Truby, Dayananda-V, deepakm, Denis Khalikov, Devansh Singh, Dheeraj R Reddy, Diederik Van Liere, Diego Caballero, Dominic Jack, dothinking, Douman, Drake Gens, Duncan Riach, Ehsan Toosi, ekuznetsov139, Elena Zhelezina, elzino, Ending2015a, Eric Schweitz, Erik Zettel, Ethan Saadia, Eugene Kuznetsov, Evgeniy Zheltonozhskiy, Ewout Ter Hoeven, exfalso, FAIJUL, Fangjun Kuang, Fei Hu, Frank Laub, Frederic Bastien, Fredrik Knutsson, frreiss, Fr\u00e9d\u00e9ric Rechtenstein, fsx950223, Gaurav Singh, gbaned, George Grzegorz Pawelczak, George Sterpu, Gian Marco Iodice, Giorgio Arena, Hans Gaiser, Hans Pabst, Haoyu Wu, Harry Slatyer, hsahovic, Hugo, Hugo Sj\u00f6berg, IrinaM21, jacco, Jake Tae, Jean-Denis Lesage, Jean-Michel Gorius, Jeff Daily, Jens Elofsson, Jerry Shih, jerryyin, Jin Mingjian, Jinjing Zhou, JKIsaacLee, jojimonv, Jonathan Dekhtiar, Jose Ignacio Gomez, Joseph-Rance, Judd, Julian Gross, Kaixi Hou, Kaustubh Maske Patil, Keunwoo Choi, Kevin Hanselman, Khor Chean Wei, Kilaru Yasaswi Sri Chandra Gandhi, Koan-Sin Tan, Koki Ibukuro, Kristian Holsheimer, kurileo, Lakshay Tokas, Lee Netherton, leike666666, Leslie-Fang-Intel, Li, Guizi, LIUJIAN435, Lukas Geiger, Lyo Nguyen, madisetti, Maher Jendoubi, Mahmoud Abuzaina, Manuel Freiberger, Marcel Koester, Marco Jacopo Ferrarotti, Markus Franke, marload, Mbah-Javis, mbhuiyan, Meng Zhang, Michael Liao, MichaelKonobeev, Michal Tarnowski, Milan Straka, minoring, Mohamed Nour Abouelseoud, MoussaMM, Mrinal Jain, mrTsjolder, M\u00e5ns Nilsson, Namrata Bhave, Nicholas Gao, Niels Ole Salscheider, nikochiko, Niranjan Hasabnis, Nishidha Panpaliya, nmostafa, Noah Trenaman, nuka137, Officium, Owen L - Sfe, Pallavi G, Paul Andrey, Peng Sun, Peng Wu, Phil Pearl, PhilipMay, pingsutw, Pooya Davoodi, PragmaTwice, pshiko, Qwerty71, R Gomathi, Rahul Huilgol, Richard Xiao, Rick Wierenga, Roberto Rosmaninho, ruchit2801, Rushabh Vasani, Sami, Sana Damani, Sarvesh Dubey, Sasan Jafarnejad, Sergii Khomenko, Shane Smiskol, Shaochen Shi, sharkdtu, Shawn Presser, ShengYang1, Shreyash Patodia, Shyam Sundar Dhanabalan, Siju Samuel, Somyajit Chakraborty Sam, Srihari Humbarwadi, srinivasan.narayanamoorthy, Srishti Yadav, Steph-En-M, Stephan Uphoff, Stephen Mugisha, SumanSudhir, Taehun Kim, Tamas Bela Feher, TengLu, Tetragramm, Thierry Herrmann, Tian Jin, tigertang, Tom Carchrae, Tom Forbes, Trent Lo, Victor Peng, vijayphoenix, Vincent Abriou, Vishal Bhola, Vishnuvardhan Janapati, vladbataev, VoVAllen, Wallyss Lima, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, William Zhang, Xiaoming (Jason) Cui, Xiaoquan Kong, Xinan Jiang, Yasir Modak, Yasuhiro Matsumoto, Yaxun (Sam) Liu, Yong Tang, Ytyt-Yt, yuan, Yuan Mingshuai, Yuan Tang, Yuki Ueda, Yusup, zhangshijin, zhuwenxi", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.2.0-rc2", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.2.0-rc2", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0-rc2", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/24949946"}, {"tag_name": "v2.2.0-rc1", "name": "TensorFlow Release 2.2.0-rc1", "author_name": "goldiegadde", "body": "# Release 2.2.0\r\n\r\n## Major Features and Improvements\r\n\r\n* Replaced the scalar type for string tensors from `std::string` to `tensorflow::tstring` which is now ABI stable.\r\n* A new Profiler for TF 2 for CPU/GPU/TPU. It offers both device and host performance analysis, including input pipeline and TF Ops. Optimization advisory is provided whenever possible. Please see [this tutorial](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) and [guide](https://www.tensorflow.org/guide/profiler) for usage guidelines.\r\n* Export C++ functions to Python using `pybind11` as opposed to `SWIG` as a part of our [deprecation of swig efforts](https://github.com/tensorflow/community/blob/master/rfcs/20190208-pybind11.md).\r\n* `tf.distribute`:\r\n  * Support added for global sync `BatchNormalization` by using the newly added `tf.keras.layers.experimental.SyncBatchNormalization` layer. This layer will sync `BatchNormalization` statistics every step across all replicas taking part in sync training.\r\n  * Performance improvements for GPU multi-worker distributed training using `tf.distribute.experimental.MultiWorkerMirroredStrategy`\r\n    * Update NVIDIA `NCCL` to `2.5.7-1` for better performance and performance tuning. Please see [nccl developer guide](https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html) for more information on this.\r\n    * Support gradient `allreduce` in `float16`. See this [example](https://github.com/tensorflow/models/blob/master/official/staging/training/grad_utils.py) usage.\r\n    * Experimental support of [all reduce gradient packing](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CollectiveHints) to allow overlapping gradient aggregation with backward path computation. \r\n* `tf.keras`:\r\n  * `Model.fit` major improvements:\r\n     * You can now use custom training logic with `Model.fit` by overriding `Model.train_step`.\r\n     * Easily write state-of-the-art training loops without worrying about all of the features `Model.fit` handles for you (distribution strategies, callbacks, data formats, looping logic, etc)\r\n     * See the default [`Model.train_step`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L433) for an example of what this function should look like\r\n     * Same applies for validation and inference via `Model.test_step` and `Model.predict_step`\r\n  * The SavedModel format now supports all Keras built-in layers (including metrics, preprocessing layers, and stateful RNN layers)\r\n* `tf.lite`:\r\n  * Enable TFLite experimental new converter by default.\r\n* XLA\r\n  * XLA now builds and works on windows. All prebuilt packages come with XLA available.\r\n  * XLA can be [enabled for a `tf.function`](https://www.tensorflow.org/xla#explicit_compilation_with_tffunction\r\n) with \u201ccompile or throw exception\u201d semantics on CPU and GPU.\r\n\r\n## Breaking Changes\r\n* `tf.keras`: \r\n  * In `tf.keras.applications` the name of the \"top\" layer has been standardized to \"predictions\". This is only a problem if your code relies on the exact name of the layer.\r\n  * Huber loss function has been updated to be consistent with other Keras losses. It now computes mean over the last axis of per-sample losses before applying the reduction function.\r\n* AutoGraph no longer converts functions passed to `tf.py_function`, `tf.py_func` and `tf.numpy_function`.\r\n* Deprecating `XLA_CPU` and `XLA_GPU` devices with this release.\r\n* Increasing the minimum bazel version to build TF to 2.0.0 to use Bazel's `cc_experimental_shared_library`.\r\n\r\n## Known Caveats\r\n* MacOS binaries are **not available** on pypi at tensorflow-cpu project, but they are identical to the binaries in [tensorflow project](https://pypi.org/project/tensorflow/2.2.0rc0/#files), since MacOS has no GPU.\r\n* Due to certain unforeseen circumstances, we are unable to release MacOS py3.8 binaries, but Windows/Linux binaries for py3.8 are available.\r\n* The current TensorFlow release now **requires** gast version 0.3.3. \r\n\r\n## Bug Fixes and Other Changes\r\n* `tf.data`:\r\n  * Removed `autotune_algorithm` from experimental optimization options.\r\n* TF Core:\r\n  * `tf.constant` always creates CPU tensors irrespective of the current device context.  \r\n  * Eager `TensorHandles` maintain a list of mirrors for any copies to local or remote devices. This avoids any redundant copies due to op execution.\r\n  * For `tf.Tensor` & `tf.Variable`, `.experimental_ref()` is no longer experimental and is available as simply `.ref()`.\r\n  * `pfor/vectorized_map`: Added support for vectorizing 56 more ops. Vectorizing `tf.cond` is also supported now.\r\n  * Set as much partial shape as we can infer statically within the gradient impl of the gather op.\r\n  * Gradient of `tf.while_loop` emits `StatelessWhile` op if `cond` and body functions are stateless. This allows multiple gradients while ops to run in parallel under distribution strategy.\r\n  * Speed up `GradientTape` in eager mode by auto-generating list of op inputs/outputs which are unused and hence not cached for gradient functions.\r\n  * Support `back_prop=False` in `while_v2` but mark it as deprecated.\r\n  * Improve error message when attempting to use `None` in data-dependent control flow.\r\n  * Add `RaggedTensor.numpy()`.\r\n  * Update `RaggedTensor.__getitem__` to preserve uniform dimensions & allow indexing into uniform dimensions.\r\n  * Update `tf.expand_dims` to always insert the new dimension as a non-ragged dimension.\r\n  * Update `tf.embedding_lookup` to use `partition_strategy` and `max_norm` when `ids` is ragged.\r\n  * Allow `batch_dims==rank(indices)` in `tf.gather`.\r\n  * Add support for bfloat16 in `tf.print`.  \r\n* `tf.distribute`: \r\n  * Support `embedding_column` with variable-length input features for `MultiWorkerMirroredStrategy`.\r\n* `tf.keras`:\r\n  * Added `all_reduce_sum_gradients` argument to `tf.keras.optimizer.Optimizer.apply_gradients`. This allows custom gradient aggregation and processing aggregated gradients in custom training loop.\r\n  * Allow `pathlib.Path` paths for loading models via Keras API.\r\n* `tf.function`/AutoGraph:\r\n  * AutoGraph is now available in `ReplicaContext.merge_call`, `Strategy.extended.update` and `Strategy.extended.update_non_slot`.\r\n  * Experimental support for shape invariants has been enabled in `tf.function`. See the API docs for `tf.autograph.experimental.set_loop_options` for additonal info.\r\n  * AutoGraph error messages now exclude frames corresponding to APIs internal to AutoGraph.\r\n  * Improve shape inference for `tf.function` input arguments to unlock more Grappler optimizations in TensorFlow 2.x.\r\n  * Improve automatic control dependency management of resources by allowing resource reads to occur in parallel and synchronizing only on writes.\r\n  * Fix execution order of multiple stateful calls to `experimental_run_v2` in `tf.function`.\r\n  * You can now iterate over `RaggedTensors` using a for loop inside `tf.function`.\r\n* `tf.lite`:\r\n  *  Migrated the `tf.lite` C inference API out of experimental into lite/c.\r\n  * Add an option to disallow `NNAPI` CPU / partial acceleration on Android 10\r\n  * TFLite Android AARs now include the C headers and APIs are required to use TFLite from native code.\r\n  * Refactors the delegate and delegate kernel sources to allow usage in the linter.\r\n  * Limit delegated ops to actually supported ones if a device name is specified or `NNAPI` CPU Fallback is disabled.\r\n  * TFLite now supports `tf.math.reciprocal1` op by lowering to `tf.div op`.\r\n  * TFLite's unpack op now supports boolean tensor inputs.\r\n  * Microcontroller and embedded code moved from experimental to main TFLite folder\r\n  * Check for large TFLite tensors.\r\n  * Fix GPU delegate crash with C++17.\r\n  * Add 5D support to TFLite `strided_slice`.\r\n  * Fix error in delegation of `DEPTH_TO_SPACE` to `NNAPI` causing op not to be accelerated.\r\n  * Fix segmentation fault when running a model with LSTM nodes using `NNAPI` Delegate\r\n  * Fix `NNAPI` delegate failure when an operand for Maximum/Minimum operation is a scalar.\r\n  * Fix `NNAPI` delegate failure when Axis input for reduce operation is a scalar.\r\n  * Expose option to limit the number of partitions that will be delegated to `NNAPI`.\r\n  * If a target accelerator is specified, use its feature level to determine operations to delegate instead of SDK version.\r\n* `tf.random`:\r\n  * Various random number generation improvements: \r\n    * Add a fast path for default `random_uniform`\r\n    * `random_seed` documentation improvement.\r\n    * `RandomBinomial` broadcasts and appends the sample shape to the left rather than the right. \r\n  * Added `tf.random.stateless_binomial`, `tf.random.stateless_gamma`, `tf.random.stateless_poisson`\r\n  * `tf.random.stateless_uniform` now supports unbounded sampling of `int` types.\r\n* Math and Linear Algebra:\r\n  * Add `tf.linalg.LinearOperatorTridiag`.\r\n  * Add `LinearOperatorBlockLowerTriangular`\r\n  * Add broadcasting support to tf.linalg.triangular_solve[#26204](https://github.com/tensorflow/tensorflow/issues/26204), tf.math.invert_permutation.\r\n  * Add `tf.math.sobol_sample` op.\r\n  * Add `tf.math.xlog1py`.\r\n  * Add `tf.math.special.{dawsn,expi,fresnel_cos,fresnel_sin,spence}`.\r\n  * Add a Modified Discrete Cosine Transform (MDCT) and its inverse to `tf.signal`.\r\n* TPU Enhancements:\r\n  * Refactor `TpuClusterResolver` to move shared logic to a separate pip package.\r\n  * Support configuring TPU software version from cloud tpu client.\r\n  * Allowed TPU embedding weight decay factor to be multiplied by learning rate.\r\n* XLA Support:\r\n  * Add standalone XLA AOT runtime target + relevant .cc sources to pip package.\r\n  * Add check for memory alignment to MemoryAllocation::MemoryAllocation() on 32-bit ARM. This ensures a deterministic early exit instead of a hard to debug bus error later.\r\n  * `saved_model_cli aot_compile_cpu` allows you to compile saved models to XLA header+object files and include them in your C++ programs.\r\n  * Enable `Igamma`, `Igammac` for XLA.\r\n  * XLA reduction emitter is deterministic when the environment variable `TF_DETERMINISTIC_OPS` is set.\r\n* Tracing and Debugging:\r\n  * Add source, destination name to `_send` traceme to allow easier debugging.\r\n  * Add traceme event to `fastpathexecute`.\r\n* Other:\r\n  * Fix an issue with AUC.reset_states for multi-label AUC [#35852](https://github.com/tensorflow/tensorflow/issues/35852)\r\n  * Fix the TF upgrade script to not delete files when there is a parsing error and the output mode is `in-place`.\r\n  * Move `tensorflow/core:framework/*_pyclif` rules to `tensorflow/core/framework:*_pyclif`.\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n372046933, 8bitmp3, aaronhma, Abin Shahab, Aditya Patwardhan, Agoniii, Ahti Kitsik, Alan Yee, Albin Joy, Alex Hoffman, Alexander Grund, Alexandre E. Eichenberger, Amit Kumar Jaiswal, amoitra, Andrew Anderson, Angus-Luo, Anthony Barbier, Anton Kachatkou, Anuj Rawat, archis, Arpan-Dhatt, Arvind Sundararajan, Ashutosh Hathidara, autoih, Bairen Yi, Balint Cristian, Bas Aarts, BashirSbaiti, Basit Ayantunde, Ben Barsdell, Benjamin Gaillard, boron, Brett Koonce, Bryan Cutler, Christian Goll, Christian Sachs, Clayne Robison, comet, Daniel Falbel, Daria Zhuravleva, darsh8200, David Truby, Dayananda-V, deepakm, Denis Khalikov, Devansh Singh, Dheeraj R Reddy, Diederik Van Liere, Diego Caballero, Dominic Jack, dothinking, Douman, Drake Gens, Duncan Riach, Ehsan Toosi, ekuznetsov139, Elena Zhelezina, elzino, Ending2015a, Eric Schweitz, Erik Zettel, Ethan Saadia, Eugene Kuznetsov, Evgeniy Zheltonozhskiy, Ewout Ter Hoeven, exfalso, FAIJUL, Fangjun Kuang, Fei Hu, Frank Laub, Frederic Bastien, Fredrik Knutsson, frreiss, Fr\u00e9d\u00e9ric Rechtenstein, fsx950223, Gaurav Singh, gbaned, George Grzegorz Pawelczak, George Sterpu, Gian Marco Iodice, Giorgio Arena, Hans Gaiser, Hans Pabst, Haoyu Wu, Harry Slatyer, hsahovic, Hugo, Hugo Sj\u00f6berg, IrinaM21, jacco, Jake Tae, Jean-Denis Lesage, Jean-Michel Gorius, Jeff Daily, Jens Elofsson, Jerry Shih, jerryyin, Jin Mingjian, Jinjing Zhou, JKIsaacLee, jojimonv, Jonathan Dekhtiar, Jose Ignacio Gomez, Joseph-Rance, Judd, Julian Gross, Kaixi Hou, Kaustubh Maske Patil, Keunwoo Choi, Kevin Hanselman, Khor Chean Wei, Kilaru Yasaswi Sri Chandra Gandhi, Koan-Sin Tan, Koki Ibukuro, Kristian Holsheimer, kurileo, Lakshay Tokas, Lee Netherton, leike666666, Leslie-Fang-Intel, Li, Guizi, LIUJIAN435, Lukas Geiger, Lyo Nguyen, madisetti, Maher Jendoubi, Mahmoud Abuzaina, Manuel Freiberger, Marcel Koester, Marco Jacopo Ferrarotti, Markus Franke, marload, Mbah-Javis, mbhuiyan, Meng Zhang, Michael Liao, MichaelKonobeev, Michal Tarnowski, Milan Straka, minoring, Mohamed Nour Abouelseoud, MoussaMM, Mrinal Jain, mrTsjolder, M\u00e5ns Nilsson, Namrata Bhave, Nicholas Gao, Niels Ole Salscheider, nikochiko, Niranjan Hasabnis, Nishidha Panpaliya, nmostafa, Noah Trenaman, nuka137, Officium, Owen L - Sfe, Pallavi G, Paul Andrey, Peng Sun, Peng Wu, Phil Pearl, PhilipMay, pingsutw, Pooya Davoodi, PragmaTwice, pshiko, Qwerty71, R Gomathi, Rahul Huilgol, Richard Xiao, Rick Wierenga, Roberto Rosmaninho, ruchit2801, Rushabh Vasani, Sami, Sana Damani, Sarvesh Dubey, Sasan Jafarnejad, Sergii Khomenko, Shane Smiskol, Shaochen Shi, sharkdtu, Shawn Presser, ShengYang1, Shreyash Patodia, Shyam Sundar Dhanabalan, Siju Samuel, Somyajit Chakraborty Sam, Srihari Humbarwadi, srinivasan.narayanamoorthy, Srishti Yadav, Steph-En-M, Stephan Uphoff, Stephen Mugisha, SumanSudhir, Taehun Kim, Tamas Bela Feher, TengLu, Tetragramm, Thierry Herrmann, Tian Jin, tigertang, Tom Carchrae, Tom Forbes, Trent Lo, Victor Peng, vijayphoenix, Vincent Abriou, Vishal Bhola, Vishnuvardhan Janapati, vladbataev, VoVAllen, Wallyss Lima, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, William Zhang, Xiaoming (Jason) Cui, Xiaoquan Kong, Xinan Jiang, Yasir Modak, Yasuhiro Matsumoto, Yaxun (Sam) Liu, Yong Tang, Ytyt-Yt, yuan, Yuan Mingshuai, Yuan Tang, Yuki Ueda, Yusup, zhangshijin, zhuwenxi\r\n", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.2.0-rc1", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.2.0-rc1", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0-rc1", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/24652133"}, {"tag_name": "v2.2.0-rc0", "name": "TensorFlow 2.2.0-rc0", "author_name": "goldiegadde", "body": "# Release 2.2.0\r\n## Major Features and Improvements\r\n\r\n* Replaced the scalar type for string tensors from `std::string` to `tensorflow::tstring` which is now ABI stable.\r\n* A new Profiler for TF 2 for CPU/GPU/TPU. It offers both device and host performance analysis, including input pipeline and TF Ops. Optimization advisory is provided whenever possible. Please see [this tutorial](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) for usage guidelines.\r\n* Export C++ functions to Python using `pybind11` as opposed to `SWIG` as a part of our [deprecation of swig efforts](https://github.com/tensorflow/community/blob/master/rfcs/20190208-pybind11.md).\r\n* `tf.distribute`:\r\n  * Support added for global sync `BatchNormalization` by using the newly added `tf.keras.layers.experimental.SyncBatchNormalization` layer. This layer will sync `BatchNormalization` statistics every step across all replicas taking part in sync training.\r\n  * Performance improvements for GPU multi-worker distributed training using `tf.distribute.experimental.MultiWorkerMirroredStrategy`\r\n    * Update NVIDIA `NCCL` to `2.5.7-1` for better performance and performance tuning. Please see [nccl developer guide](https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html) for more information on this.\r\n    * Support gradient `allreduce` in `float16`. See this [example](https://github.com/tensorflow/models/blob/master/official/staging/training/grad_utils.py) usage.\r\n    * Experimental support of [all reduce gradient packing](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CollectiveHints) to allow overlapping gradient aggregation with backward path computation. \r\n* `tf.keras`:\r\n  * `Model.fit` major improvements:\r\n     * You can now use custom training logic with `Model.fit` by overriding `Model.train_step`.\r\n     * Easily write state-of-the-art training loops without worrying about all of the features `Model.fit` handles for you (distribution strategies, callbacks, data formats, looping logic, etc)\r\n     * See the default [`Model.train_step`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L433) for an example of what this function should look like\r\n     * Same applies for validation and inference via `Model.test_step` and `Model.predict_step`\r\n  * The SavedModel format now supports all Keras built-in layers (including metrics, preprocessing layers, and stateful RNN layers)\r\n* `tf.lite`:\r\n  * Enable TFLite experimental new converter by default.\r\n* XLA\r\n  * XLA now builds and works on windows. All prebuilt packages come with XLA available.\r\n  * XLA can be [enabled for a `tf.function`](https://www.tensorflow.org/xla#explicit_compilation_with_tffunction\r\n) with \u201ccompile or throw exception\u201d semantics on CPU and GPU.\r\n\r\n## Breaking Changes\r\n* `tf.keras`: \r\n  * In `tf.keras.applications` the name of the \"top\" layer has been standardized to \"predictions\". This is only a problem if your code relies on the exact name of the layer.\r\n  * Huber loss function has been updated to be consistent with other Keras losses. It now computes mean over the last axis of per-sample losses before applying the reduction function.\r\n* AutoGraph no longer converts functions passed to `tf.py_function`, `tf.py_func` and `tf.numpy_function`.\r\n* Deprecating `XLA_CPU` and `XLA_GPU` devices with this release.\r\n* Increasing the minimum bazel version to build TF to 1.2.1 to use Bazel's `cc_experimental_shared_library`.\r\n\r\n## Known Caveats\r\n* MacOS binaries are **not available** on pypi at tensorflow-cpu project, but they are identical to the binaries in [tensorflow project](https://pypi.org/project/tensorflow/2.2.0rc0/#files), since MacOS has no GPU.\r\n\r\n## Bug Fixes and Other Changes\r\n* `tf.data`:\r\n  * Removed `autotune_algorithm` from experimental optimization options.\r\n* TF Core:\r\n  * `tf.constant` always creates CPU tensors irrespective of the current device context.  \r\n  * Eager TensorHandles maintain a list of mirrors for any copies to local or remote devices. This avoids any redundant copies due to op execution.\r\n  * For `tf.Tensor` & `tf.Variable`, `.experimental_ref()` is no longer experimental and is available as simply `.ref()`.\r\n  * Support matrix inverse and solves in `pfor/vectorized_map`.\r\n  * Set as much partial shape as we can infer statically within the gradient impl of the gather op.\r\n  * Gradient of `tf.while_loop` emits `StatelessWhile` op if `cond` and body functions are stateless. This allows multiple gradients while ops to run in parallel under distribution strategy.\r\n  * Speed up `GradientTape` in eager mode by auto-generating list of op inputs/outputs which are unused and hence not cached for gradient functions.\r\n  * Support `back_prop=False` in `while_v2` but mark it as deprecated.\r\n  * Improve error message when attempting to use `None` in data-dependent control flow.\r\n  * Add `RaggedTensor.numpy()`.\r\n  * Update `RaggedTensor.__getitem__` to preserve uniform dimensions & allow indexing into uniform dimensions.\r\n  * Update `tf.expand_dims` to always insert the new dimension as a non-ragged dimension.\r\n  * Update `tf.embedding_lookup` to use `partition_strategy` and `max_norm` when `ids` is ragged.\r\n  * Allow `batch_dims==rank(indices)` in `tf.gather`.\r\n  * Add support for bfloat16 in `tf.print`.  \r\n* `tf.distribute`: \r\n  * Support `embedding_column` with variable-length input features for `MultiWorkerMirroredStrategy`.\r\n* `tf.keras`:\r\n  * Added `all_reduce_sum_gradients` argument to `tf.keras.optimizer.Optimizer.apply_gradients`. This allows custom gradient aggregation and processing aggregated gradients in custom training loop.\r\n  * Allow `pathlib.Path` paths for loading models via Keras API.\r\n* `tf.function`/AutoGraph:\r\n  * AutoGraph is now available in `ReplicaContext.merge_call`, `Strategy.extended.update` and `Strategy.extended.update_non_slot`.\r\n  * Experimental support for shape invariants has been enabled in `tf.function`. See the API docs for `tf.autograph.experimental.set_loop_options` for additonal info.\r\n  * AutoGraph error messages now exclude frames corresponding to APIs internal to AutoGraph.\r\n  * Improve shape inference for `tf.function` input arguments to unlock more Grappler optimizations in TensorFlow 2.x.\r\n  * Improve automatic control dependency management of resources by allowing resource reads to occur in parallel and synchronizing only on writes.\r\n  * Fix execution order of multiple stateful calls to `experimental_run_v2` in `tf.function`.\r\n  * You can now iterate over `RaggedTensors` using a for loop inside `tf.function`.\r\n* `tf.lite`:\r\n  *  Migrated the `tf.lite` C inference API out of experimental into lite/c.\r\n  * Add an option to disallow `NNAPI` CPU / partial acceleration on Android 10\r\n  * TFLite Android AARs now include the C headers and APIs are required to use TFLite from native code.\r\n  * Refactors the delegate and delegate kernel sources to allow usage in the linter.\r\n  * Limit delegated ops to actually supported ones if a device name is specified or `NNAPI` CPU Fallback is disabled.\r\n  * TFLite now supports `tf.math.reciprocal1` op by lowering to `tf.div op`.\r\n  * TFLite's unpack op now supports boolean tensor inputs.\r\n  * Microcontroller and embedded code moved from experimental to main TensorFlow Lite folder\r\n  * Check for large TFLite tensors.\r\n  * Fix GPU delegate crash with C++17.\r\n  * Add 5D support to TFLite `strided_slice`.\r\n  * Fix error in delegation of `DEPTH_TO_SPACE` to `NNAPI` causing op not to be accelerated.\r\n  * Fix segmentation fault when running a model with LSTM nodes using `NNAPI` Delegate\r\n  * Fix `NNAPI` delegate failure when an operand for Maximum/Minimum operation is a scalar.\r\n  * Fix `NNAPI` delegate failure when Axis input for reduce operation is a scalar.\r\n  * Expose option to limit the number of partitions that will be delegated to `NNAPI`.\r\n  * If a target accelerator is specified, use its feature level to determine operations to delegate instead of SDK version.\r\n* `tf.random`:\r\n  * Various random number generation improvements: \r\n    * Add a fast path for default `random_uniform`\r\n    * `random_seed` documentation improvement.\r\n    * `RandomBinomial` broadcasts and appends the sample shape to the left rather than the right. \r\n  * Added `tf.random.stateless_binomial`, `tf.random.stateless_gamma`, `tf.random.stateless_poisson`\r\n  * `tf.random.stateless_uniform` now supports unbounded sampling of `int` types.\r\n* Math and Linear Algebra:\r\n  * Add `tf.linalg.LinearOperatorTridiag`.\r\n  * Add `LinearOperatorBlockLowerTriangular`\r\n  * Add broadcasting support to tf.linalg.triangular_solve[#26204](https://github.com/tensorflow/tensorflow/issues/26204), tf.math.invert_permutation.\r\n  * Add `tf.math.sobol_sample` op.\r\n  * Add `tf.math.xlog1py`.\r\n  * Add `tf.math.special.{dawsn,expi,fresnel_cos,fresnel_sin,spence}`.\r\n  * Add a Modified Discrete Cosine Transform (MDCT) and its inverse to `tf.signal`.\r\n* TPU Enhancements:\r\n  * Refactor `TpuClusterResolver` to move shared logic to a separate pip package.\r\n  * Support configuring TPU software version from cloud tpu client.\r\n  * Allowed TPU embedding weight decay factor to be multiplied by learning rate.\r\n* XLA Support:\r\n  * Add standalone XLA AOT runtime target + relevant .cc sources to pip package.\r\n  * Add check for memory alignment to MemoryAllocation::MemoryAllocation() on 32-bit ARM. This ensures a deterministic early exit instead of a hard to debug bus error later.\r\n  * `saved_model_cli aot_compile_cpu` allows you to compile saved models to XLA header+object files and include them in your C++ programs.\r\n  * Enable `Igamma`, `Igammac` for XLA.\r\n  * XLA reduction emitter is deterministic when the environment variable `TF_DETERMINISTIC_OPS` is set.\r\n* Tracing and Debugging:\r\n  * Add source, destination name to `_send` traceme to allow easier debugging.\r\n  * Add traceme event to `fastpathexecute`.\r\n* Other:\r\n  * Fix an issue with AUC.reset_states for multi-label AUC [#35852](https://github.com/tensorflow/tensorflow/issues/35852)\r\n  * Fix the TF upgrade script to not delete files when there is a parsing error and the output mode is `in-place`.\r\n  * Move `tensorflow/core:framework/*_pyclif` rules to `tensorflow/core/framework:*_pyclif`.\r\n\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n372046933, 8bitmp3, aaronhma, Abin Shahab, Aditya Patwardhan, Agoniii, Ahti Kitsik, Alan Yee, Albin Joy, Alex Hoffman, Alexander Grund, Alexandre E. Eichenberger, Amit Kumar Jaiswal, amoitra, Andrew Anderson, Angus-Luo, Anthony Barbier, Anton Kachatkou, Anuj Rawat, archis, Arpan-Dhatt, Arvind Sundararajan, Ashutosh Hathidara, autoih, Bairen Yi, Balint Cristian, Bas Aarts, BashirSbaiti, Basit Ayantunde, Ben Barsdell, Benjamin Gaillard, boron, Brett Koonce, Bryan Cutler, Christian Goll, Christian Sachs, Clayne Robison, comet, Daniel Falbel, Daria Zhuravleva, darsh8200, David Truby, Dayananda-V, deepakm, Denis Khalikov, Devansh Singh, Dheeraj R Reddy, Diederik Van Liere, Diego Caballero, Dominic Jack, dothinking, Douman, Drake Gens, Duncan Riach, Ehsan Toosi, ekuznetsov139, Elena Zhelezina, elzino, Ending2015a, Eric Schweitz, Erik Zettel, Ethan Saadia, Eugene Kuznetsov, Evgeniy Zheltonozhskiy, Ewout Ter Hoeven, exfalso, FAIJUL, Fangjun Kuang, Fei Hu, Frank Laub, Frederic Bastien, Fredrik Knutsson, frreiss, Fr\u00e9d\u00e9ric Rechtenstein, fsx950223, Gaurav Singh, gbaned, George Grzegorz Pawelczak, George Sterpu, Gian Marco Iodice, Giorgio Arena, Hans Gaiser, Hans Pabst, Haoyu Wu, Harry Slatyer, hsahovic, Hugo, Hugo Sj\u00f6berg, IrinaM21, jacco, Jake Tae, Jean-Denis Lesage, Jean-Michel Gorius, Jeff Daily, Jens Elofsson, Jerry Shih, jerryyin, Jin Mingjian, Jinjing Zhou, JKIsaacLee, jojimonv, Jonathan Dekhtiar, Jose Ignacio Gomez, Joseph-Rance, Judd, Julian Gross, Kaixi Hou, Kaustubh Maske Patil, Keunwoo Choi, Kevin Hanselman, Khor Chean Wei, Kilaru Yasaswi Sri Chandra Gandhi, Koan-Sin Tan, Koki Ibukuro, Kristian Holsheimer, kurileo, Lakshay Tokas, Lee Netherton, leike666666, Leslie-Fang-Intel, Li, Guizi, LIUJIAN435, Lukas Geiger, Lyo Nguyen, madisetti, Maher Jendoubi, Mahmoud Abuzaina, Manuel Freiberger, Marcel Koester, Marco Jacopo Ferrarotti, Markus Franke, marload, Mbah-Javis, mbhuiyan, Meng Zhang, Michael Liao, MichaelKonobeev, Michal Tarnowski, Milan Straka, minoring, Mohamed Nour Abouelseoud, MoussaMM, Mrinal Jain, mrTsjolder, M\u00e5ns Nilsson, Namrata Bhave, Nicholas Gao, Niels Ole Salscheider, nikochiko, Niranjan Hasabnis, Nishidha Panpaliya, nmostafa, Noah Trenaman, nuka137, Officium, Owen L - Sfe, Pallavi G, Paul Andrey, Peng Sun, Peng Wu, Phil Pearl, PhilipMay, pingsutw, Pooya Davoodi, PragmaTwice, pshiko, Qwerty71, R Gomathi, Rahul Huilgol, Richard Xiao, Rick Wierenga, Roberto Rosmaninho, ruchit2801, Rushabh Vasani, Sami, Sana Damani, Sarvesh Dubey, Sasan Jafarnejad, Sergii Khomenko, Shane Smiskol, Shaochen Shi, sharkdtu, Shawn Presser, ShengYang1, Shreyash Patodia, Shyam Sundar Dhanabalan, Siju Samuel, Somyajit Chakraborty Sam, Srihari Humbarwadi, srinivasan.narayanamoorthy, Srishti Yadav, Steph-En-M, Stephan Uphoff, Stephen Mugisha, SumanSudhir, Taehun Kim, Tamas Bela Feher, TengLu, Tetragramm, Thierry Herrmann, Tian Jin, tigertang, Tom Carchrae, Tom Forbes, Trent Lo, Victor Peng, vijayphoenix, Vincent Abriou, Vishal Bhola, Vishnuvardhan Janapati, vladbataev, VoVAllen, Wallyss Lima, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, William Zhang, Xiaoming (Jason) Cui, Xiaoquan Kong, Xinan Jiang, Yasir Modak, Yasuhiro Matsumoto, Yaxun (Sam) Liu, Yong Tang, Ytyt-Yt, yuan, Yuan Mingshuai, Yuan Tang, Yuki Ueda, Yusup, zhangshijin, zhuwenxi\r\n", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.2.0-rc0", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.2.0-rc0", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0-rc0", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/24407698"}, {"tag_name": "v1.15.2", "name": "TensorFlow 1.15.2", "author_name": "tensorflow-jenkins", "body": "# Release 1.15.2\r\n\r\nNote that this release no longer has a single pip package for GPU and CPU. Please see #36347 for history and details\r\n\r\n## Bug Fixes and Other Changes\r\n* Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))\r\n* Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)\r\n* Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.15.2", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.15.2", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.15.2", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/23133744"}, {"tag_name": "v2.0.1", "name": "TensorFlow 2.0.1", "author_name": "tensorflow-jenkins", "body": "# Release 2.0.1\r\n\r\n## Bug Fixes and Other Changes\r\n* Fixes a security vulnerability where converting a Python string to a `tf.float16` value produces a segmentation fault ([CVE-2020-5215](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-5215))\r\n* Updates `curl` to `7.66.0` to handle [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482) and [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)\r\n* Updates `sqlite3` to `3.30.01` to handle [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646), [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645) and [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.0.1", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.0.1", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.0.1", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/23125231"}, {"tag_name": "v2.1.0", "name": "TensorFlow 2.1.0", "author_name": "tensorflow-jenkins", "body": "# Release 2.1.0\r\n\r\nTensorFlow 2.1 will be the last TF release supporting Python 2. Python 2 support [officially ends an January 1, 2020](https://www.python.org/dev/peps/pep-0373/#update). [As announced earlier](https://groups.google.com/a/tensorflow.org/d/msg/announce/gVwS5RC8mds/dCt1ka2XAAAJ), TensorFlow will also stop supporting Python 2 starting January 1, 2020, and no more releases are expected in 2019.\r\n\r\n## Major Features and Improvements\r\n* The `tensorflow` pip package now includes GPU support by default (same as `tensorflow-gpu`) for both Linux and Windows. This runs on machines with and without NVIDIA GPUs. `tensorflow-gpu` is still available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.\r\n* **Windows users:** Officially-released `tensorflow` Pip packages are now built with Visual Studio 2019 version 16.4 in order to take advantage of the new `/d2ReducedOptimizeHugeFunctions` compiler flag. To use these new packages, you must install \"Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019\", available from Microsoft's website [here](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads).\r\n  * This does not change the minimum required version for building TensorFlow from source on Windows, but builds enabling `EIGEN_STRONG_INLINE` can take over 48 hours to compile without this flag. Refer to `configure.py` for more information about `EIGEN_STRONG_INLINE` and `/d2ReducedOptimizeHugeFunctions`.\r\n  * If either of the required DLLs, `msvcp140.dll` (old) or `msvcp140_1.dll` (new), are missing on your machine, `import tensorflow` will print a warning message.\r\n* The `tensorflow` pip package is built with CUDA 10.1 and cuDNN 7.6.\r\n* `tf.keras`\r\n  * Experimental support for mixed precision is available on GPUs and Cloud TPUs. See [usage guide](https://www.tensorflow.org/guide/keras/mixed_precision).\r\n  * Introduced the `TextVectorization` layer, which takes as input raw strings and takes care of text standardization, tokenization, n-gram generation, and vocabulary indexing. See this [end-to-end text classification example](https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3). \r\n  * Keras `.compile` `.fit` `.evaluate` and `.predict` are allowed to be outside of the DistributionStrategy scope, as long as the model was constructed inside of a scope.\r\n  * Experimental support for Keras `.compile`, `.fit`, `.evaluate`, and `.predict` is available for Cloud TPUs, Cloud TPU, for all types of Keras models (sequential, functional and subclassing models).\r\n  * Automatic outside compilation is now enabled for Cloud TPUs. This allows `tf.summary` to be used more conveniently with Cloud TPUs.\r\n  * Dynamic batch sizes with DistributionStrategy and Keras are supported on Cloud TPUs.\r\n  * Support for `.fit`, `.evaluate`, `.predict` on TPU using numpy data, in addition to `tf.data.Dataset`.\r\n  * Keras reference implementations for many popular models are available in the TensorFlow [Model Garden](https://github.com/tensorflow/models/tree/master/official).\r\n* `tf.data`\r\n  * Changes rebatching for `tf.data datasets` + DistributionStrategy for better performance. Note that the dataset also behaves slightly differently, in that the rebatched dataset cardinality will always be a multiple of the number of replicas.\r\n  * `tf.data.Dataset` now supports automatic data distribution and sharding in distributed environments, including on TPU pods.\r\n  * Distribution policies for `tf.data.Dataset` can now be tuned with 1. `tf.data.experimental.AutoShardPolicy(OFF, AUTO, FILE, DATA)` 2. `tf.data.experimental.ExternalStatePolicy(WARN, IGNORE, FAIL)`\r\n* `tf.debugging`\r\n  * Add `tf.debugging.enable_check_numerics()` and `tf.debugging.disable_check_numerics()` to help debugging the root causes of issues involving infinities and `NaN`s.\r\n* `tf.distribute`\r\n  * Custom training loop support on TPUs and TPU pods is avaiable through `strategy.experimental_distribute_dataset`, `strategy.experimental_distribute_datasets_from_function`, `strategy.experimental_run_v2`, `strategy.reduce`.\r\n  * Support for a global distribution strategy through `tf.distribute.experimental_set_strategy(),` in addition to `strategy.scope()`.\r\n* `TensorRT`\r\n  * [TensorRT 6.0](https://developer.nvidia.com/tensorrt#tensorrt-whats-new) is now supported and enabled by default. This adds support for more TensorFlow ops including Conv3D, Conv3DBackpropInputV2, AvgPool3D, MaxPool3D, ResizeBilinear, and ResizeNearestNeighbor. In addition, the TensorFlow-TensorRT python conversion API is exported as `tf.experimental.tensorrt.Converter`.\r\n* Environment variable `TF_DETERMINISTIC_OPS` has been added. When set to \"true\" or \"1\", this environment variable makes `tf.nn.bias_add` operate deterministically (i.e. reproducibly), but currently only when XLA JIT compilation is *not* enabled. Setting `TF_DETERMINISTIC_OPS` to \"true\" or \"1\" also makes cuDNN convolution and max-pooling operate deterministically. This makes Keras Conv\\*D and MaxPool\\*D layers operate deterministically in both the forward and backward directions when running on a CUDA-enabled GPU.\r\n\r\n## Breaking Changes\r\n* Deletes `Operation.traceback_with_start_lines` for which we know of no usages.\r\n* Removed `id` from `tf.Tensor.__repr__()` as `id` is not useful other than internal debugging.\r\n* Some `tf.assert_*` methods now raise assertions at operation creation time if the input tensors' values are known at that time, not during the `session.run()`. This only changes behavior when the graph execution would have resulted in an error. When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).\r\n* The following APIs are not longer experimental: `tf.config.list_logical_devices`, `tf.config.list_physical_devices`, `tf.config.get_visible_devices`, `tf.config.set_visible_devices`, `tf.config.get_logical_device_configuration`, `tf.config.set_logical_device_configuration`.\r\n* `tf.config.experimentalVirtualDeviceConfiguration` has been renamed to `tf.config.LogicalDeviceConfiguration`.\r\n* `tf.config.experimental_list_devices` has been removed, please use\r\n`tf.config.list_logical_devices`.\r\n\r\n## Bug Fixes and Other Changes\r\n* `tf.data`\r\n  * Fixes concurrency issue with `tf.data.experimental.parallel_interleave` with `sloppy=True`.\r\n  * Add `tf.data.experimental.dense_to_ragged_batch()`.\r\n  * Extend `tf.data` parsing ops to support `RaggedTensors`.\r\n* `tf.distribute`\r\n  * Fix issue where GRU would crash or give incorrect output when a `tf.distribute.Strategy` was used. \r\n* `tf.estimator`\r\n  * Added option in `tf.estimator.CheckpointSaverHook` to not save the `GraphDef`.\r\n  * Moving the checkpoint reader from swig to pybind11.\r\n* `tf.keras`\r\n  * Export `depthwise_conv2d` in `tf.keras.backend`.\r\n  * In Keras Layers and Models, Variables in `trainable_weights`, `non_trainable_weights`, and `weights` are explicitly deduplicated.\r\n  * Keras `model.load_weights` now accepts `skip_mismatch` as an argument. This was available in external Keras, and has now been copied over to `tf.keras`.\r\n  * Fix the input shape caching behavior of Keras convolutional layers.\r\n  * `Model.fit_generator`, `Model.evaluate_generator`, `Model.predict_generator`, `Model.train_on_batch`, `Model.test_on_batch`, and `Model.predict_on_batch` methods now respect the `run_eagerly` property, and will correctly run using `tf.function` by default. Note that `Model.fit_generator`, `Model.evaluate_generator`, and `Model.predict_generator` are deprecated endpoints. They are subsumed by `Model.fit`, `Model.evaluate`, and `Model.predict` which now support generators and Sequences.\r\n* `tf.lite`\r\n  * Legalization for `NMS` ops in TFLite.\r\n  * add `narrow_range` and `axis` to `quantize_v2` and `dequantize` ops. \r\n  * Added support for `FusedBatchNormV3` in converter.\r\n  * Add an `errno`-like field to `NNAPI` delegate for detecting `NNAPI` errors for fallback behaviour.\r\n  * Refactors `NNAPI` Delegate to support detailed reason why an operation is not accelerated.\r\n  * Converts hardswish subgraphs into atomic ops.\r\n* Other\r\n  * Critical stability updates for TPUs, especially in cases where the XLA compiler produces compilation errors.\r\n  * TPUs can now be re-initialized multiple times, using `tf.tpu.experimental.initialize_tpu_system`. \r\n  * Add `RaggedTensor.merge_dims()`.\r\n  * Added new `uniform_row_length` row-partitioning tensor to `RaggedTensor`.\r\n  * Add `shape` arg to `RaggedTensor.to_tensor`; Improve speed of `RaggedTensor.to_tensor`.\r\n  * `tf.io.parse_sequence_example` and `tf.io.parse_single_sequence_example` now support ragged features.\r\n  * Fix `while_v2` with variables in custom gradient.\r\n  * Support taking gradients of V2 `tf.cond` and `tf.while_loop` using `LookupTable`.\r\n  * Fix bug where `vectorized_map` failed on inputs with unknown static shape.\r\n  * Add preliminary support for sparse CSR matrices.\r\n  * Tensor equality with `None` now behaves as expected.\r\n  * Make calls to `tf.function(f)()`, `tf.function(f).get_concrete_function` and `tf.function(f).get_initialization_function` thread-safe.\r\n  * Extend `tf.identity` to work with CompositeTensors (such as SparseTensor)\r\n  * Added more `dtypes` and zero-sized inputs to `Einsum` Op and improved its performance\r\n  * Enable multi-worker `NCCL` `all-reduce` inside functions executing eagerly.\r\n  * Added complex128 support to `RFFT`, `RFFT2D`, `RFFT3D`, `IRFFT`, `IRFFT2D`, and `IRFFT3D`.\r\n  * Add `pfor` converter for `SelfAdjointEigV2`.\r\n  * Add `tf.math.ndtri` and `tf.math.erfinv`.\r\n  * Add `tf.config.experimental.enable_mlir_bridge` to allow using MLIR compiler bridge in eager model.\r\n  * Added support for MatrixSolve on Cloud TPU / XLA.\r\n  * Added `tf.autodiff.ForwardAccumulator` for forward-mode autodiff\r\n  * Add `LinearOperatorPermutation`.\r\n  * A few performance optimizations on `tf.reduce_logsumexp`.\r\n  * Added multilabel handling to `AUC` metric\r\n  * Optimization on `zeros_like`.\r\n  * Dimension constructor now requires `None` or types with an `__index__` method.\r\n  * Add `tf.random.uniform` microbenchmark.\r\n  * Use `_protogen` suffix for proto library targets instead of `_cc_protogen` suffix.\r\n  * Moving the checkpoint reader from `swig` to `pybind11`.\r\n  * `tf.device` & `MirroredStrategy` now supports passing in a `tf.config.LogicalDevice`\r\n  * If you're building Tensorflow from source, consider using [bazelisk](https://github.com/bazelbuild/bazelisk) to automatically download and use the correct Bazel version. Bazelisk reads the `.bazelversion` file at the root of the project directory.\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n8bitmp3, Aaron Ma, Abd\u00fcLhamit Yilmaz, Abhai Kollara, aflc, Ag Ramesh, Albert Z. Guo, Alex Torres, amoitra, Andrii Prymostka, angeliand, Anshuman Tripathy, Anthony Barbier, Anton Kachatkou, Anubh-V, Anuja Jakhade, Artem Ryabov, autoih, Bairen Yi, Bas Aarts, Basit Ayantunde, Ben Barsdell, Bhavani Subramanian, Brett Koonce, candy.dc, Captain-Pool, caster, cathy, Chong Yan, Choong Yin Thong, Clayne Robison, Colle, Dan Ganea, David Norman, David Refaeli, dengziming, Diego Caballero, Divyanshu, djshen, Douman, Duncan Riach, EFanZh, Elena Zhelezina, Eric Schweitz, Evgenii Zheltonozhskii, Fei Hu, fo40225, Fred Reiss, Frederic Bastien, Fredrik Knutsson, fsx950223, fwcore, George Grzegorz Pawelczak, George Sterpu, Gian Marco Iodice, Giorgio Arena, giuros01, Gomathi Ramamurthy, Guozhong Zhuang, Haifeng Jin, Haoyu Wu, HarikrishnanBalagopal, HJYOO, Huang Chen-Yi, Ilham Firdausi Putra, Imran Salam, Jared Nielsen, Jason Zaman, Jasper Vicenti, Jeff Daily, Jeff Poznanovic, Jens Elofsson, Jerry Shih, jerryyin, Jesper Dramsch, jim.meyer, Jongwon Lee, Jun Wan, Junyuan Xie, Kaixi Hou, kamalkraj, Kan Chen, Karthik Muthuraman, Keiji Ariyama, Kevin Rose, Kevin Wang, Koan-Sin Tan, kstuedem, Kwabena W. Agyeman, Lakshay Tokas, latyas, Leslie-Fang-Intel, Li, Guizi, Luciano Resende, Lukas Folle, Lukas Geiger, Mahmoud Abuzaina, Manuel Freiberger, Mark Ryan, Martin Mlostek, Masaki Kozuki, Matthew Bentham, Matthew Denton, mbhuiyan, mdfaijul, Muhwan Kim, Nagy Mostafa, nammbash, Nathan Luehr, Nathan Wells, Niranjan Hasabnis, Oleksii Volkovskyi, Olivier Moindrot, olramde, Ouyang Jin, OverLordGoldDragon, Pallavi G, Paul Andrey, Paul Wais, pkanwar23, Pooya Davoodi, Prabindh Sundareson, Rajeshwar Reddy T, Ralovich, Kristof, Refraction-Ray, Richard Barnes, richardbrks, Robert Herbig, Romeo Kienzler, Ryan Mccormick, saishruthi, Saket Khandelwal, Sami Kama, Sana Damani, Satoshi Tanaka, Sergey Mironov, Sergii Khomenko, Shahid, Shawn Presser, ShengYang1, Siddhartha Bagaria, Simon Plovyt, skeydan, srinivasan.narayanamoorthy, Stephen Mugisha, sunway513, Takeshi Watanabe, Taylor Jakobson, TengLu, TheMindVirus, ThisIsIsaac, Tim Gates, Timothy Liu, Tomer Gafner, Trent Lo, Trevor Hickey, Trevor Morris, vcarpani, Wei Wang, Wen-Heng (Jack) Chung, wenshuai, Wenshuai-Xiaomi, wenxizhu, william, William D. Irons, Xinan Jiang, Yannic, Yasir Modak, Yasuhiro Matsumoto, Yong Tang, Yongfeng Gu, Youwei Song, Zaccharie Ramzi, Zhang, Zhenyu Guo, \u738b\u632f\u534e (Zhenhua Wang), \u97e9\u8463, \uc774\uc911\uac74 Isaac Lee", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.1.0", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.1.0", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/22687731"}, {"tag_name": "v2.1.0-rc2", "name": "TensorFlow 2.1.0-rc2", "author_name": "tensorflow-jenkins", "body": "# Release 2.1.0\r\n\r\nTensorFlow 2.1 will be the last TF release supporting Python 2. Python 2 support [officially ends an January 1, 2020](https://www.python.org/dev/peps/pep-0373/#update). [As announced earlier](https://groups.google.com/a/tensorflow.org/d/msg/announce/gVwS5RC8mds/dCt1ka2XAAAJ), TensorFlow will also stop supporting Python 2 starting January 1, 2020, and no more releases are expected in 2019.\r\n\r\n## Major Features and Improvements\r\n* The `tensorflow` pip package now includes GPU support by default (same as `tensorflow-gpu`) for both Linux and Windows. This runs on machines with and without NVIDIA GPUs. `tensorflow-gpu` is still available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.\r\n* **Windows users:** Officially-released `tensorflow` Pip packages are now built with Visual Studio 2019 version 16.4 in order to take advantage of the new `/d2ReducedOptimizeHugeFunctions` compiler flag. To use these new packages, you must install \"Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019\", available from Microsoft's website [here](https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads).\r\n  * This does not change the minimum required version for building TensorFlow from source on Windows, but builds enabling `EIGEN_STRONG_INLINE` can take over 48 hours to compile without this flag. Refer to `configure.py` for more information about `EIGEN_STRONG_INLINE` and `/d2ReducedOptimizeHugeFunctions`.\r\n  * If either of the required DLLs, `msvcp140.dll` (old) or `msvcp140_1.dll` (new), are missing on your machine, `import tensorflow` will print a warning message.\r\n* The `tensorflow` pip package is built with CUDA 10.1 and cuDNN 7.6.\r\n* `tf.keras`\r\n  * Experimental support for mixed precision is available on GPUs and Cloud TPUs. See [usage guide](https://www.tensorflow.org/guide/keras/mixed_precision).\r\n  * Introduced the `TextVectorization` layer, which takes as input raw strings and takes care of text standardization, tokenization, n-gram generation, and vocabulary indexing. See this [end-to-end text classification example](https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3). \r\n  * Keras `.compile` `.fit` `.evaluate` and `.predict` are allowed to be outside of the DistributionStrategy scope, as long as the model was constructed inside of a scope.\r\n  * Experimental support for Keras `.compile`, `.fit`, `.evaluate`, and `.predict` is available for Cloud TPUs, Cloud TPU, for all types of Keras models (sequential, functional and subclassing models).\r\n  * Automatic outside compilation is now enabled for Cloud TPUs. This allows `tf.summary` to be used more conveniently with Cloud TPUs.\r\n  * Dynamic batch sizes with DistributionStrategy and Keras are supported on Cloud TPUs.\r\n  * Support for `.fit`, `.evaluate`, `.predict` on TPU using numpy data, in addition to `tf.data.Dataset`.\r\n  * Keras reference implementations for many popular models are available in the TensorFlow [Model Garden](https://github.com/tensorflow/models/tree/master/official).\r\n* `tf.data`\r\n  * Changes rebatching for `tf.data datasets` + DistributionStrategy for better performance. Note that the dataset also behaves slightly differently, in that the rebatched dataset cardinality will always be a multiple of the number of replicas.\r\n  * `tf.data.Dataset` now supports automatic data distribution and sharding in distributed environments, including on TPU pods.\r\n  * Distribution policies for `tf.data.Dataset` can now be tuned with 1. `tf.data.experimental.AutoShardPolicy(OFF, AUTO, FILE, DATA)` 2. `tf.data.experimental.ExternalStatePolicy(WARN, IGNORE, FAIL)`\r\n* `tf.debugging`\r\n  * Add `tf.debugging.enable_check_numerics()` and `tf.debugging.disable_check_numerics()` to help debugging the root causes of issues involving infinities and `NaN`s.\r\n* `tf.distribute`\r\n  * Custom training loop support on TPUs and TPU pods is avaiable through `strategy.experimental_distribute_dataset`, `strategy.experimental_distribute_datasets_from_function`, `strategy.experimental_run_v2`, `strategy.reduce`.\r\n  * Support for a global distribution strategy through `tf.distribute.experimental_set_strategy(),` in addition to `strategy.scope()`.\r\n* `TensorRT`\r\n  * [TensorRT 6.0](https://developer.nvidia.com/tensorrt#tensorrt-whats-new) is now supported and enabled by default. This adds support for more TensorFlow ops including Conv3D, Conv3DBackpropInputV2, AvgPool3D, MaxPool3D, ResizeBilinear, and ResizeNearestNeighbor. In addition, the TensorFlow-TensorRT python conversion API is exported as `tf.experimental.tensorrt.Converter`.\r\n* Environment variable `TF_DETERMINISTIC_OPS` has been added. When set to \"true\" or \"1\", this environment variable makes `tf.nn.bias_add` operate deterministically (i.e. reproducibly), but currently only when XLA JIT compilation is *not* enabled. Setting `TF_DETERMINISTIC_OPS` to \"true\" or \"1\" also makes cuDNN convolution and max-pooling operate deterministically. This makes Keras Conv\\*D and MaxPool\\*D layers operate deterministically in both the forward and backward directions when running on a CUDA-enabled GPU.\r\n\r\n## Breaking Changes\r\n* Deletes `Operation.traceback_with_start_lines` for which we know of no usages.\r\n* Removed `id` from `tf.Tensor.__repr__()` as `id` is not useful other than internal debugging.\r\n* Some `tf.assert_*` methods now raise assertions at operation creation time if the input tensors' values are known at that time, not during the `session.run()`. This only changes behavior when the graph execution would have resulted in an error. When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).\r\n* The following APIs are not longer experimental: `tf.config.list_logical_devices`, `tf.config.list_physical_devices`, `tf.config.get_visible_devices`, `tf.config.set_visible_devices`, `tf.config.get_logical_device_configuration`, `tf.config.set_logical_device_configuration`.\r\n* `tf.config.experimentalVirtualDeviceConfiguration` has been renamed to `tf.config.LogicalDeviceConfiguration`.\r\n* `tf.config.experimental_list_devices` has been removed, please use\r\n`tf.config.list_logical_devices`.\r\n\r\n## Bug Fixes and Other Changes\r\n* `tf.data`\r\n  * Fixes concurrency issue with `tf.data.experimental.parallel_interleave` with `sloppy=True`.\r\n  * Add `tf.data.experimental.dense_to_ragged_batch()`.\r\n  * Extend `tf.data` parsing ops to support `RaggedTensors`.\r\n* `tf.distribute`\r\n  * Fix issue where GRU would crash or give incorrect output when a `tf.distribute.Strategy` was used. \r\n* `tf.estimator`\r\n  * Added option in `tf.estimator.CheckpointSaverHook` to not save the `GraphDef`.\r\n  * Moving the checkpoint reader from swig to pybind11.\r\n* `tf.keras`\r\n  * Export `depthwise_conv2d` in `tf.keras.backend`.\r\n  * In Keras Layers and Models, Variables in `trainable_weights`, `non_trainable_weights`, and `weights` are explicitly deduplicated.\r\n  * Keras `model.load_weights` now accepts `skip_mismatch` as an argument. This was available in external Keras, and has now been copied over to `tf.keras`.\r\n  * Fix the input shape caching behavior of Keras convolutional layers.\r\n  * `Model.fit_generator`, `Model.evaluate_generator`, `Model.predict_generator`, `Model.train_on_batch`, `Model.test_on_batch`, and `Model.predict_on_batch` methods now respect the `run_eagerly` property, and will correctly run using `tf.function` by default. Note that `Model.fit_generator`, `Model.evaluate_generator`, and `Model.predict_generator` are deprecated endpoints. They are subsumed by `Model.fit`, `Model.evaluate`, and `Model.predict` which now support generators and Sequences.\r\n* `tf.lite`\r\n  * Legalization for `NMS` ops in TFLite.\r\n  * add `narrow_range` and `axis` to `quantize_v2` and `dequantize` ops. \r\n  * Added support for `FusedBatchNormV3` in converter.\r\n  * Add an `errno`-like field to `NNAPI` delegate for detecting `NNAPI` errors for fallback behaviour.\r\n  * Refactors `NNAPI` Delegate to support detailed reason why an operation is not accelerated.\r\n  * Converts hardswish subgraphs into atomic ops.\r\n* Other\r\n  * Critical stability updates for TPUs, especially in cases where the XLA compiler produces compilation errors.\r\n  * TPUs can now be re-initialized multiple times, using `tf.tpu.experimental.initialize_tpu_system`. \r\n  * Add `RaggedTensor.merge_dims()`.\r\n  * Added new `uniform_row_length` row-partitioning tensor to `RaggedTensor`.\r\n  * Add `shape` arg to `RaggedTensor.to_tensor`; Improve speed of `RaggedTensor.to_tensor`.\r\n  * `tf.io.parse_sequence_example` and `tf.io.parse_single_sequence_example` now support ragged features.\r\n  * Fix `while_v2` with variables in custom gradient.\r\n  * Support taking gradients of V2 `tf.cond` and `tf.while_loop` using `LookupTable`.\r\n  * Fix bug where `vectorized_map` failed on inputs with unknown static shape.\r\n  * Add preliminary support for sparse CSR matrices.\r\n  * Tensor equality with `None` now behaves as expected.\r\n  * Make calls to `tf.function(f)()`, `tf.function(f).get_concrete_function` and `tf.function(f).get_initialization_function` thread-safe.\r\n  * Extend `tf.identity` to work with CompositeTensors (such as SparseTensor)\r\n  * Added more `dtypes` and zero-sized inputs to `Einsum` Op and improved its performance\r\n  * Enable multi-worker `NCCL` `all-reduce` inside functions executing eagerly.\r\n  * Added complex128 support to `RFFT`, `RFFT2D`, `RFFT3D`, `IRFFT`, `IRFFT2D`, and `IRFFT3D`.\r\n  * Add `pfor` converter for `SelfAdjointEigV2`.\r\n  * Add `tf.math.ndtri` and `tf.math.erfinv`.\r\n  * Add `tf.config.experimental.enable_mlir_bridge` to allow using MLIR compiler bridge in eager model.\r\n  * Added support for MatrixSolve on Cloud TPU / XLA.\r\n  * Added `tf.autodiff.ForwardAccumulator` for forward-mode autodiff\r\n  * Add `LinearOperatorPermutation`.\r\n  * A few performance optimizations on `tf.reduce_logsumexp`.\r\n  * Added multilabel handling to `AUC` metric\r\n  * Optimization on `zeros_like`.\r\n  * Dimension constructor now requires `None` or types with an `__index__` method.\r\n  * Add `tf.random.uniform` microbenchmark.\r\n  * Use `_protogen` suffix for proto library targets instead of `_cc_protogen` suffix.\r\n  * Moving the checkpoint reader from `swig` to `pybind11`.\r\n  * `tf.device` & `MirroredStrategy` now supports passing in a `tf.config.LogicalDevice`\r\n  * If you're building Tensorflow from source, consider using [bazelisk](https://github.com/bazelbuild/bazelisk) to automatically download and use the correct Bazel version. Bazelisk reads the `.bazelversion` file at the root of the project directory.\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n8bitmp3, Aaron Ma, Abd\u00fcLhamit Yilmaz, Abhai Kollara, aflc, Ag Ramesh, Albert Z. Guo, Alex Torres, amoitra, Andrii Prymostka, angeliand, Anshuman Tripathy, Anthony Barbier, Anton Kachatkou, Anubh-V, Anuja Jakhade, Artem Ryabov, autoih, Bairen Yi, Bas Aarts, Basit Ayantunde, Ben Barsdell, Bhavani Subramanian, Brett Koonce, candy.dc, Captain-Pool, caster, cathy, Chong Yan, Choong Yin Thong, Clayne Robison, Colle, Dan Ganea, David Norman, David Refaeli, dengziming, Diego Caballero, Divyanshu, djshen, Douman, Duncan Riach, EFanZh, Elena Zhelezina, Eric Schweitz, Evgenii Zheltonozhskii, Fei Hu, fo40225, Fred Reiss, Frederic Bastien, Fredrik Knutsson, fsx950223, fwcore, George Grzegorz Pawelczak, George Sterpu, Gian Marco Iodice, Giorgio Arena, giuros01, Gomathi Ramamurthy, Guozhong Zhuang, Haifeng Jin, Haoyu Wu, HarikrishnanBalagopal, HJYOO, Huang Chen-Yi, Ilham Firdausi Putra, Imran Salam, Jared Nielsen, Jason Zaman, Jasper Vicenti, Jeff Daily, Jeff Poznanovic, Jens Elofsson, Jerry Shih, jerryyin, Jesper Dramsch, jim.meyer, Jongwon Lee, Jun Wan, Junyuan Xie, Kaixi Hou, kamalkraj, Kan Chen, Karthik Muthuraman, Keiji Ariyama, Kevin Rose, Kevin Wang, Koan-Sin Tan, kstuedem, Kwabena W. Agyeman, Lakshay Tokas, latyas, Leslie-Fang-Intel, Li, Guizi, Luciano Resende, Lukas Folle, Lukas Geiger, Mahmoud Abuzaina, Manuel Freiberger, Mark Ryan, Martin Mlostek, Masaki Kozuki, Matthew Bentham, Matthew Denton, mbhuiyan, mdfaijul, Muhwan Kim, Nagy Mostafa, nammbash, Nathan Luehr, Nathan Wells, Niranjan Hasabnis, Oleksii Volkovskyi, Olivier Moindrot, olramde, Ouyang Jin, OverLordGoldDragon, Pallavi G, Paul Andrey, Paul Wais, pkanwar23, Pooya Davoodi, Prabindh Sundareson, Rajeshwar Reddy T, Ralovich, Kristof, Refraction-Ray, Richard Barnes, richardbrks, Robert Herbig, Romeo Kienzler, Ryan Mccormick, saishruthi, Saket Khandelwal, Sami Kama, Sana Damani, Satoshi Tanaka, Sergey Mironov, Sergii Khomenko, Shahid, Shawn Presser, ShengYang1, Siddhartha Bagaria, Simon Plovyt, skeydan, srinivasan.narayanamoorthy, Stephen Mugisha, sunway513, Takeshi Watanabe, Taylor Jakobson, TengLu, TheMindVirus, ThisIsIsaac, Tim Gates, Timothy Liu, Tomer Gafner, Trent Lo, Trevor Hickey, Trevor Morris, vcarpani, Wei Wang, Wen-Heng (Jack) Chung, wenshuai, Wenshuai-Xiaomi, wenxizhu, william, William D. Irons, Xinan Jiang, Yannic, Yasir Modak, Yasuhiro Matsumoto, Yong Tang, Yongfeng Gu, Youwei Song, Zaccharie Ramzi, Zhang, Zhenyu Guo, \u738b\u632f\u534e (Zhenhua Wang), \u97e9\u8463, \uc774\uc911\uac74 Isaac Lee", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.1.0-rc2", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.1.0-rc2", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0-rc2", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/22435583"}, {"tag_name": "v2.1.0-rc1", "name": "TensorFlow 2.1.0-rc1", "author_name": "tensorflow-jenkins", "body": "# Release 2.1.0-rc1\r\n\r\nTensorFlow 2.1 will be the last TF release supporting Python 2. Python 2 support [officially ends an January 1, 2020](https://www.python.org/dev/peps/pep-0373/#update). [As announced earlier](https://groups.google.com/a/tensorflow.org/d/msg/announce/gVwS5RC8mds/dCt1ka2XAAAJ), TensorFlow will also stop supporting Python 2 starting January 1, 2020, and no more releases are expected in 2019.\r\n\r\n## Major Features and Improvements\r\n* The `tensorflow` pip package now includes GPU support by default (same as `tensorflow-gpu`) for both Linux and Windows. This runs on machines with and without NVIDIA GPUs. `tensorflow-gpu` is still available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.\r\n* The `tensorflow` pip package is built with CUDA 10.1 and cuDNN 7.6.\r\n* `tf.keras`\r\n  * `Model.fit_generator`, `Model.evaluate_generator`, `Model.predict_generator`, `Model.train_on_batch`, `Model.test_on_batch`, and `Model.predict_on_batch` methods now respect the `run_eagerly` property, and will correctly run using tf.function by default.\r\n  * `Model.fit_generator`, `Model.evaluate_generator`, and `Model.predict_generator` are deprecated endpoints. They are subsumed by `Model.fit`, `Model.evaluate`, and `Model.predict` which now support generators and Sequences.\r\n  * Keras `.compile` `.fit` `.evaluate` and `.predict` are allowed to be outside of the DistributionStrategy scope, as long as the model was constructed inside of a scope.\r\n  * Keras `model.load_weights` now accepts `skip_mismatch` as an argument. This was available in external Keras, and has now been copied over to `tf.keras`.\r\n  * Introduced the `TextVectorization` layer, which takes as input raw strings and takes care of text standardization, tokenization, n-gram generation, and vocabulary indexing. See this [end-to-end text classification example](https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3). \r\n  * Experimental support for Keras `.compile`, `.fit`, `.evaluate`, and `.predict` is available for Cloud TPU Pods.\r\n  * Automatic outside compilation is now enabled for Cloud TPUs. This allows `tf.summary` to be used more conveniently with Cloud TPUs.\r\n  * Dynamic batch sizes with DistributionStrategy and Keras are supported on Cloud TPUs.\r\n  * Experimental support for mixed precision is available on GPUs and Cloud TPUs.\r\n  * Keras reference implementations for many popular models are available in the TensorFlow [Model Garden](https://github.com/tensorflow/models/tree/master/official).\r\n* `tf.data`\r\n  * Changes rebatching for `tf.data datasets` + distribution strategies for better performance.   Note that the dataset also behaves slightly differently, in that the rebatched dataset cardinality will always be a multiple of the number of replicas.\r\n* `TensorRT`\r\n  * [TensorRT 6.0](https://developer.nvidia.com/tensorrt#tensorrt-whats-new) is now supported and enabled by default. This adds support for more TensorFlow ops including Conv3D, Conv3DBackpropInputV2, AvgPool3D, MaxPool3D, ResizeBilinear, and ResizeNearestNeighbor. In addition, the TensorFlow-TensorRT python conversion API is exported as `tf.experimental.tensorrt.Converter`.\r\n* Environment variable `TF_DETERMINISTIC_OPS` has been added. When set to \"true\" or \"1\", this environment variable makes `tf.nn.bias_add` operate deterministically (i.e. reproducibly), but currently only when XLA JIT compilation is *not* enabled. Setting `TF_DETERMINISTIC_OPS` to \"true\" or \"1\" also makes cuDNN convolution and max-pooling operate deterministically. This makes Keras Conv\\*D and MaxPool\\*D layers operate deterministically in both the forward and backward directions when running on a CUDA-enabled GPU.\r\n\r\n## Breaking Changes\r\n* Deletes `Operation.traceback_with_start_lines` for which we know of no usages.\r\n* Removed `id` from `tf.Tensor.__repr__()` as `id` is not useful other than internal debugging.\r\n* Some `tf.assert_*` methods now raise assertions at operation creation time if the input tensors' values are known at that time, not during the `session.run()`. This only changes behavior when the graph execution would have resulted in an error. When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).\r\n* The following APIs are no longer experimental: `tf.config.list_logical_devices`, `tf.config.list_physical_devices`, `tf.config.get_visible_devices`, `tf.config.set_visible_devices`, `tf.config.get_logical_device_configuration`, `tf.config.set_logical_device_configuration`.\r\n* `tf.config.experimentalVirtualDeviceConfiguration` has been renamed to `tf.config.LogicalDeviceConfiguration`.\r\n* `tf.config.experimental_list_devices` has been removed, please use\r\n`tf.config.list_logical_devices`.\r\n\r\n## Bug Fixes and Other Changes\r\n* `tf.data`\r\n  * Fixes concurrency issue with `tf.data.experimental.parallel_interleave` with sloppy=True.\r\n  * Add `tf.data.experimental.dense_to_ragged_batch()`.\r\n  * Extend `tf.data` parsing ops to support `RaggedTensors`.\r\n* `tf.distribute`\r\n  * Fix issue where GRU would crash or give incorrect output when a `tf.distribute.Strategy` was used. \r\n* `tf.estimator`\r\n  * Added option in `tf.estimator.CheckpointSaverHook` to not save the `GraphDef`.\r\n  * Moving the checkpoint reader from swig to pybind11.\r\n* `tf.keras`\r\n  * Export depthwise_conv2d in `tf.keras.backend`.\r\n  * In Keras Layers and Models, Variables in `trainable_weights`, `non_trainable_weights`, and `weights` are explicitly deduplicated.\r\n  * Fix the incorrect stateful behavior of Keras convolutional layers.\r\n* `tf.lite`\r\n  * Legalization for `NMS` ops in TFLite.\r\n  * add `narrow_range` and `axis` to `quantize_v2` and `dequantize` ops. \r\n  * Added support for `FusedBatchNormV3` in converter.\r\n  * Add an `errno`-like field to `NNAPI` delegate for detecting `NNAPI` errors for fallback behaviour.\r\n  * Refactors `NNAPI` Delegate to support detailed reason why an operation is not accelerated.\r\n  * Converts hardswish subgraphs into atomic ops.\r\n* Other\r\n  * Add `RaggedTensor.merge_dims()`.\r\n  * Added new `uniform_row_length` row-partitioning tensor to `RaggedTensor`.\r\n  * Add `shape` arg to `RaggedTensor.to_tensor`; Improve speed of `RaggedTensor.to_tensor`.\r\n  * `tf.io.parse_sequence_example` and `tf.io.parse_single_sequence_example` now support ragged features.\r\n  * Fix `while_v2` with variables in custom gradient.\r\n  * Support taking gradients of V2 `tf.cond` and `tf.while_loop` using `LookupTable`.\r\n  * Fix bug where `vectorized_map` failed on inputs with unknown static shape.\r\n  * Add preliminary support for sparse CSR matrices.\r\n  * Tensor equality with `None` now behaves as expected.\r\n  * Make calls to `tf.function(f)()`, `tf.function(f).get_concrete_function` and `tf.function(f).get_initialization_function` thread-safe.\r\n  * Add `tf.debugging.enable_check_numerics()` and `tf.debugging.disable_check_numerics()` to facilitate debugging of numeric instability (`Infinity`s and `NaN`s) under eager mode and `tf.function`s.\r\n  * Extend `tf.identity` to work with CompositeTensors (such as SparseTensor)\r\n  * Added more `dtypes` and zero-sized inputs to `Einsum` Op and improved its performance\r\n  * Enable multi-worker `NCCL` `all-reduce` inside functions executing eagerly.\r\n  * Added complex128 support to `RFFT`, `RFFT2D`, `RFFT3D`, `IRFFT`, `IRFFT2D`, and `IRFFT3D`.\r\n  * Add `pfor` converter for `SelfAdjointEigV2`.\r\n  * Add `tf.math.ndtri` and `tf.math.erfinv`.\r\n  * Add `tf.config.experimental.enable_mlir_bridge` to allow using MLIR compiler bridge in eager model.\r\n  * Added support for MatrixSolve on Cloud TPU / XLA.\r\n  * Added `tf.autodiff.ForwardAccumulator` for forward-mode autodiff\r\n  * Add `LinearOperatorPermutation`.\r\n  * A few performance optimizations on `tf.reduce_logsumexp`.\r\n  * Added multilabel handling to `AUC` metric\r\n  * Optimization on `zeros_like`.\r\n  * Dimension constructor now requires `None` or types with an `__index__` method.\r\n  * Add `tf.random.uniform` microbenchmark.\r\n  * Use `_protogen` suffix for proto library targets instead of `_cc_protogen` suffix.\r\n  * Moving the checkpoint reader from `swig` to `pybind11`.\r\n  * `tf.device` & `MirroredStrategy` now supports passing in a `tf.config.LogicalDevice`\r\n  * If you're building Tensorflow from source, consider using [bazelisk](https://github.com/bazelisk/bazel) to automatically download and use the correct Bazel version. Bazelisk reads the `.bazelversion` file at the root of the project directory.\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n8bitmp3, Aaron Ma, Abd\u00fcLhamit Yilmaz, Abhai Kollara, aflc, Ag Ramesh, Albert Z. Guo, Alex Torres, amoitra, Andrii Prymostka, angeliand, Anshuman Tripathy, Anthony Barbier, Anton Kachatkou, Anubh-V, Anuja Jakhade, Artem Ryabov, autoih, Bairen Yi, Bas Aarts, Basit Ayantunde, Ben Barsdell, Bhavani Subramanian, Brett Koonce, candy.dc, Captain-Pool, caster, cathy, Chong Yan, Choong Yin Thong, Clayne Robison, Colle, Dan Ganea, David Norman, David Refaeli, dengziming, Diego Caballero, Divyanshu, djshen, Douman, Duncan Riach, EFanZh, Elena Zhelezina, Eric Schweitz, Evgenii Zheltonozhskii, Fei Hu, fo40225, Fred Reiss, Frederic Bastien, Fredrik Knutsson, fsx950223, fwcore, George Grzegorz Pawelczak, George Sterpu, Gian Marco Iodice, Giorgio Arena, giuros01, Gomathi Ramamurthy, Guozhong Zhuang, Haifeng Jin, Haoyu Wu, HarikrishnanBalagopal, HJYOO, Huang Chen-Yi, Ilham Firdausi Putra, Imran Salam, Jared Nielsen, Jason Zaman, Jasper Vicenti, Jeff Daily, Jeff Poznanovic, Jens Elofsson, Jerry Shih, jerryyin, Jesper Dramsch, jim.meyer, Jongwon Lee, Jun Wan, Junyuan Xie, Kaixi Hou, kamalkraj, Kan Chen, Karthik Muthuraman, Keiji Ariyama, Kevin Rose, Kevin Wang, Koan-Sin Tan, kstuedem, Kwabena W. Agyeman, Lakshay Tokas, latyas, Leslie-Fang-Intel, Li, Guizi, Luciano Resende, Lukas Folle, Lukas Geiger, Mahmoud Abuzaina, Manuel Freiberger, Mark Ryan, Martin Mlostek, Masaki Kozuki, Matthew Bentham, Matthew Denton, mbhuiyan, mdfaijul, Muhwan Kim, Nagy Mostafa, nammbash, Nathan Luehr, Nathan Wells, Niranjan Hasabnis, Oleksii Volkovskyi, Olivier Moindrot, olramde, Ouyang Jin, OverLordGoldDragon, Pallavi G, Paul Andrey, Paul Wais, pkanwar23, Pooya Davoodi, Prabindh Sundareson, Rajeshwar Reddy T, Ralovich, Kristof, Refraction-Ray, Richard Barnes, richardbrks, Robert Herbig, Romeo Kienzler, Ryan Mccormick, saishruthi, Saket Khandelwal, Sami Kama, Sana Damani, Satoshi Tanaka, Sergey Mironov, Sergii Khomenko, Shahid, Shawn Presser, ShengYang1, Siddhartha Bagaria, Simon Plovyt, skeydan, srinivasan.narayanamoorthy, Stephen Mugisha, sunway513, Takeshi Watanabe, Taylor Jakobson, TengLu, TheMindVirus, ThisIsIsaac, Tim Gates, Timothy Liu, Tomer Gafner, Trent Lo, Trevor Hickey, Trevor Morris, vcarpani, Wei Wang, Wen-Heng (Jack) Chung, wenshuai, Wenshuai-Xiaomi, wenxizhu, william, William D. Irons, Xinan Jiang, Yannic, Yasir Modak, Yasuhiro Matsumoto, Yong Tang, Yongfeng Gu, Youwei Song, Zaccharie Ramzi, Zhang, Zhenyu Guo, \u738b\u632f\u534e (Zhenhua Wang), \u97e9\u8463, \uc774\uc911\uac74 Isaac Lee", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.1.0-rc1", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.1.0-rc1", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0-rc1", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/22133126"}, {"tag_name": "v2.1.0-rc0", "name": "TensorFlow 2.1.0-rc0", "author_name": "tensorflow-jenkins", "body": "# Release 2.1.0\r\n\r\nTensorFlow 2.1 will be the last TF release supporting Python 2. Python 2 support [officially ends an January 1, 2020](https://www.python.org/dev/peps/pep-0373/#update). [As announced earlier](https://groups.google.com/a/tensorflow.org/d/msg/announce/gVwS5RC8mds/dCt1ka2XAAAJ), TensorFlow will also stop supporting Python 2 starting January 1, 2020, and no more releases are expected in 2019.\r\n\r\n## Major Features and Improvements\r\n* The `tensorflow` pip package now includes GPU support by default (same as `tensorflow-gpu`) for both Linux and Windows. This runs on machines with and without NVIDIA GPUs. `tensorflow-gpu` is still available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.\r\n* `tf.keras`\r\n  * `Model.fit_generator`, `Model.evaluate_generator`, `Model.predict_generator`, `Model.train_on_batch`, `Model.test_on_batch`, and `Model.predict_on_batch` methods now respect the `run_eagerly` property, and will correctly run using tf.function by default.\r\n  * `Model.fit_generator`, `Model.evaluate_generator`, and `Model.predict_generator` are deprecated endpoints. They are subsumed by `Model.fit`, `Model.evaluate`, and `Model.predict` which now support generators and Sequences.\r\n  * Keras `.compile` `.fit` `.evaluate` and `.predict` are allowed to be outside of the DistributionStrategy scope, as long as the model was constructed inside of a scope.\r\n  * Keras `model.load_weights` now accepts `skip_mismatch` as an argument. This was available in external Keras, and has now been copied over to `tf.keras`.\r\n  * Introduced the `TextVectorization` layer, which takes as input raw strings and takes care of text standardization, tokenization, n-gram generation, and vocabulary indexing. See this [end-to-end text classification example](https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3). \r\n  * Experimental support for Keras `.compile`, `.fit`, `.evaluate`, and `.predict` is available for Cloud TPU Pods.\r\n  * Automatic outside compilation is now enabled for Cloud TPUs. This allows `tf.summary` to be used more conveniently with Cloud TPUs.\r\n  * Dynamic batch sizes with DistributionStrategy and Keras are supported on Cloud TPUs.\r\n  * Experimental support for mixed precision is available on GPUs and Cloud TPUs.\r\n  * Keras reference implementations for many popular models are available in the TensorFlow [Model Garden](https://github.com/tensorflow/models/tree/master/official).\r\n* `tf.data`\r\n  * Changes rebatching for `tf.data datasets` + distribution strategies for better performance.   Note that the dataset also behaves slightly differently, in that the rebatched dataset cardinality will always be a multiple of the number of replicas.\r\n* `TensorRT`\r\n  * [TensorRT 6.0](https://developer.nvidia.com/tensorrt#tensorrt-whats-new) is now supported and enabled by default. This adds support for more TensorFlow ops including Conv3D, Conv3DBackpropInputV2, AvgPool3D, MaxPool3D, ResizeBilinear, and ResizeNearestNeighbor. In addition, the TensorFlow-TensorRT python conversion API is exported as `tf.experimental.tensorrt.Converter`.\r\n\r\n## Known issues\r\nBecause of [issues with building on windows](https://github.com/tensorflow/tensorflow/issues/10521), we turned off eigen strong inlining for the Windows builds. Windows binaries are expected to be slightly slower until the build issues are resolved. \r\n\r\n## Breaking Changes\r\n* Deletes `Operation.traceback_with_start_lines` for which we know of no usages.\r\n* Removed `id` from `tf.Tensor.__repr__()` as `id` is not useful other than internal debugging.\r\n* Some `tf.assert_*` methods now raise assertions at operation creation time if the input tensors' values are known at that time, not during the `session.run()`. This only changes behavior when the graph execution would have resulted in an error. When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).\r\n* The following APIs are not longer experimental: `tf.config.list_logical_devices`, `tf.config.list_physical_devices`, `tf.config.get_visible_devices`, `tf.config.set_visible_devices`, `tf.config.get_logical_device_configuration`, `tf.config.set_logical_device_configuration`.\r\n* `tf.config.experimentalVirtualDeviceConfiguration` has been renamed to `tf.config.LogicalDeviceConfiguration`.\r\n* `tf.config.experimental_list_devices` has been removed, please use\r\n`tf.config.list_logical_devices`.\r\n\r\n## Bug Fixes and Other Changes\r\n* `tf.data`\r\n  * Fixes concurrency issue with `tf.data.experimental.parallel_interleave` with sloppy=True.\r\n  * Add `tf.data.experimental.dense_to_ragged_batch()`.\r\n  * Extend `tf.data` parsing ops to support `RaggedTensors`.\r\n* `tf.distribute`\r\n  * Fix issue where GRU would crash or give incorrect output when a `tf.distribute.Strategy` was used. \r\n* `tf.estimator`\r\n  * Added option in `tf.estimator.CheckpointSaverHook` to not save the `GraphDef`.\r\n* `tf.keras`\r\n  * Export depthwise_conv2d in `tf.keras.backend`.\r\n  * In Keras Layers and Models, Variables in `trainable_weights`, `non_trainable_weights`, and `weights` are explicitly deduplicated.\r\n  * Fix the incorrect stateful behavior of Keras convolutional layers.\r\n* `tf.lite`\r\n  * Legalization for `NMS` ops in TFLite.\r\n  * add `narrow_range` and `axis` to `quantize_v2` and `dequantize` ops. \r\n  * Added support for `FusedBatchNormV3` in converter.\r\n  * Add an `errno`-like field to `NNAPI` delegate for detecting `NNAPI` errors for fallback behaviour.\r\n  * Refactors `NNAPI` Delegate to support detailed reason why an operation is not accelerated.\r\n  * Converts hardswish subgraphs into atomic ops.\r\n* Other\r\n  * Add `RaggedTensor.merge_dims()`.\r\n  * Added new `uniform_row_length` row-partitioning tensor to `RaggedTensor`.\r\n  * Add `shape` arg to `RaggedTensor.to_tensor`; Improve speed of `RaggedTensor.to_tensor`.\r\n  * `tf.io.parse_sequence_example` and `tf.io.parse_single_sequence_example` now support ragged features.\r\n  * Fix `while_v2` with variables in custom gradient.\r\n  * Support taking gradients of V2 `tf.cond` and `tf.while_loop` using `LookupTable`.\r\n  * Fix bug where `vectorized_map` failed on inputs with unknown static shape.\r\n  * Add preliminary support for sparse CSR matrices.\r\n  * Tensor equality with `None` now behaves as expected.\r\n  * Make calls to `tf.function(f)()`, `tf.function(f).get_concrete_function` and `tf.function(f).get_initialization_function` thread-safe.\r\n  * Add `tf.debugging.enable_check_numerics()` and `tf.debugging.disable_check_numerics()` to facilitate debugging of numeric instability (`Infinity`s and `NaN`s) under eager mode and `tf.function`s.\r\n  * Extend `tf.identity` to work with CompositeTensors (such as SparseTensor)\r\n  * Added more `dtypes` and zero-sized inputs to `Einsum` Op and improved its performance\r\n  * Enable multi-worker `NCCL` `all-reduce` inside functions executing eagerly.\r\n  * Added complex128 support to `RFFT`, `RFFT2D`, `RFFT3D`, `IRFFT`, `IRFFT2D`, and `IRFFT3D`.\r\n  * Add `pfor` converter for `SelfAdjointEigV2`.\r\n  * Add `tf.math.ndtri` and `tf.math.erfinv`.\r\n  * Add `tf.config.experimental.enable_mlir_bridge` to allow using MLIR compiler bridge in eager model.\r\n  * Added support for MatrixSolve on Cloud TPU / XLA.\r\n  * Added `tf.autodiff.ForwardAccumulator` for forward-mode autodiff\r\n  * Add `LinearOperatorPermutation`.\r\n  * A few performance optimizations on `tf.reduce_logsumexp`.\r\n  * Added multilabel handling to `AUC` metric\r\n  * Optimization on `zeros_like`.\r\n  * Dimension constructor now requires `None` or types with an `__index__` method.\r\n  * Add `tf.random.uniform` microbenchmark.\r\n  * Use `_protogen` suffix for proto library targets instead of `_cc_protogen` suffix.\r\n  * Moving the checkpoint reader from `swig` to `pybind11`.\r\n  * `tf.device` & `MirroredStrategy` now supports passing in a `tf.config.LogicalDevice`\r\n  * If you're building Tensorflow from source, consider using [bazelisk](https://github.com/bazelisk/bazel) to automatically download and use the correct Bazel version. Bazelisk reads the `.bazelversion` file at the root of the project directory.\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n8bitmp3, Aaron Ma, Abd\u00fcLhamit Yilmaz, Abhai Kollara, aflc, Ag Ramesh, Albert Z. Guo, Alex Torres, amoitra, Andrii Prymostka, angeliand, Anshuman Tripathy, Anthony Barbier, Anton Kachatkou, Anubh-V, Anuja Jakhade, Artem Ryabov, autoih, Bairen Yi, Bas Aarts, Basit Ayantunde, Ben Barsdell, Bhavani Subramanian, Brett Koonce, candy.dc, Captain-Pool, caster, cathy, Chong Yan, Choong Yin Thong, Clayne Robison, Colle, Dan Ganea, David Norman, David Refaeli, dengziming, Diego Caballero, Divyanshu, djshen, Douman, Duncan Riach, EFanZh, Elena Zhelezina, Eric Schweitz, Evgenii Zheltonozhskii, Fei Hu, fo40225, Fred Reiss, Frederic Bastien, Fredrik Knutsson, fsx950223, fwcore, George Grzegorz Pawelczak, George Sterpu, Gian Marco Iodice, Giorgio Arena, giuros01, Gomathi Ramamurthy, Guozhong Zhuang, Haifeng Jin, Haoyu Wu, HarikrishnanBalagopal, HJYOO, Huang Chen-Yi, Ilham Firdausi Putra, Imran Salam, Jared Nielsen, Jason Zaman, Jasper Vicenti, Jeff Daily, Jeff Poznanovic, Jens Elofsson, Jerry Shih, jerryyin, Jesper Dramsch, jim.meyer, Jongwon Lee, Jun Wan, Junyuan Xie, Kaixi Hou, kamalkraj, Kan Chen, Karthik Muthuraman, Keiji Ariyama, Kevin Rose, Kevin Wang, Koan-Sin Tan, kstuedem, Kwabena W. Agyeman, Lakshay Tokas, latyas, Leslie-Fang-Intel, Li, Guizi, Luciano Resende, Lukas Folle, Lukas Geiger, Mahmoud Abuzaina, Manuel Freiberger, Mark Ryan, Martin Mlostek, Masaki Kozuki, Matthew Bentham, Matthew Denton, mbhuiyan, mdfaijul, Muhwan Kim, Nagy Mostafa, nammbash, Nathan Luehr, Nathan Wells, Niranjan Hasabnis, Oleksii Volkovskyi, Olivier Moindrot, olramde, Ouyang Jin, OverLordGoldDragon, Pallavi G, Paul Andrey, Paul Wais, pkanwar23, Pooya Davoodi, Prabindh Sundareson, Rajeshwar Reddy T, Ralovich, Kristof, Refraction-Ray, Richard Barnes, richardbrks, Robert Herbig, Romeo Kienzler, Ryan Mccormick, saishruthi, Saket Khandelwal, Sami Kama, Sana Damani, Satoshi Tanaka, Sergey Mironov, Sergii Khomenko, Shahid, Shawn Presser, ShengYang1, Siddhartha Bagaria, Simon Plovyt, skeydan, srinivasan.narayanamoorthy, Stephen Mugisha, sunway513, Takeshi Watanabe, Taylor Jakobson, TengLu, TheMindVirus, ThisIsIsaac, Tim Gates, Timothy Liu, Tomer Gafner, Trent Lo, Trevor Hickey, Trevor Morris, vcarpani, Wei Wang, Wen-Heng (Jack) Chung, wenshuai, Wenshuai-Xiaomi, wenxizhu, william, William D. Irons, Xinan Jiang, Yannic, Yasir Modak, Yasuhiro Matsumoto, Yong Tang, Yongfeng Gu, Youwei Song, Zaccharie Ramzi, Zhang, Zhenyu Guo, \u738b\u632f\u534e (Zhenhua Wang), \u97e9\u8463, \uc774\uc911\uac74 Isaac Lee", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.1.0-rc0", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.1.0-rc0", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.1.0-rc0", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/21808828"}, {"tag_name": "v1.15.0", "name": "TensorFlow 1.15.0", "author_name": "goldiegadde", "body": "# Release 1.15.0\r\nThis is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year.\r\n\r\n## Major Features and Improvements\r\n* As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.\r\n* TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2` module. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1` module. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.\r\nThis enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.\r\n* `EagerTensor` now supports numpy buffer interface for tensors.\r\n* Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.\r\n* Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.\r\n* AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.\r\n* Adds `enable_tensor_equality()`, which switches the behavior such that: \r\n  * Tensors are no longer hashable.\r\n  * Tensors can be compared with `==` and `!=`, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.\r\n* Auto Mixed-Precision graph optimizer simplifies converting models to `float16` for acceleration on Volta and Turing Tensor Cores. This feature can be enabled by wrapping an optimizer class with `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.\r\n* Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to \"true\" or \"1\" forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic.\r\n* TensorRT\r\n  * Migrate TensorRT conversion sources from contrib to compiler directory in preparation for TF 2.0.\r\n  * Add additional, user friendly `TrtGraphConverter` API for TensorRT conversion.\r\n  * Expand support for TensorFlow operators in TensorRT conversion (e.g.\r\n    `Gather`, `Slice`, `Pack`, `Unpack`, `ArgMin`, `ArgMax`,`DepthSpaceShuffle`). \r\n  * Support TensorFlow operator `CombinedNonMaxSuppression` in TensorRT conversion which \r\n     significantly accelerates object detection models.\r\n\r\n## Breaking Changes\r\n* Tensorflow code now produces 2 different pip packages: `tensorflow_core` containing all the code (in the future it will contain only the private implementation) and `tensorflow` which is a virtual pip package doing forwarding to `tensorflow_core` (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.\r\n* TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.\r\n* Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.\r\n* `tf.keras`:\r\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.\r\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.\r\n  * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.\r\n  * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer \"layer-name\" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.\r\n  * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).\r\n\r\n## Bug Fixes and Other Changes\r\n* `tf.estimator`:\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * Fix tests in canned estimators.\r\n  * Expose Head as public API.\r\n  * Fixes critical bugs that help with `DenseFeatures` usability in TF2\r\n* `tf.data`:\r\n  * Promoting `unbatch` from experimental to core API.\r\n  * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.\r\n* `tf.keras`:\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to tf.train.Checkpoint format, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.\r\n  * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.\r\n  * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.\r\n  * Enable the Keras compile API `experimental_run_tf_function` flag by default. This flag enables single training/eval/predict execution path. With this 1. All input types are converted to `Dataset`. 2. When distribution strategy is not specified this goes through the no-op distribution strategy path. 3. Execution is wrapped in tf.function unless `run_eagerly=True` is set in compile.\r\n  * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.\r\n* `tf.lite`\r\n  * Add `GATHER` support to NN API delegate.\r\n  * tflite object detection script has a debug mode.\r\n  * Add delegate support for `QUANTIZE`.\r\n  * Added evaluation script for COCO minival.\r\n  * Add delegate support for `QUANTIZED_16BIT_LSTM`.\r\n  * Converts hardswish subgraphs into atomic ops.\r\n* Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\r\n* `parallel_for`: Add converter for `MatrixDiag`.\r\n* Add `narrow_range` attribute to `QuantizeAndDequantizeV2` and V3.\r\n* Added new op: `tf.strings.unsorted_segment_join`.\r\n* Add HW acceleration support for `topK_v2`.\r\n* Add new `TypeSpec` classes.\r\n* CloudBigtable version updated to v0.10.0.\r\n* Expose `Head` as public API.\r\n* Update docstring for gather to properly describe the non-empty `batch_dims` case.\r\n* Added `tf.sparse.from_dense` utility function.\r\n* Improved ragged tensor support in `TensorFlowTestCase`.\r\n* Makes the a-normal form transformation in Pyct configurable as to which nodes are converted to variables and which are not.\r\n* `ResizeInputTensor` now works for all delegates.\r\n* Add `EXPAND_DIMS` support to NN API delegate TEST:  expand_dims_test\r\n* `tf.cond` emits a StatelessIf op if the branch functions are stateless and do not touch any resources.\r\n* `tf.cond`, `tf.while` and `if` and `while` in AutoGraph now accept a nonscalar predicate if has a single element. This does not affect non-V2 control flow.\r\n* `tf.while_loop` emits a StatelessWhile op if the cond and body functions are stateless and do not touch any resources.\r\n* Refactors code in Quant8 LSTM support to reduce TFLite binary size.\r\n* Add support of local soft device placement for eager op.\r\n* Add HW acceleration support for `LogSoftMax`.\r\n* Added a function `nested_value_rowids` for ragged tensors.\r\n* Add guard to avoid acceleration of L2 Normalization with input rank != 4\r\n* Add `tf.math.cumulative_logsumexp operation`.\r\n* Add `tf.ragged.stack`.\r\n* Fix memory allocation problem when calling `AddNewInputConstantTensor`.\r\n* Delegate application failure leaves interpreter in valid state.\r\n* Add check for correct memory alignment to `MemoryAllocation::MemoryAllocation()`.\r\n* Extracts `NNAPIDelegateKernel` from nnapi_delegate.cc\r\n* Added support for `FusedBatchNormV3` in converter.\r\n* A ragged to dense op for directly calculating tensors.\r\n* Fix accidental quadratic graph construction cost in graph-mode `tf.gradients()`.\r\n* The `precision_mode` argument to `TrtGraphConverter` is now case insensitive.\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\na6802739, Aaron Ma, Abdullah Selek, Abolfazl Shahbazi, Ag Ramesh, Albert Z. Guo, Albin Joy, Alex Itkes, Alex Sergeev, Alexander Pivovarov, Alexey Romanov, alhkad, Amit Srivastava, amoitra, Andrew Lihonosov, Andrii Prymostka, Anuj Rawat, Astropeak, Ayush Agrawal, Bairen Yi, Bas Aarts, Bastian Eichenberger, Ben Barsdell, Benjamin Peterson, bhack, Bharat Raghunathan, Bhavani Subramanian, Bryan Cutler, candy.dc, Cao Zongyan, Captain-Pool, Casper Da Costa-Luis, Chen Guoyin, Cheng Chang, chengchingwen, Chong Yan, Choong Yin Thong, Christopher Yeh, Clayne Robison, Coady, Patrick, Dan Ganea, David Norman, Denis Khalikov, Deven Desai, Diego Caballero, Duncan Dean, Duncan Riach, Dwight J Lyle, Eamon Ito-Fisher, eashtian3, EFanZh, ejot, Elroy Ashtian Jr, Eric Schweitz, Fangjun Kuang, Fei Hu, fo40225, formath, Fred Reiss, Frederic Bastien, Fredrik Knutsson, G. Hussain Chinoy, Gabriel, gehring, George Grzegorz Pawelczak, Gianluca Varisco, Gleb Popov, Greg Peatfield, Guillaume Klein, Gurpreet Singh, Gustavo Lima Chaves, haison, Haraldur T\u00f3Mas Hallgr\u00edMsson, HarikrishnanBalagopal, H\u00e5Kon Sandsmark, I-Hong, Ilham Firdausi Putra, Imran Salam, Jason Zaman, Jason Zavaglia, jayhpark530, jefby, Jeff Daily, Jeffrey Poznanovic, Jekyll Lai, Jeroen B\u00e9Dorf, Jerry Shih, jerryyin, jiakai, JiangXIAO, Joe Bowser, Joel Shapiro, Johan Gunnarsson, Jojimon Varghese, Joon, Josh Beal, Julian Niedermeier, Jun Wan, Junqin Zhang, Junyuan Xie, Justin Tunis, Kaixi Hou, Karl Lessard, Karthik Muthuraman, Kbhute-Ibm, khanhlvg, Koock Yoon, kstuedem, Kyuwon Kim, Lakshay Tokas, leike666666, leonard951, Leslie-Fang, Leslie-Fang-Intel, Li, Guizi, Lukas Folle, Lukas Geiger, Mahmoud Abuzaina, Manraj Singh Grover, Margaret Maynard-Reid, Mark Ryan, Matt Conley, Matthew Bentham, Matthew Denton, mbhuiyan, mdfaijul, Mei Jie, merturl, MichaelKonobeev, Michal W. Tarnowski, minds, mpppk, musikisomorphie, Nagy Mostafa, Nayana Thorat, Neil, Niels Ole Salscheider, Niklas Silfverstr\u00f6M, Niranjan Hasabnis, ocjosen, olramde, Pariksheet Pinjari, Patrick J. Lopresti, Patrik Gustavsson, per1234, PeterLee, Phan Van Nguyen Duc, Phillip Kravtsov, Pooya Davoodi, Pranav Marathe, Putra Manggala, Qingqing Cao, Rajeshwar Reddy T, Ramon Vi\u00f1As, Rasmus Diederichsen, Reuben Morais, richardbrks, robert, RonLek, Ryan Jiang, saishruthi, Saket Khandelwal, Saleem Abdulrasool, Sami Kama, Sana-Damani, Sergii Khomenko, Severen Redwood, Shubham Goyal, Sigrid Keydana, Siju Samuel, sleighsoft, smilu97, Son Tran, Srini511, srinivasan.narayanamoorthy, Sumesh Udayakumaran, Sungmann Cho, Tae-Hwan Jung, Taehoon Lee, Takeshi Watanabe, TengLu, terryky, TheMindVirus, ThisIsIsaac, Till Hoffmann, Timothy Liu, Tomer Gafner, Tongxuan Liu, Trent Lo, Trevor Morris, Uday Bondhugula, Vasileios Lioutas, vbvg2008, Vishnuvardhan Janapati, Vivek Suryamurthy, Wei Wang, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, winstonq, wyzhao, Xiaoming (Jason) Cui, Xinan Jiang, Xinping Wang, Yann-Yy, Yasir Modak, Yong Tang, Yongfeng Gu, Yuchen Ying, Yuxin Wu, zyeric, \u738b\u632f\u534e (Zhenhua Wang)", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.15.0", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.15.0", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.15.0", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/20750483"}, {"tag_name": "v1.15.0-rc3", "name": "TensorFlow 1.15.0-rc3", "author_name": "goldiegadde", "body": "# Release 1.15.0-rc3\r\nThis is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year. \r\n\r\n## Major Features and Improvements\r\n* As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.\r\n* TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2 module`. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1 module`. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.\r\nThis enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.\r\n* EagerTensor now supports numpy buffer interface for tensors.\r\n* Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.\r\n* Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.\r\n* AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.\r\n* Adds `enable_tensor_equality()`, which switches the behavior such that: \r\n  * Tensors are no longer hashable.\r\n  * Tensors can be compared with == and !=, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.\r\n* Auto Mixed-Precision graph optimizer simplifies converting models to float16 for acceleration on Volta and Turing Tensor Cores. This feature can be enabled by wrapping an optimizer class with `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.\r\n* Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to \"true\" or \"1\" forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic.\r\n* TensorRT\r\n  * Migrate TensorRT conversion sources from contrib to compiler directory in preparation for TF 2.0.\r\n  * Add additional, user friendly `TrtGraphConverter` API for TensorRT conversion.\r\n  * Expand support for TensorFlow operators in TensorRT conversion (e.g.\r\n    `Gather`, `Slice`, `Pack`, `Unpack`, `ArgMin`, `ArgMax`,`DepthSpaceShuffle`). \r\n  * Support TensorFlow operator `CombinedNonMaxSuppression` in TensorRT conversion which \r\n     significantly accelerates object detection models.\r\n\r\n## Breaking Changes\r\n* Tensorflow code now produces 2 different pip packages: tensorflow_core containing all the code (in the future it will contain only the private implementation) and tensorflow which is a virtual pip package doing forwarding to tensorflow_core (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.\r\n* TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.\r\n* Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.\r\n* `tf.keras`:\r\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.\r\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.\r\n  * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.\r\n  * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer \"layer-name\" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.\r\n  * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).\r\n\r\n## Bug Fixes and Other Changes\r\n* `tf.estimator`:\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * Fix tests in canned estimators.\r\n  * Expose Head as public API.\r\n  * Fixes critical bugs that help with `DenseFeatures` usability in TF2\r\n* `tf.data`:\r\n  * Promoting `unbatch` from experimental to core API.\r\n  * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.\r\n* `tf.keras`:\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to tf.train.Checkpoint format, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.\r\n  * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.\r\n  * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.\r\n  * Enable the Keras compile API `experimental_run_tf_function` flag by default. This flag enables single training/eval/predict execution path. With this 1. All input types are converted to `Dataset`. 2. When distribution strategy is not specified this goes through the no-op distribution strategy path. 3. Execution is wrapped in tf.function unless `run_eagerly=True` is set in compile.\r\n  * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.\r\n* `tf.lite`\r\n  * Add `GATHER` support to NN API delegate.\r\n  * tflite object detection script has a debug mode.\r\n  * Add delegate support for `QUANTIZE`.\r\n  * Added evaluation script for COCO minival.\r\n  * Add delegate support for `QUANTIZED_16BIT_LSTM`.\r\n  * Converts hardswish subgraphs into atomic ops.\r\n* Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\r\n* `parallel_for`: Add converter for `MatrixDiag`.\r\n* Add `narrow_range` attribute to `QuantizeAndDequantizeV2` and V3.\r\n* Added new op: `tf.strings.unsorted_segment_join`.\r\n* Add HW acceleration support for `topK_v2`.\r\n* Add new `TypeSpec` classes.\r\n* CloudBigtable version updated to v0.10.0.\r\n* Expose `Head` as public API.\r\n* Update docstring for gather to properly describe the non-empty `batch_dims` case.\r\n* Added `tf.sparse.from_dense` utility function.\r\n* Improved ragged tensor support in `TensorFlowTestCase`.\r\n* Makes the a-normal form transformation in Pyct configurable as to which nodes are converted to variables and which are not.\r\n* `ResizeInputTensor` now works for all delegates.\r\n* Add `EXPAND_DIMS` support to NN API delegate TEST:  expand_dims_test\r\n* `tf.cond` emits a StatelessIf op if the branch functions are stateless and do not touch any resources.\r\n* `tf.cond`, `tf.while` and `if` and `while` in AutoGraph now accept a nonscalar predicate if has a single element. This does not affect non-V2 control flow.\r\n* `tf.while_loop` emits a StatelessWhile op if the cond and body functions are stateless and do not touch any resources.\r\n* Refactors code in Quant8 LSTM support to reduce TFLite binary size.\r\n* Add support of local soft device placement for eager op.\r\n* Add HW acceleration support for `LogSoftMax`.\r\n* Added a function `nested_value_rowids` for ragged tensors.\r\n* Add guard to avoid acceleration of L2 Normalization with input rank != 4\r\n* Add `tf.math.cumulative_logsumexp operation`.\r\n* Add `tf.ragged.stack`.\r\n* Fix memory allocation problem when calling `AddNewInputConstantTensor`.\r\n* Delegate application failure leaves interpreter in valid state.\r\n* Add check for correct memory alignment to `MemoryAllocation::MemoryAllocation()`.\r\n* Extracts `NNAPIDelegateKernel` from nnapi_delegate.cc\r\n* Added support for `FusedBatchNormV3` in converter.\r\n* A ragged to dense op for directly calculating tensors.\r\n* Fix accidental quadratic graph construction cost in graph-mode `tf.gradients()`.\r\n* The `precision_mode` argument to `TrtGraphConverter` is now case insensitive.\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\na6802739, Aaron Ma, Abdullah Selek, Abolfazl Shahbazi, Ag Ramesh, Albert Z. Guo, Albin Joy, Alex Itkes, Alex Sergeev, Alexander Pivovarov, Alexey Romanov, alhkad, Amit Srivastava, amoitra, Andrew Lihonosov, Andrii Prymostka, Anuj Rawat, Astropeak, Ayush Agrawal, Bairen Yi, Bas Aarts, Bastian Eichenberger, Ben Barsdell, Benjamin Peterson, bhack, Bharat Raghunathan, Bhavani Subramanian, Bryan Cutler, candy.dc, Cao Zongyan, Captain-Pool, Casper Da Costa-Luis, Chen Guoyin, Cheng Chang, chengchingwen, Chong Yan, Choong Yin Thong, Christopher Yeh, Clayne Robison, Coady, Patrick, Dan Ganea, David Norman, Denis Khalikov, Deven Desai, Diego Caballero, Duncan Dean, Duncan Riach, Dwight J Lyle, Eamon Ito-Fisher, eashtian3, EFanZh, ejot, Elroy Ashtian Jr, Eric Schweitz, Fangjun Kuang, Fei Hu, fo40225, formath, Fred Reiss, Frederic Bastien, Fredrik Knutsson, G. Hussain Chinoy, Gabriel, gehring, George Grzegorz Pawelczak, Gianluca Varisco, Gleb Popov, Greg Peatfield, Guillaume Klein, Gurpreet Singh, Gustavo Lima Chaves, haison, Haraldur T\u00f3Mas Hallgr\u00edMsson, HarikrishnanBalagopal, H\u00e5Kon Sandsmark, I-Hong, Ilham Firdausi Putra, Imran Salam, Jason Zaman, Jason Zavaglia, jayhpark530, jefby, Jeff Daily, Jeffrey Poznanovic, Jekyll Lai, Jeroen B\u00e9Dorf, Jerry Shih, jerryyin, jiakai, JiangXIAO, Joe Bowser, Joel Shapiro, Johan Gunnarsson, Jojimon Varghese, Joon, Josh Beal, Julian Niedermeier, Jun Wan, Junqin Zhang, Junyuan Xie, Justin Tunis, Kaixi Hou, Karl Lessard, Karthik Muthuraman, Kbhute-Ibm, khanhlvg, Koock Yoon, kstuedem, Kyuwon Kim, Lakshay Tokas, leike666666, leonard951, Leslie-Fang, Leslie-Fang-Intel, Li, Guizi, Lukas Folle, Lukas Geiger, Mahmoud Abuzaina, Manraj Singh Grover, Margaret Maynard-Reid, Mark Ryan, Matt Conley, Matthew Bentham, Matthew Denton, mbhuiyan, mdfaijul, Mei Jie, merturl, MichaelKonobeev, Michal W. Tarnowski, minds, mpppk, musikisomorphie, Nagy Mostafa, Nayana Thorat, Neil, Niels Ole Salscheider, Niklas Silfverstr\u00f6M, Niranjan Hasabnis, ocjosen, olramde, Pariksheet Pinjari, Patrick J. Lopresti, Patrik Gustavsson, per1234, PeterLee, Phan Van Nguyen Duc, Phillip Kravtsov, Pooya Davoodi, Pranav Marathe, Putra Manggala, Qingqing Cao, Rajeshwar Reddy T, Ramon Vi\u00f1As, Rasmus Diederichsen, Reuben Morais, richardbrks, robert, RonLek, Ryan Jiang, saishruthi, Saket Khandelwal, Saleem Abdulrasool, Sami Kama, Sana-Damani, Sergii Khomenko, Severen Redwood, Shubham Goyal, Sigrid Keydana, Siju Samuel, sleighsoft, smilu97, Son Tran, Srini511, srinivasan.narayanamoorthy, Sumesh Udayakumaran, Sungmann Cho, Tae-Hwan Jung, Taehoon Lee, Takeshi Watanabe, TengLu, terryky, TheMindVirus, ThisIsIsaac, Till Hoffmann, Timothy Liu, Tomer Gafner, Tongxuan Liu, Trent Lo, Trevor Morris, Uday Bondhugula, Vasileios Lioutas, vbvg2008, Vishnuvardhan Janapati, Vivek Suryamurthy, Wei Wang, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, winstonq, wyzhao, Xiaoming (Jason) Cui, Xinan Jiang, Xinping Wang, Yann-Yy, Yasir Modak, Yong Tang, Yongfeng Gu, Yuchen Ying, Yuxin Wu, zyeric, \u738b\u632f\u534e (Zhenhua Wang)", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.15.0-rc3", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.15.0-rc3", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.15.0-rc3", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/20554260"}, {"tag_name": "v1.15.0-rc2", "name": "TensorFlow 1.15.0-rc2", "author_name": "goldiegadde", "body": "# Release 1.15.0-rc2\r\nThis is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year. \r\n\r\n## Major Features and Improvements\r\n* As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.\r\n* TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2 module`. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1 module`. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.\r\nThis enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.\r\n* EagerTensor now supports numpy buffer interface for tensors.\r\n* Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.\r\n* Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.\r\n* AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.\r\n* Adds `enable_tensor_equality()`, which switches the behavior such that: \r\n  * Tensors are no longer hashable.\r\n  * Tensors can be compared with == and !=, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.\r\n* Auto Mixed-Precision graph optimizer simplifies converting models to float16 for acceleration on Volta and Turing Tensor Cores. This feature can be enabled by wrapping an optimizer class with `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.\r\n* Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to \"true\" or \"1\" forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic.\r\n* TensorRT\r\n  * Migrate TensorRT conversion sources from contrib to compiler directory in preparation for TF 2.0.\r\n  * Add additional, user friendly `TrtGraphConverter` API for TensorRT conversion.\r\n  * Expand support for TensorFlow operators in TensorRT conversion (e.g.\r\n    `Gather`, `Slice`, `Pack`, `Unpack`, `ArgMin`, `ArgMax`,`DepthSpaceShuffle`). \r\n  * Support TensorFlow operator `CombinedNonMaxSuppression` in TensorRT conversion which \r\n     significantly accelerates object detection models.\r\n\r\n## Breaking Changes\r\n* Tensorflow code now produces 2 different pip packages: tensorflow_core containing all the code (in the future it will contain only the private implementation) and tensorflow which is a virtual pip package doing forwarding to tensorflow_core (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.\r\n* TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.\r\n* Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.\r\n* `tf.keras`:\r\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.\r\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.\r\n  * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.\r\n  * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer \"layer-name\" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.\r\n  * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).\r\n\r\n## Bug Fixes and Other Changes\r\n* `tf.estimator`:\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * Fix tests in canned estimators.\r\n  * Expose Head as public API.\r\n  * Fixes critical bugs that help with `DenseFeatures` usability in TF2\r\n* `tf.data`:\r\n  * Promoting `unbatch` from experimental to core API.\r\n  * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.\r\n* `tf.keras`:\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to tf.train.Checkpoint format, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.\r\n  * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.\r\n  * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.\r\n  * Enable the Keras compile API `experimental_run_tf_function` flag by default. This flag enables single training/eval/predict execution path. With this 1. All input types are converted to `Dataset`. 2. When distribution strategy is not specified this goes through the no-op distribution strategy path. 3. Execution is wrapped in tf.function unless `run_eagerly=True` is set in compile.\r\n  * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.\r\n* `tf.lite`\r\n  * Add `GATHER` support to NN API delegate.\r\n  * tflite object detection script has a debug mode.\r\n  * Add delegate support for `QUANTIZE`.\r\n  * Added evaluation script for COCO minival.\r\n  * Add delegate support for `QUANTIZED_16BIT_LSTM`.\r\n  * Converts hardswish subgraphs into atomic ops.\r\n* Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\r\n* `parallel_for`: Add converter for `MatrixDiag`.\r\n* Add `narrow_range` attribute to `QuantizeAndDequantizeV2` and V3.\r\n* Added new op: `tf.strings.unsorted_segment_join`.\r\n* Add HW acceleration support for `topK_v2`.\r\n* Add new `TypeSpec` classes.\r\n* CloudBigtable version updated to v0.10.0.\r\n* Expose `Head` as public API.\r\n* Update docstring for gather to properly describe the non-empty `batch_dims` case.\r\n* Added `tf.sparse.from_dense` utility function.\r\n* Improved ragged tensor support in `TensorFlowTestCase`.\r\n* Makes the a-normal form transformation in Pyct configurable as to which nodes are converted to variables and which are not.\r\n* `ResizeInputTensor` now works for all delegates.\r\n* Add `EXPAND_DIMS` support to NN API delegate TEST:  expand_dims_test\r\n* `tf.cond` emits a StatelessIf op if the branch functions are stateless and do not touch any resources.\r\n* `tf.cond`, `tf.while` and `if` and `while` in AutoGraph now accept a nonscalar predicate if has a single element. This does not affect non-V2 control flow.\r\n* `tf.while_loop` emits a StatelessWhile op if the cond and body functions are stateless and do not touch any resources.\r\n* Refactors code in Quant8 LSTM support to reduce TFLite binary size.\r\n* Add support of local soft device placement for eager op.\r\n* Add HW acceleration support for `LogSoftMax`.\r\n* Added a function `nested_value_rowids` for ragged tensors.\r\n* Add guard to avoid acceleration of L2 Normalization with input rank != 4\r\n* Add `tf.math.cumulative_logsumexp operation`.\r\n* Add `tf.ragged.stack`.\r\n* Fix memory allocation problem when calling `AddNewInputConstantTensor`.\r\n* Delegate application failure leaves interpreter in valid state.\r\n* Add check for correct memory alignment to `MemoryAllocation::MemoryAllocation()`.\r\n* Extracts `NNAPIDelegateKernel` from nnapi_delegate.cc\r\n* Added support for `FusedBatchNormV3` in converter.\r\n* A ragged to dense op for directly calculating tensors.\r\n* Fix accidental quadratic graph construction cost in graph-mode `tf.gradients()`.\r\n* The `precision_mode` argument to `TrtGraphConverter` is now case insensitive.\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\na6802739, Aaron Ma, Abdullah Selek, Abolfazl Shahbazi, Ag Ramesh, Albert Z. Guo, Albin Joy, Alex Itkes, Alex Sergeev, Alexander Pivovarov, Alexey Romanov, alhkad, Amit Srivastava, amoitra, Andrew Lihonosov, Andrii Prymostka, Anuj Rawat, Astropeak, Ayush Agrawal, Bairen Yi, Bas Aarts, Bastian Eichenberger, Ben Barsdell, Benjamin Peterson, bhack, Bharat Raghunathan, Bhavani Subramanian, Bryan Cutler, candy.dc, Cao Zongyan, Captain-Pool, Casper Da Costa-Luis, Chen Guoyin, Cheng Chang, chengchingwen, Chong Yan, Choong Yin Thong, Christopher Yeh, Clayne Robison, Coady, Patrick, Dan Ganea, David Norman, Denis Khalikov, Deven Desai, Diego Caballero, Duncan Dean, Duncan Riach, Dwight J Lyle, Eamon Ito-Fisher, eashtian3, EFanZh, ejot, Elroy Ashtian Jr, Eric Schweitz, Fangjun Kuang, Fei Hu, fo40225, formath, Fred Reiss, Frederic Bastien, Fredrik Knutsson, G. Hussain Chinoy, Gabriel, gehring, George Grzegorz Pawelczak, Gianluca Varisco, Gleb Popov, Greg Peatfield, Guillaume Klein, Gurpreet Singh, Gustavo Lima Chaves, haison, Haraldur T\u00f3Mas Hallgr\u00edMsson, HarikrishnanBalagopal, H\u00e5Kon Sandsmark, I-Hong, Ilham Firdausi Putra, Imran Salam, Jason Zaman, Jason Zavaglia, jayhpark530, jefby, Jeff Daily, Jeffrey Poznanovic, Jekyll Lai, Jeroen B\u00e9Dorf, Jerry Shih, jerryyin, jiakai, JiangXIAO, Joe Bowser, Joel Shapiro, Johan Gunnarsson, Jojimon Varghese, Joon, Josh Beal, Julian Niedermeier, Jun Wan, Junqin Zhang, Junyuan Xie, Justin Tunis, Kaixi Hou, Karl Lessard, Karthik Muthuraman, Kbhute-Ibm, khanhlvg, Koock Yoon, kstuedem, Kyuwon Kim, Lakshay Tokas, leike666666, leonard951, Leslie-Fang, Leslie-Fang-Intel, Li, Guizi, Lukas Folle, Lukas Geiger, Mahmoud Abuzaina, Manraj Singh Grover, Margaret Maynard-Reid, Mark Ryan, Matt Conley, Matthew Bentham, Matthew Denton, mbhuiyan, mdfaijul, Mei Jie, merturl, MichaelKonobeev, Michal W. Tarnowski, minds, mpppk, musikisomorphie, Nagy Mostafa, Nayana Thorat, Neil, Niels Ole Salscheider, Niklas Silfverstr\u00f6M, Niranjan Hasabnis, ocjosen, olramde, Pariksheet Pinjari, Patrick J. Lopresti, Patrik Gustavsson, per1234, PeterLee, Phan Van Nguyen Duc, Phillip Kravtsov, Pooya Davoodi, Pranav Marathe, Putra Manggala, Qingqing Cao, Rajeshwar Reddy T, Ramon Vi\u00f1As, Rasmus Diederichsen, Reuben Morais, richardbrks, robert, RonLek, Ryan Jiang, saishruthi, Saket Khandelwal, Saleem Abdulrasool, Sami Kama, Sana-Damani, Sergii Khomenko, Severen Redwood, Shubham Goyal, Sigrid Keydana, Siju Samuel, sleighsoft, smilu97, Son Tran, Srini511, srinivasan.narayanamoorthy, Sumesh Udayakumaran, Sungmann Cho, Tae-Hwan Jung, Taehoon Lee, Takeshi Watanabe, TengLu, terryky, TheMindVirus, ThisIsIsaac, Till Hoffmann, Timothy Liu, Tomer Gafner, Tongxuan Liu, Trent Lo, Trevor Morris, Uday Bondhugula, Vasileios Lioutas, vbvg2008, Vishnuvardhan Janapati, Vivek Suryamurthy, Wei Wang, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, winstonq, wyzhao, Xiaoming (Jason) Cui, Xinan Jiang, Xinping Wang, Yann-Yy, Yasir Modak, Yong Tang, Yongfeng Gu, Yuchen Ying, Yuxin Wu, zyeric, \u738b\u632f\u534e (Zhenhua Wang)", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.15.0-rc2", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.15.0-rc2", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.15.0-rc2", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/20400227"}, {"tag_name": "v2.0.0", "name": "TensorFlow 2.0.0", "author_name": "goldiegadde", "body": "# Release 2.0.0\r\n\r\n## Major Features and Improvements\r\n\r\nTensorFlow 2.0 focuses on **simplicity** and **ease of use**, featuring updates like:\r\n\r\n* Easy model building with Keras and eager execution.\r\n* Robust model deployment in production on any platform.\r\n* Powerful experimentation for research.\r\n* API simplification by reducing duplication and removing deprecated endpoints.\r\n\r\nFor details on best practices with 2.0, see [the Effective 2.0 guide](https://www.tensorflow.org/beta/guide/effective_tf2)\r\n\r\n\r\nFor information on upgrading your existing TensorFlow 1.x models, please refer to our [Upgrade](https://medium.com/tensorflow/upgrading-your-code-to-tensorflow-2-0-f72c3a4d83b5) and [Migration](https://www.tensorflow.org/guide/migrate) guides. We have also released a collection of [tutorials and getting started guides](https://www.tensorflow.org/beta).\r\n\r\n## Highlights\r\n\r\n* TF 2.0 delivers Keras as the central high level API used to build and train models. Keras provides several model-building APIs such as Sequential, Functional, and Subclassing along with eager execution, for immediate iteration and intuitive debugging, and `tf.data`, for building scalable input pipelines. Checkout [guide](https://www.tensorflow.org/beta/guide/keras/overview) for additional details.\r\n* Distribution Strategy: TF 2.0 users will be able to use the [`tf.distribute.Strategy`](https://www.tensorflow.org/beta/guide/distribute_strategy) API to distribute training with minimal code changes, yielding great out-of-the-box performance. It supports distributed training with Keras model.fit, as well as with custom training loops. Multi-GPU support is available, along with experimental support for multi worker and Cloud TPUs. Check out the [guide](https://www.tensorflow.org/beta/guide/distribute_strategy) for more details.\r\n* Functions, not Sessions. The traditional declarative programming model of building a graph and executing it via a `tf.Session` is discouraged, and replaced with by writing regular Python functions. Using the `tf.function` decorator, such functions can be turned into graphs which can be executed remotely, serialized, and optimized for performance.\r\n* Unification of `tf.train.Optimizers` and `tf.keras.Optimizers`. Use `tf.keras.Optimizers` for TF2.0. `compute_gradients` is removed as public API, use `GradientTape` to compute gradients.\r\n* AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with tf.data, tf.distribute and tf.keras APIs. \r\n* Unification of exchange formats to SavedModel. All TensorFlow ecosystem projects (TensorFlow Lite, TensorFlow JS, TensorFlow Serving, TensorFlow Hub) accept SavedModels. Model state should be saved to and restored from SavedModels.\r\n* API Changes: Many API symbols have been renamed or removed, and argument names have changed. Many of these changes are motivated by consistency and clarity. The 1.x API remains available in the compat.v1 module. A list of all symbol changes can be found [here](https://docs.google.com/spreadsheets/d/1FLFJLzg7WNP6JHODX5q8BDgptKafq_slHpnHVbJIteQ/edit#gid=0). \r\n * API clean-up, included removing `tf.app`, `tf.flags`, and `tf.logging` in favor of [absl-py](https://github.com/abseil/abseil-py).\r\n* No more global variables with helper methods like `tf.global_variables_initializer` and `tf.get_global_step`.\r\n* Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.\r\n* Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.\r\n* Fixes autocomplete for most TensorFlow API references by switching to use relative imports in API `__init__.py` files.\r\n* Auto Mixed-Precision graph optimizer simplifies converting models to `float16` for acceleration on Volta and Turing Tensor Cores. This feature can be enabled by wrapping an optimizer class with `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.\r\n* Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to `TRUE` or \"1\" forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic.\r\n\r\n## Breaking Changes\r\n* Many backwards incompatible API changes have been made to clean up the APIs and make them more consistent.\r\n* Toolchains:\r\n  * TensorFlow 2.0.0 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.\r\n  * Tensorflow code now produces 2 different pip packages: tensorflow_core containing all the code (in the future it will contain only the private implementation) and tensorflow which is a virtual pip package doing forwarding to tensorflow_core (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.\r\n  Removed the `freeze_graph` command line tool; `SavedModel` should be used in place of frozen graphs.\r\n  \r\n* `tf.contrib`:\r\n  * `tf.contrib` has been deprecated, and functionality has been either migrated to the core TensorFlow API, to an ecosystem project such as [tensorflow/addons](https://www.github.com/tensorflow/addons) or [tensorflow/io](https://www.github.com/tensorflow/io), or removed entirely.\r\n  * Remove `tf.contrib.timeseries` dependency on TF distributions.\r\n  * Replace contrib references with `tf.estimator.experimental.*` for apis in `early_stopping.py`.\r\n  \r\n* `tf.estimator`:\r\n  * Premade estimators in the tf.estimator.DNN/Linear/DNNLinearCombined family have been updated to use `tf.keras.optimizers` instead of the `tf.compat.v1.train.Optimizer`s. If you do not pass in an `optimizer=` arg or if you use a string, the premade estimator will use the Keras optimizer. This is checkpoint breaking, as the optimizers have separate variables. A checkpoint converter tool for converting optimizers is included with the release,  but if you want to avoid any change, switch to the v1 version of the estimator:  `tf.compat.v1.estimator.DNN/Linear/DNNLinearCombined*`.\r\n  * Default aggregation for canned Estimators is now `SUM_OVER_BATCH_SIZE`. To maintain previous default behavior, please pass `SUM` as the loss aggregation method.\r\n  * Canned Estimators don\u2019t support `input_layer_partitioner` arg in the API. If you have this arg, you will have to switch to `tf.compat.v1 canned Estimators`.\r\n  * `Estimator.export_savedmodel` has been renamed to `export_saved_model`.\r\n  * When saving to SavedModel, Estimators will strip default op attributes. This is almost always the correct behavior, as it is more forwards compatible, but if you require that default attributes to be saved with the model, please use `tf.compat.v1.Estimator`.\r\n  * Feature Columns have been upgraded to be more Eager-friendly and to work with Keras. As a result, `tf.feature_column.input_layer` has been deprecated in favor of `tf.keras.layers.DenseFeatures`. v1 feature columns have direct analogues in v2 except for `shared_embedding_columns`, which are not cross-compatible with v1 and v2. Use `tf.feature_column.shared_embeddings` instead.\r\n  \r\n* `tf.keras`:\r\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config.  To configure the number of threads, use `tf.config.threading` APIs.\r\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel. HDF5 files are still supported.\r\n  * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.\r\n  * Layers now default to float32, and automatically cast their inputs to the layer's dtype. If you had a model that used float64, it will probably silently use float32 in TensorFlow 2, and a warning will be issued that starts with `Layer <layer-name>` is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.\r\n \r\n* `tf.lite`:\r\n  * Removed `lite.OpHint`, `lite.experimental`, and `lite.constant` from 2.0 API.\r\n* Tensors are no longer hashable, but instead compare element-wise with `==` and `!=`. Use `tf.compat.v1.disable_tensor_equality()` to return to the previous behavior.\r\n* Performing equality operations on Tensors or Variables with incompatible shapes an exception is no longer thrown. Instead `__eq__` returns False and `__ne__` returns True.\r\n* Removed `tf.string_split` from v2 API.\r\n* Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.\r\n* Add `UnifiedGRU` as the new GRU implementation for tf2.0. Change the default recurrent activation function for GRU from `hard_sigmoid` to `sigmoid`, and `reset_after` to True in 2.0. Historically recurrent activation is `hard_sigmoid` since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default GRU will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with GRU(recurrent_activation='hard_sigmoid', reset_after=False) to fallback to 1.x behavior.\r\n* `CUDNN_INSTALL_PATH`, `TENSORRT_INSTALL_PATH`, `NCCL_INSTALL_PATH`, `NCCL_HDR_PATH` are deprecated. Use `TF_CUDA_PATHS` instead which supports a comma-separated list of base paths that are searched to find CUDA libraries and headers.\r\n\r\nRefer to our [public project status tracker](https://github.com/orgs/tensorflow/projects/4) and [issues tagged with 2.0](https://github.com/tensorflow/tensorflow/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3A%22TF+2.0%22+) on GitHub for insight into recent issues and development progress.\r\n\r\nIf you experience any snags when using TF 2.0, please let us know at the [TF 2.0 Testing User Group](https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/testing). We have a support mailing list as well as weekly testing meetings, and would love to hear your migration feedback and questions.\r\n\r\n\r\n## Bug Fixes and Other Changes\r\n\r\n* `tf.contrib`:\r\n  * Expose `tf.contrib.proto.*` ops in `tf.io` (they will exist in TF2)\r\n  \r\n* `tf.data`:\r\n  * Add support for TensorArrays to `tf.data Dataset`.\r\n  * Integrate Ragged Tensors with `tf.data`.\r\n  * All core and experimental tf.data transformations that input user-defined functions can span multiple devices now.\r\n  * Extending the TF 2.0 support for `shuffle(..., reshuffle_each_iteration=True)` and `cache()` to work across different Python iterators for the same dataset.\r\n  * Removing the `experimental_numa_aware` option from `tf.data.Options`.\r\n  * Add `num_parallel_reads` and passing in a Dataset containing filenames into `TextLineDataset` and `FixedLengthRecordDataset`.\r\n  * Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\r\n  * Promoting `tf.data.experimental.enumerate_dataset` to core as `tf.data.Dataset.enumerate`.\r\n  * Promoting `tf.data.experimental.unbatch` to core as `tf.data.Dataset.unbatch`.\r\n  * Adds option for introducing slack in the pipeline to reduce CPU contention, via `tf.data.Options().experimental_slack = True`\r\n  * Added experimental support for parallel batching to `batch()` and `padded_batch()`. This functionality can be enabled through `tf.data.Options()`.\r\n  * Support cancellation of long-running `reduce`.\r\n  * Now we use `dataset` node name as prefix instead of the op name, to identify the component correctly in metrics, for pipelines with repeated components.\r\n  * Improve the performance of datasets using `from_tensors()`.\r\n  * Promoting `unbatch` from experimental to core API.\r\n  * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.\r\n\r\n* `tf.distribute`:\r\n  * Enable `tf.distribute.experimental.MultiWorkerMirroredStrategy` working in eager mode.\r\n  * Callbacks are supported in `MultiWorkerMirroredStrategy`.\r\n  * Disable `run_eagerly` and distribution strategy if there are symbolic tensors added to the model using `add_metric` or `add_loss`.\r\n  * Loss and gradients should now more reliably be correctly scaled w.r.t. the global batch size when using a `tf.distribute.Strategy`.\r\n  * Set default loss reduction as `AUTO` for improving reliability of loss scaling with distribution strategy and custom training loops. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used in distribution strategy scope, outside of built-in training loops such as `tf.keras` `compile` and `fit`, we expect reduction value to be 'None' or 'SUM'. Using other values will raise an error.\r\n  * Support for multi-host `ncclAllReduce` in Distribution Strategy.\r\n\r\n* `tf.estimator`:\r\n  * Replace `tf.contrib.estimator.add_metrics` with `tf.estimator.add_metrics`\r\n  * Use `tf.compat.v1.estimator.inputs` instead of `tf.estimator.inputs`\r\n  * Replace contrib references with `tf.estimator.experimental.*` for apis in early_s in Estimator\r\n  * Canned Estimators will now use keras optimizers by default. An error will be raised if tf.train.Optimizers are used, and you will have to switch to tf.keras.optimizers or tf.compat.v1 canned Estimators.\r\n  * A checkpoint converter for canned Estimators has been provided to transition canned Estimators that are warm started from `tf.train.Optimizers` to `tf.keras.optimizers`.\r\n  * Losses are scaled in canned estimator v2 and not in the optimizers anymore. If you are using Estimator + distribution strategy + optimikzer v1 then the behavior does not change. This implies that if you are using custom estimator with optimizer v2, you have to scale losses. We have new utilities to help scale losses `tf.nn.compute_average_loss`, `tf.nn.scale_regularization_loss`.\r\n\r\n* `tf.keras`:\r\n  * Premade models (including Linear and WideDeep) have been introduced for the purpose of replacing Premade estimators.\r\n  * Model saving changes\r\n  * `model.save` and `tf.saved_model.save` may now save to the TensorFlow SavedModel format. The model can be restored using `tf.keras.models.load_model`. HDF5 files are still supported, and may be used by specifying `save_format=\"h5\"` when saving.\r\n  * Raw TensorFlow functions can now be used in conjunction with the Keras Functional API during model creation. This obviates the need for users to create Lambda layers in most cases when using the Functional API. Like Lambda layers, TensorFlow functions that result in Variable creation or assign ops are not supported.\r\n  * Add support for passing list of lists to the `metrics` argument in Keras `compile`.\r\n  * Add `tf.keras.layers.AbstractRNNCell` as the preferred implementation for RNN cells in TF v2. User can use it to implement RNN cells with custom behavior.\r\n  * Keras training and validation curves are shown on the same plot when using the TensorBoard callback.\r\n  * Switched Keras `fit/evaluate/predict` execution to use only a single unified path by default unless eager execution has been explicitly disabled, regardless of input type. This unified path places an eager-friendly training step inside of a `tf.function`. With this \r\n   1.  All input types are converted to `Dataset`.\r\n   2. The path assumes there is always a distribution strategy. when distribution strategy is not specified the path uses a no-op distribution strategy. \r\n   3. The training step is wrapped in `tf.function` unless `run_eagerly=True` is set in compile. The single path execution code does not yet support all use cases. We fallback to the existing v1 execution paths if your model contains the following: \r\n     1. `sample_weight_mode` in compile \r\n     2. `weighted_metrics` in compile \r\n     3. v1 optimizer \r\n     4. target tensors in compile\r\nIf you are experiencing any issues because of this change, please inform us (file an issue) about your use case and you can unblock yourself by setting `experimental_run_tf_function=False` in compile meanwhile. We have seen couple of use cases where the model usage pattern is not as expected and would not work with this change.\r\n   1. output tensors of one layer is used in the constructor of another.\r\n   2. symbolic tensors outside the scope of the model are used in custom loss functions.\r\n   The flag can be disabled for these cases and ideally the usage pattern will need to be fixed.\r\n  * Mark Keras `set_session` as `compat.v1` only.\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint format`, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.\r\n  * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.\r\n  * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.\r\n  * Update TF 2.0 `keras.backend.name_scope` to use TF 2.0 `name_scope`.\r\n  * Add v2 module aliases for losses, metrics, initializers and optimizers: `tf.losses = tf.keras.losses` & `tf.metrics = tf.keras.metrics` &  `tf.initializers = tf.keras.initializers` & `tf.optimizers = tf.keras.optimizers`.\r\n  * Updates binary cross entropy logic in Keras when input is probabilities. Instead of converting probabilities to logits, we are using the cross entropy formula for probabilities.\r\n  * Added public APIs for `cumsum` and `cumprod` keras backend functions.\r\n  * Add support for temporal sample weight mode in subclassed models.\r\n  * Raise `ValueError` if an integer is passed to the training APIs. \r\n  * Added fault-tolerance support for training Keras model via `model.fit()` with `MultiWorkerMirroredStrategy`, tutorial available.\r\n  * Custom Callback tutorial is now available.\r\n  * To train with `tf.distribute`, Keras API is recommended over estimator.\r\n  * `steps_per_epoch` and `steps` arguments are supported with numpy arrays.\r\n  * New error message when unexpected keys are used in sample_weight/class_weight dictionaries \r\n  * Losses are scaled in Keras compile/fit and not in the optimizers anymore. If you are using custom training loop, we have new utilities to help scale losses `tf.nn.compute_average_loss`, `tf.nn.scale_regularization_loss`.\r\n  * `Layer` apply and add_variable APIs are deprecated.\r\n  * Added support for channels first data format in cross entropy losses with logits and support for tensors with unknown ranks.\r\n  * Error messages will be raised if `add_update`, `add_metric`, `add_loss`, activity regularizers are used inside of a control flow branch.\r\n  * New loss reduction types: \r\n    1. `AUTO`: Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When    used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, we expect reduction value to be `SUM` or `NONE`. Using `AUTO` in that case will raise an error. \r\n    2. `NONE`: Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like `fit`/`evaluate`, the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. \r\n    3. `SUM`: Scalar sum of weighted losses. 4. `SUM_OVER_BATCH_SIZE`: Scalar `SUM` divided by number of elements in losses. This reduction type is not supported when used with `tf.distribute.Strategy` outside of built-in training loops like `tf.keras` `compile`/`fit`.\r\n  * Wraps losses passed to the `compile` API (strings and v1 losses) which are not instances of v2 `Loss` class in `LossWrapper` class. => All losses will now use `SUM_OVER_BATCH_SIZE` reduction as default.\r\n  * `model.add_loss(symbolic_tensor)` should work in ambient eager.\r\n  * Update metric name to always reflect what the user has given in compile. Affects following cases \r\n    1. When name is given as 'accuracy'/'crossentropy' \r\n    2. When an aliased function name is used eg. 'mse' \r\n    3. Removing the `weighted` prefix from weighted metric names.\r\n  * Allow non-Tensors through v2 losses.\r\n  * Add v2 sparse categorical crossentropy metric.\r\n  * Add v2 APIs for `AUCCurve` and `AUCSummationMethod` enums.\r\n  * `add_update` can now be passed a zero-arg callable in order to support turning off the update when setting `trainable=False` on a Layer of a Model compiled with `run_eagerly=True`.\r\n  * Standardize the LayerNormalization API by replacing the args `norm_axis` and `params_axis` with `axis`.\r\n  * Fixed critical bugs that help with DenseFeatures usability in TF2\r\n\r\n* `tf.lite`:\r\n  * Added evaluation script for `COCO` minival\r\n  * Add delegate support for `QUANTIZE`.\r\n  * Add `GATHER` support to NN API delegate.\r\n  * Added support for TFLiteConverter Python API in 2.0. Contains functions from_saved_model, from_keras_file, and from_concrete_functions.\r\n  * Add `EXPAND_DIMS` support to NN API delegate TEST.\r\n  * Add `narrow_range` attribute to QuantizeAndDequantizeV2 and V3.\r\n  * Added support for `tflite_convert` command line tool in 2.0.\r\n  * Post-training quantization tool supports quantizing weights shared by multiple operations. The models made with versions of this tool will use INT8 types for weights and will only be executable interpreters from this version onwards.\r\n  * Post-training quantization tool supports fp16 weights and GPU delegate acceleration for fp16.\r\n  * Add delegate support for `QUANTIZED_16BIT_LSTM`.\r\n  * Extracts `NNAPIDelegateKernel` from nnapi_delegate.cc\r\n\r\n* TensorRT\r\n  * Add TensorFlow 2.0-compatible `TrtGraphConverterV2` API for TensorRT conversion.\r\n    TensorRT initialization arguments are now passed wrapped in a named-tuple,\r\n    `TrtConversionParams`, rather than as separate arguments as in `TrtGraphConverter`.\r\n  * Changed API to optimize TensorRT enginges during graph optimization. This is now\r\n    done by calling `converter.build()` where previously `is_dynamic_op=False` would\r\n    be set.\r\n  * `converter.convert()` no longer returns a `tf.function`. Now the funtion must be\r\n    accessed from the saved model.\r\n  * The `converter.calibrate()` method has been removed. To trigger calibration, a\r\n    `calibration_input_fn` should be provided to `converter.convert()`.\r\n\r\n* Other:\r\n  * Fix accidental quadratic graph construction cost in graph-mode `tf.gradients()`.\r\n  * ResourceVariable's gather op supports batch dimensions.\r\n  * ResourceVariable support for `gather_nd`.\r\n  * `ResourceVariable` and `Variable` no longer accepts `constraint` in the constructor, nor expose it as a @property.\r\n  * Added gradient for `SparseToDense` op.\r\n  * Expose a flag that allows the number of threads to vary across Python benchmarks.\r\n  * `image.resize` in 2.0 now supports gradients for the new resize kernels.\r\n  * `image.resize` now considers proper pixel centers and has new kernels (incl. anti-aliasing).\r\n  * Renamed `tf.image` functions to remove duplicate \"image\" where it is redundant.\r\n  * Variadic reduce is supported on CPU Variadic reduce is supported on CPU\r\n  * Remove unused `StringViewVariantWrapper`.\r\n  * Delete unused `Fingerprint64Map` op registration\r\n  * Add broadcasting support to `tf.matmul`.\r\n  * Add C++ Gradient for `BatchMatMulV2`.\r\n  * Add `tf.math.cumulative_logsumexp` operation.\r\n  * Add ellipsis (...) support for `tf.einsum()`.\r\n  * Add expand_composites argument to all `nest.*` methods.\r\n  * Added `strings.byte_split`.\r\n  * Add a new \"result_type\" parameter to `tf.strings.split`.\r\n  * Add name argument to `tf.string_split` and `tf.strings_split`.\r\n  * Extend `tf.strings.split` to support inputs with any rank.\r\n  * Added `tf.random.binomial`.\r\n  * Added `key` and `skip` methods to `random.experimental.Generator`.\r\n  * Extend `tf.function` with basic support for CompositeTensors arguments (such as `SparseTensor` and `RaggedTensor`).\r\n  * `parallel_for.pfor`: add converters for Softmax, LogSoftmax, IsNaN, All, Any, and MatrixSetDiag.\r\n  * `parallel_for`: add converters for LowerTriangularSolve and Cholesky.\r\n  * `parallel_for`: add converters for `LogMatrixDeterminant` and `MatrixBandPart`.\r\n  * `parallel_for`: Add converter for `MatrixDiag`.\r\n  * `parallel_for`: Add converters for `OneHot`, `LowerBound`, `UpperBound`.\r\n  * `parallel_for`: add converter for `BroadcastTo`.\r\n  * Add `pfor` converter for `Squeeze`.\r\n  * Add `RaggedTensor.placeholder()`.\r\n  * Add ragged tensor support to `tf.squeeze`.\r\n  * Update RaggedTensors to support int32 row_splits.\r\n  * Allow `LinearOperator.solve` to take a `LinearOperator`.\r\n  * Allow all dtypes for `LinearOperatorCirculant`.\r\n  * Introduce MaxParallelism method\r\n  * Add `LinearOperatorHouseholder`.\r\n  * Adds Philox support to new stateful RNG's XLA path.\r\n  * Added `TensorSpec` support for CompositeTensors.\r\n  * Added `tf.linalg.tridiagonal_solve` op.\r\n  * Added partial_pivoting input parameter to `tf.linalg.tridiagonal_solve`.\r\n  * Added gradient to `tf.linalg.tridiagonal_solve`.\r\n  * Added `tf.linalg.tridiagonal_mul op`.\r\n  * Added GPU implementation of `tf.linalg.tridiagonal_matmul`.\r\n  * Added `LinearOperatorToeplitz`.\r\n  * Upgraded LIBXSMM to version 1.11.\r\n  * Uniform processing of quantized embeddings by Gather and EmbeddingLookup Ops.\r\n  * Correct a misstatement in the documentation of the sparse softmax cross entropy logit parameter.\r\n  * Add `tf.ragged.boolean_mask`.\r\n  * `tf.switch_case` added, which selects a branch_fn based on a branch_index.\r\n  * The C++ kernel of gather op supports batch dimensions.\r\n  * Fixed default value and documentation for `trainable` arg of tf.Variable.\r\n  * `EagerTensor` now supports numpy buffer interface for tensors.\r\n  * This change bumps the version number of the `FullyConnected` Op to 5.\r\n  * Added new op: `tf.strings.unsorted_segment_join`.\r\n  * Added HW acceleration support for `topK_v2`.\r\n  * CloudBigtable version updated to v0.10.0 BEGIN_PUBLIC CloudBigtable version updated to v0.10.0.\r\n  * Expose `Head` as public API.\r\n  * Added `tf.sparse.from_dense` utility function.\r\n  * Improved ragged tensor support in `TensorFlowTestCase`.\r\n  * Added a function `nested_value_rowids` for ragged tensors.\r\n  * Added `tf.ragged.stack`.\r\n  * Makes the a-normal form transformation in Pyct configurable as to which nodes are converted to variables and which are not.\r\n  * `ResizeInputTensor` now works for all delegates.\r\n  * `tf.cond` emits a StatelessIf op if the branch functions are stateless and do not touch any resources.\r\n  * Add support of local soft device placement for eager op.\r\n  * Pass partial_pivoting to the `_TridiagonalSolveGrad`.\r\n  * Add HW acceleration support for `LogSoftMax`.\r\n  * Add guard to avoid acceleration of L2 Normalization with input rank != 4\r\n  * Fix memory allocation problem when calling `AddNewInputConstantTensor`.\r\n  * Delegate application failure leaves interpreter in valid state\r\n  * `tf.while_loop` emits a StatelessWhile op if the cond and body functions are stateless and do not touch any resources.\r\n  * `tf.cond`, `tf.while` and if and while in AutoGraph now accept a nonscalar predicate if has a single element. This does not affect non-V2 control flow.\r\n  * Fix potential security vulnerability where decoding variant tensors from proto could result in heap out of bounds memory access.\r\n  * Only create a GCS directory object if the object does not already exist.\r\n  * Introduce `dynamic` constructor argument in Layer and Model, which should be set to `True` when using imperative control flow in the `call` method.\r\n  * Begin adding Go wrapper for C Eager API.\r\n  * XLA HLO graphs can be inspected with interactive_graphviz tool now.\r\n  * Add dataset ops to the graph (or create kernels in Eager execution) during the python Dataset object creation instead doing it during Iterator creation time.\r\n  * Add `batch_dims` argument to `tf.gather`.\r\n  * The behavior of `tf.gather` is now correct when `axis=None` and `batch_dims<0`.\r\n  * Update docstring for gather to properly describe the non-empty `batch_dims` case.\r\n  * Removing of dtype in the constructor of initializers and partition_info in call.\r\n  * Add `tf.math.nextafter` op.\r\n  * Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically dispatches the best kernel implementation based on CPU vector architecture. To disable them, build with `--define=tensorflow_mkldnn_contraction_kernel=0`.\r\n  * `tf.linspace(start, stop, num)` now always uses \"stop\" as last value (for num > 1)\r\n  * Added top-k to precision and recall to keras metrics.\r\n  * Add a ragged size op and register it to the op dispatcher\r\n  * Transitive dependencies on :`pooling_ops` were removed.  Some users may need to add explicit dependencies on :`pooling_ops` if they reference the operators from that library.\r\n  * Add `CompositeTensor` base class.\r\n  * Malformed gif images could result in an access out of bounds in the color palette of the frame. This has been fixed now\r\n  * Add templates and interfaces for creating lookup tables\r\n  * `Tensor::UnsafeCopyFromInternal` deprecated in favor `Tensor::BitcastFrom`.\r\n  * In `map_vectorization` optimization, reduce the degree of parallelism in the vectorized map node.\r\n  * Add variant wrapper for `absl::string_view`.\r\n  * Add OpKernels for some stateless maps.\r\n  * DType is no longer convertible to an int. Use `dtype.as_datatype_enum` instead of `int(dtype)` to get the same result.\r\n  * Support both binary and -1/1 label input in v2 hinge and squared hinge losses.\r\n  * Added `LinearOperator.adjoint` and `LinearOperator.H` (alias).\r\n  * Expose CriticalSection in core as `tf.CriticalSection`.\r\n  * Enhanced graphviz output.\r\n  * Add opkernel templates for common table operations.\r\n  * Fix callbacks do not log values in eager mode when a deferred build model is used.\r\n  * `SignatureDef` util functions have been deprecated.\r\n  * Update `Fingerprint64Map` to use aliases\r\n  * Add legacy string flat hash map op kernels.\r\n  * Add support for `add_metric` in the graph function mode.\r\n  * Updating cosine similarity loss - removed the negate sign from cosine similarity.\r\n  * Changed default for gradient accumulation for TPU embeddings to true.\r\n  * Adds summary trace API for collecting graph and profile information. \r\n  * The `precision_mode` argument to `TrtGraphConverter` is now case insensitive.\r\n\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n1e100, a6802739, 4d55397500, a6802739, Abdullah Selek, abenmao, Abolfazl Shahbazi, Adam Richter, Adam Weiss, Ag Ramesh, Alan Du, Albin Joy, Alex, Alex Itkes, Alex Sergeev, Alexander Pivovarov, Alexey Romanov, alhkad, Aman Patel, Amit, Amit Kumar Jaiswal, Amit Srivastava, amoitra, Andreas Eberle, Andrew Lihonosov, Andy Craze, Anshuman Tripathy, Anthony Hsu, Anthony Platanios, Anuj Rawat, arp95, Arpit Shah, Armen Poghosov, armenpoghosov, Astropeak, Ashwin Ramaswami, Arpit Shah, Augustina Ragwitz, Aurelien Geron, Aur\u00e9Lien Geron, avasid, aweers, awesomealex1, Ayush Agrawal, Bas Aarts, Bastian Eichenberger, Bairen Yi, Bayberry Z, Ben Barsdell, Benjamin Peterson, bhack, Bharat Raghunathan, Bhavani Subramanian, Bin Fan, blairhan, Bl\u00e9Nesi Attila, Bodin-E, Brandon Carter, Bryan Cutler, candy.dc, Cao Zongyan, Casper Da Costa-Luis, Chao Liu, Chen Guoyin, chenchc, chengchingwen, chie8842, Christian Hansen, Christoph Boeddeker, Christopher Yeh, Clayne Robison, Coady, Patrick, crafet, csukuangfj, ctiijima, Dan Jarvis, Dan Lazewatsky, Daniel Ingram, Daniel Rasmussen, Daniel Salvadori, Dave Airlie, David Norman, Dayananda V, delock, Denis Khalikov, Deven Desai, Dheeraj Rajaram Reddy, Diego Caballero, dmitrievanthony, Donovan Ong, Drew Szurko, Duncan Dean, Duncan Riach, Dustin Neighly, Dwight J Lyle, Eamon Ito-Fisher, eashtian3, Edward Forgacs, EFanZh, ejot, Elroy Ashtian Jr, Eric Schweitz, Evgeniy Polyakov, Fangjun Kuang, Federico Martinez, Fei Hu, Felix Lemke, Filip Matzner, FlashTek, fo40225, formath, Franc\u0327Ois Chollet, frreiss, Fred Reiss, Frederic Bastien, Fredrik Knutsson, G. Hussain Chinoy, Gabriel, Gautam, gehring, Geoffrey Irving, George Grzegorz Pawelczak, Grzegorz Pawelczak, George Sterpu, Gianluca Varisco, Gleb Popov, Greg Peatfield, Guillaume Klein, Gurpreet Singh, Gustavo Lima Chaves, Gyoung-Yoon Ryoo, haison, Hanton Yang, HanGuo97, Haraldur T\u00f3Mas Hallgr\u00edMsson, Hari Shankar, hehongliang, Heungsub Lee, Hoeseong Kim, Huan Li (\u674e\u5353\u6853), H\u00e5Kon Sandsmark, I-Hong, I-Hong Jhuo, Ilham Firdausi Putra, Ilango R, Imran Salam, Innovimax, Jacky Ko, Irene Dea, Ivan Habernal, Jakub Lipinski, Jacky, Jason Zaman, Jason Zavaglia, jayhpark530, jcf94, jefby, Jeff Daily, Jeff Poznanovic, Jeffrey Poznanovic, Jekyll Lai, jer, Jeroen B\u00e9Dorf, jerryyin, jhalakp, jiakai, Jia Qingtong, Jiankang, JiangXIAO, Joe Bowser, Joe Q, Joe Quadrino, Joel Shapiro, Johan Gunnarsson, Jojimon Varghese, Jonas Rauber, Jonathan Kyl, Jonathan, Joon, Joppe Geluykens, Joseph Friedman, Josh Beal,  jtressle, Julian Niedermeier, Junqin Zhang, Justin Dujardin, Justin Tunis, jwu, K. Hodges, kaixih, Kaixi Hou, kjopek, Karl Lessard, Karl Weinmeister, Karthik Muthuraman, Kashif Rasul, Kay Zhu, Kbhute-Ibm, KDR, Keno Fischer, Kevin Mader, khanhlvg, Kilaru Yasaswi Sri Chandra Gandhi, Koan-Sin Tan, Koock Yoon, kouml, ktaebum, Kyuwon Kim, Lakshay Tokas, Laurent Le Brun, leike666666, leonard951, Leslie-Fang, Letian Kang, Li, Guizi, Loo Rong Jie, Lucas Hendren, Lukas Folle, Lukas Geiger, Luke Han, luxupu, lvli, Ma, Guokai, Mahmoud Abuzaina, Maksym Kysylov, Mandar Deshpande, manhyuk, Manraj Singh Grover, Marco Gaido, Marek Drozdowski, Margaret Maynard-Reid, Mark Ryan, mars20, Mateusz Chudyk, Matt Conley, mbhuiyan, mdfaijul, Mei Jie, Melissa Grueter, merturl, MichaelKonobeev, Michael K\u00e4Ufl, Michal W. Tarnowski, Micka\u00ebL Schoentgen, Miguel Morin, Mihail Salnikov, Mikalai Drabovich, Mike Arpaia, Mike Holcomb, minds, monklof, Moses Marin, mpppk, Mr. Metal, Mshr-H, musikisomorphie, nammbash, Natalia Gimelshein, Nathan Luehr, Nayana-Ibm, Nayana Thorat, neargye, Neeraj Pradhan, Nehal J Wani, Neil, Nick, Nick Lewycky, Niels Ole Salscheider, Niklas Silfverstr\u00f6M, Niranjan Hasabnis, Nuka-137, Nutti, ocjosen, olicht, omeir1, P Sudeepam, Paige Bailey, Palmer Lao, Pan Daoxin, Pariksheet Pinjari, Pasquale Minervini, Patrick J. Lopresti, Patrik Gustavsson, Pavel Akhtyamov, Pavel Samolysov, PENGWA, per1234, PeterLee, Phan Van Nguyen Duc, Philipp Jund, Phillip Kravtsov, Pooya Davoodi, Pranav Marathe, Putra Manggala, Qingqing Cao, R S Nikhil Krishna, Rajeshwar Reddy T, Ramon Vi\u00f1As, Rasmus Diederichsen, Reuben Morais, robert, Rohit Gupta, Roland Zimmermann, Roman Soldatow, RonLek, Ruizhe, Ryan Jiang, saishruthi, Saleem Abdulrasool, Samantha Andow, Sami Kama, Sami Kama, Sana-Damani, Saurabh Deoras, sdamani, Sean Morgan, seanshpark, Sebastien Iooss, Serv-Inc, Severen Redwood, Shahzad Lone, Shashank Gupta, shashvat, Shashvat Chand Shahi, Shubham Goyal, Shashi, Sigrid Keydana, Siju, Siju Samuel, sleighsoft, smilu97, Snease-Abq, Son Tran, Spencer Schaber, sremedios, Srini511, srinivasan.narayanamoorthy, Steve Lang, Steve Nesae, Subin, Sumesh Udayakumaran, Sungmann Cho, sunway513, Supriya Rao, sxwang, Tae-Hwan Jung, Taehoon Lee, Takeo Sawada, Taylor Jakobson, Taylor Thornton, Ted Chang, TengLu, terryky, ThisIsIsaac, ThisIsPIRI, Thomas Deegan, Thomas Hagebols, tianyapiaozi, Till Hoffmann, Tim Zaman, tomguluson92, Tongxuan Liu, Trent Lo, Trevor Morris, TungJerry, Tyorden, Uday Bondhugula, v1incent, Vagif, Vasileios Lioutas, vbvg2008, vcarpani, Vijay Ravichandran, Vikram Tiwari,Viktor Gal,  Vishwak Srinivasan, Vincent, Vishnuvardhan Janapati, Vitor-Alves, Vivek Suryamurthy, wangsiyu, wateryzephyr, WeberXie, Wei Wang, WeijieSun, Wen-Heng (Jack) Chung, wenxizhu, Will Battel, William D. Irons, winstonq, wyzhao, Xiaoming (Jason) Cui, Xiaoquan Kong, Xin, Xinping Wang, Yan Facai (\u989c\u53d1\u624d), Yann-Yy, Yasir Modak, Yasuhiro Matsumoto, ymodak, Yong Tang, Yongfeng Gu, Younes Khoudli, Yuan Lin, Yuan (Terry) Tang, Yuchen Ying, Yves-Noel Weweler, zhangyujing, zjjott, zyeric, \u738b\u632f\u534e (Zhenhua Wang), \u9ec4\u946b", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.0.0", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.0.0", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/20360510"}, {"tag_name": "v2.0.0-rc2", "name": "TensorFlow 2.0.0-rc2", "author_name": "goldiegadde", "body": "# Release 2.0.0-rc2\r\n\r\n## Major Features and Improvements\r\n\r\nTensorFlow 2.0 focuses on **simplicity** and **ease of use**, featuring updates like:\r\n\r\n* Easy model building with Keras and eager execution.\r\n* Robust model deployment in production on any platform.\r\n* Powerful experimentation for research.\r\n* API simplification by reducing duplication and removing deprecated endpoints.\r\n\r\nFor details on best practices with 2.0, see [the Effective 2.0 guide](https://www.tensorflow.org/beta/guide/effective_tf2)\r\n\r\n\r\nFor information on upgrading your existing TensorFlow 1.x models, please refer to our [Upgrade](https://medium.com/tensorflow/upgrading-your-code-to-tensorflow-2-0-f72c3a4d83b5) and [Migration](https://www.tensorflow.org/beta/guide/migration_guide) guides. We have also released a collection of [tutorials and getting started guides](https://www.tensorflow.org/beta).\r\n\r\n## Highlights\r\n\r\n* TF 2.0 delivers Keras as the central high level API used to build and train models. Keras provides several model-building APIs such as Sequential, Functional, and Subclassing along with eager execution, for immediate iteration and intuitive debugging, and tf.data, for building scalable input pipelines. Checkout [guide](https://www.tensorflow.org/beta/guide/keras/overview) for additional details.\r\n* Distribution Strategy: TF 2.0 users will be able to use the [`tf.distribute.Strategy`](https://www.tensorflow.org/beta/guide/distribute_strategy) API to distribute training with minimal code changes, yielding great out-of-the-box performance. It supports distributed training with Keras model.fit, as well as with custom training loops. Multi-GPU support is available, along with experimental support for multi worker and Cloud TPUs. Check out the [guide](https://www.tensorflow.org/beta/guide/distribute_strategy) for more details.\r\n* Functions, not Sessions. The traditional declarative programming model of building a graph and executing it via a `tf.Session` is discouraged, and replaced by writing regular Python functions. Using the `tf.function` decorator, such functions can be turned into graphs which can be executed remotely, serialized, and optimized for performance.\r\n* Unification of tf.train.Optimizers and tf.keras.Optimizers. Use tf.keras.Optimizers for TF2.0. `compute_gradients` is removed as public API, and use GradientTape to compute gradients.\r\n* AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with tf.data, tf.distribute and tf.keras APIs. \r\n* Unification of exchange formats to SavedModel. All TensorFlow ecosystem projects (TensorFlow Lite, TensorFlow JS, TensorFlow Serving, TensorFlow Hub) accept SavedModels. Model state should be saved to and restored from SavedModels.\r\n* API Changes: Many API symbols have been renamed or removed, and argument names have changed. Many of these changes are motivated by consistency and clarity. The 1.x API remains available in the compat.v1 module. A list of all symbol changes can be found [here](https://docs.google.com/spreadsheets/d/1FLFJLzg7WNP6JHODX5q8BDgptKafq_slHpnHVbJIteQ/edit#gid=0). \r\n * API clean-up, included removing `tf.app`, `tf.flags`, and `tf.logging` in favor of [absl-py](https://github.com/abseil/abseil-py).\r\n* No more global variables with helper methods like `tf.global_variables_initializer` and `tf.get_global_step`.\r\n* Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.\r\n* Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.\r\n* Fixes autocomplete for most TensorFlow API references by switching to use relative imports in API `__init__.py` files.\r\n* Auto Mixed-Precision graph optimizer simplifies converting models to float16 for acceleration on Volta and Turing Tensor Cores. This feature can be enabled by wrapping an optimizer class with `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.\r\n* Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to \"true\" or \"1\" forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic.\r\n\r\n## Breaking Changes\r\n* Many backwards incompatible API changes have been made to clean up the APIs and make them more consistent.\r\n* Toolchains:\r\n  * TensorFlow 2.0.0 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.\r\n  * Tensorflow code now produces 2 different pip packages: tensorflow_core containing all the code (in the future it will contain only the private implementation) and tensorflow which is a virtual pip package doing forwarding to tensorflow_core (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.\r\n\r\n* `tf.contrib`:\r\n  * `tf.contrib` has been deprecated, and functionality has been either migrated to the core TensorFlow API, to an ecosystem project such as [tensorflow/addons](https://www.github.com/tensorflow/addons) or [tensorflow/io](https://www.github.com/tensorflow/io), or removed entirely.\r\n  * Remove `tf.contrib.timeseries` dependency on TF distributions.\r\n  * Replace contrib references with `tf.estimator.experimental.*` for apis in early_stopping.py.\r\n\r\n* `tf.estimator`:\r\n  * Premade estimators in the tf.estimator.DNN/Linear/DNNLinearCombined family have been updated to use `tf.keras.optimizers` instead of the `tf.compat.v1.train.Optimizer`s. If you do not pass in an `optimizer=` arg or if you use a string, the premade estimator will use the Keras optimizer. This is checkpoint breaking, as the optimizers have separate variables. A checkpoint converter tool for converting optimizers is included with the release,  but if you want to avoid any change, switch to the v1 version of the estimator:  `tf.compat.v1.estimator.DNN/Linear/DNNLinearCombined*`.\r\n  * Default aggregation for canned Estimators is now `SUM_OVER_BATCH_SIZE`. To maintain previous default behavior, please pass `SUM` as the loss aggregation method.\r\n  * Canned Estimators don\u2019t support `input_layer_partitioner` arg in the API. If you have this arg, you will have to switch to `tf.compat.v1 canned Estimators`.\r\n  * `Estimator.export_savedmodel` has been renamed `export_saved_model`.\r\n  * When saving to SavedModel, Estimators will strip default op attributes. This is almost always the correct behavior, as it is more forwards compatible, but if you require that default attributes are saved with the model, please use `tf.compat.v1.Estimator`.\r\n  * Feature Columns have been upgraded to be more Eager-friendly and to work with Keras. As a result, `tf.feature_column.input_layer` has been deprecated in favor of `tf.keras.layers.DenseFeatures`. v1 feature columns have direct analogues in v2 except for `shared_embedding_columns`, which are not cross-compatible with v1 and v2. Use `tf.feature_column.shared_embeddings` instead.\r\n\r\n* `tf.keras`:\r\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config.  To configure the number of threads, use `tf.config.threading` APIs.\r\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel. HDF5 files are still supported.\r\n  * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.\r\n  * Layers now default to float32, and automatically cast their inputs to the layer's dtype. If you had a model that used float64, it will probably silently use float32 in TensorFlow 2, and a warning will be issued that starts with \"Layer <layer-name> is casting an input tensor from dtype float64 to the layer's dtype of float32\". To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.\r\n\r\n* `tf.lite`:\r\n  * Removed `lite.OpHint`, `lite.experimental`, and `lite.constant` from 2.0 API.\r\n\r\n* Tensors are no longer hashable, but instead compare element-wise with `==` and `!=`. Use `tf.compat.v1.disable_tensor_equality()` to return to the previous behavior.\r\n* Performing equality operations on Tensors or Variables with incompatible shapes an exception is no longer thrown. Instead `__eq__` returns False and `__ne__` returns True.\r\n* Removed `tf.string_split` from v2 API.\r\n* Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.\r\n* Add `UnifiedGRU` as the new GRU implementation for tf2.0. Change the default recurrent activation function for GRU from `hard_sigmoid` to `sigmoid`, and `reset_after` to True in 2.0. Historically recurrent activation is `hard_sigmoid` since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default GRU will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with GRU(recurrent_activation='hard_sigmoid', reset_after=False) to fallback to 1.x behavior.\r\n* `CUDNN_INSTALL_PATH`, `TENSORRT_INSTALL_PATH`, `NCCL_INSTALL_PATH`, `NCCL_HDR_PATH` are deprecated. Use `TF_CUDA_PATHS` instead which supports a comma-separated list of base paths that are searched to find CUDA libraries and headers.\r\n\r\nRefer to our [public project status tracker](https://github.com/orgs/tensorflow/projects/4) and [issues tagged with `2.0`](https://github.com/tensorflow/tensorflow/issues?q=is%3Aopen+is%3Aissue+label%3A2.0) on GitHub for insight into recent issues and development progress.\r\n\r\nIf you experience any snags when using TF 2.0, please let us know at the [TF 2.0 Testing User Group](https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/testing). We have a support mailing list as well as weekly testing meetings, and would love to hear your migration feedback and questions.\r\n\r\n\r\n## Bug Fixes and Other Changes\r\n\r\n* `tf.contrib`:\r\n  * Expose `tf.contrib.proto.*` ops in `tf.io` (they will exist in TF2)\r\n  \r\n* `tf.data`:\r\n  * Add support for TensorArrays to `tf.data Dataset`.\r\n  * Integrate Ragged Tensors with `tf.data`.\r\n  * All core and experimental tf.data transformations that input user-defined functions can span multiple devices now.\r\n  * Extending the TF 2.0 support for `shuffle(..., reshuffle_each_iteration=True)` and `cache()` to work across different Python iterators for the same dataset.\r\n  * Removing the `experimental_numa_aware` option from `tf.data.Options`.\r\n  * Add `num_parallel_reads` and passing in a Dataset containing filenames into `TextLineDataset` and `FixedLengthRecordDataset`.\r\n  * Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\r\n  * Promoting `tf.data.experimental.enumerate_dataset` to core as `tf.data.Dataset.enumerate`.\r\n  * Promoting `tf.data.experimental.unbatch` to core as `tf.data.Dataset.unbatch`.\r\n  * Adds option for introducing slack in the pipeline to reduce CPU contention, via `tf.data.Options().experimental_slack = True`\r\n  * Added experimental support for parallel batching to `batch()` and `padded_batch()`. This functionality can be enabled through tf.data.Options()\r\n  * Support cancellation of long-running `reduce`.\r\n  * Now we use `dataset` node name as prefix instead of the op name, to identify the component correctly in metrics, for pipelines with repeated components.\r\n  * Improve the performance of datasets using `from_tensors()`.\r\n  * Promoting `unbatch` from experimental to core API.\r\n  * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.\r\n\r\n* `tf.distribute`:\r\n  * Enable `tf.distribute.experimental.MultiWorkerMirroredStrategy` working in eager mode.\r\n  * Callbacks are supported in `MultiWorkerMirroredStrategy`.\r\n  * Disable `run_eagerly` and distribution strategy if there are symbolic tensors added to the model using `add_metric` or `add_loss`.\r\n  * Loss and gradients should now more reliably be correctly scaled w.r.t. the global batch size when using a `tf.distribute.Strategy`.\r\n  * Set default loss reduction as `AUTO` for improving reliability of loss scaling with distribution strategy and custom training loops. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used in distribution strategy scope, outside of built-in training loops such as `tf.keras` `compile` and `fit`, we expect reduction value to be 'None' or 'SUM'. Using other values will raise an error.\r\n  * Support for multi-host `ncclAllReduce` in Distribution Strategy.\r\n\r\n* `tf.estimator`:\r\n  * Replace `tf.contrib.estimator.add_metrics` with `tf.estimator.add_metrics`\r\n  * Use `tf.compat.v1.estimator.inputs` instead of `tf.estimator.inputs`\r\n  * Replace contrib references with `tf.estimator.experimental.*` for apis in early_s in Estimator\r\n  * Canned Estimators will now use keras optimizers by default. An error will be raised if tf.train.Optimizers are used, and you will have to switch to tf.keras.optimizers or tf.compat.v1 canned Estimators.\r\n  * A checkpoint converter for canned Estimators has been provided to transition canned Estimators that are warm started from `tf.train.Optimizers` to `tf.keras.optimizers`.\r\n  * Losses are scaled in canned estimator v2 and not in the optimizers anymore. If you are using Estimator + distribution strategy + optimikzer v1 then the behavior does not change. This implies that if you are using custom estimator with optimizer v2, you have to scale losses. We have new utilities to help scale losses `tf.nn.compute_average_loss`, `tf.nn.scale_regularization_loss`.\r\n\r\n* `tf.keras`:\r\n  * Premade models (including Linear and WideDeep) have been introduced for the purpose of replacing Premade estimators.\r\n  * Model saving changes\r\n  * `model.save` and `tf.saved_model.save` may now save to the TensorFlow SavedModel format. The model can be restored using `tf.keras.models.load_model`. HDF5 files are still supported, and may be used by specifying `save_format=\"h5\"` when saving.\r\n  * Raw TensorFlow functions can now be used in conjunction with the Keras Functional API during model creation. This obviates the need for users to create Lambda layers in most cases when using the Functional API. Like Lambda layers, TensorFlow functions that result in Variable creation or assign ops are not supported.\r\n  * Add support for passing list of lists to the `metrics` argument in Keras `compile`.\r\n  * Add `tf.keras.layers.AbstractRNNCell` as the preferred implementation for RNN cells in TF v2. User can use it to implement RNN cells with custom behavior.\r\n  * Keras training and validation curves are shown on the same plot when using the TensorBoard callback.\r\n  * Switched Keras `fit/evaluate/predict` execution to use only a single unified path by default unless eager execution has been explicitly disabled, regardless of input type. This unified path places an eager-friendly training step inside of a `tf.function`. With this\r\n    1.  All input types are converted to `Dataset`.\r\n    2. The path assumes there is always a distribution strategy, when distribution strategy is not specified the path uses a no-op distribution strategy. \r\n    3. The training step is wrapped in `tf.function` unless `run_eagerly=True` is set in compile. The single path execution code does not yet support all use cases. We fallback to the existing v1 execution paths if your model contains the following: \r\n       1. `sample_weight_mode` in compile \r\n       2. `weighted_metrics` in compile \r\n       3. v1 optimizer \r\n       4. target tensors in compile.  \r\n    4. If you are experiencing any issues because of this change, please inform us (file an issue) about your use case and you can unblock yourself by setting `experimental_run_tf_function=False` in compile meanwhile. We have seen couple of use cases where the model usage pattern is not as expected and would not work with this change. \r\n       1. output tensors of one layer is used in the constructor of another. \r\n       2. symbolic tensors outside the scope of the model are used in custom loss functions.\r\nThe flag can be disabled for these cases and ideally the usage pattern will need to be fixed.\r\n  * Mark Keras `set_session` as `compat.v1` only.\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint format`, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.\r\n  * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.\r\n  * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.\r\n  * Update TF 2.0 `keras.backend.name_scope` to use TF 2.0 `name_scope`.\r\n  * Add v2 module aliases for losses, metrics, initializers and optimizers: `tf.losses = tf.keras.losses` & `tf.metrics = tf.keras.metrics` &  `tf.initializers = tf.keras.initializers` & `tf.optimizers = tf.keras.optimizers`.\r\n  * Updates binary cross entropy logic in Keras when input is probabilities. Instead of converting probabilities to logits, we are using the cross entropy formula for probabilities.\r\n  * Added public APIs for `cumsum` and `cumprod` keras backend functions.\r\n  * Add support for temporal sample weight mode in subclassed models.\r\n  * Raise ValueError if an integer is passed to the training APIs. \r\n  * Added fault-tolerance support for training Keras model via `model.fit()` with `MultiWorkerMirroredStrategy`, tutorial available.\r\n  * Custom Callback tutorial is now available.\r\n  * To train with `tf.distribute`, Keras api is recommended over estimator.\r\n  * `steps_per_epoch` and `steps` arguments are supported with numpy arrays.\r\n  * New error message when unexpected keys are used in sample_weight/class_weight dictionaries \r\n  * Losses are scaled in Keras compile/fit and not in the optimizers anymore. If you are using custom training loop, we have new utilities to help scale losses `tf.nn.compute_average_loss`, `tf.nn.scale_regularization_loss`.\r\n  * `Layer` apply and add_variable APIs are deprecated.\r\n  * Added support for channels first data format in cross entropy losses with logits and support for tensors with unknown ranks.\r\n  * Error messages will be raised if `add_update`, `add_metric`, `add_loss`, activity regularizers are used inside of a control flow branch.\r\n  * New loss reduction types: \r\n    1. `AUTO`: Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When    used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, we expect reduction value to be `SUM` or `NONE`. Using `AUTO` in that case will raise an error. \r\n    2. `NONE`: Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like `fit`/`evaluate`, the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. \r\n    3. `SUM`: Scalar sum of weighted losses. 4. `SUM_OVER_BATCH_SIZE`: Scalar `SUM` divided by number of elements in losses. This reduction type is not supported when used with `tf.distribute.Strategy` outside of built-in training loops like `tf.keras` `compile`/`fit`.\r\n  * Wraps losses passed to the `compile` API (strings and v1 losses) which are not instances of v2 `Loss` class in `LossWrapper` class. => All losses will now use `SUM_OVER_BATCH_SIZE` reduction as default.\r\n  * `model.add_loss(symbolic_tensor)` should work in ambient eager.\r\n  * Update metric name to always reflect what the user has given in compile. Affects following cases \r\n    1. When name is given as 'accuracy'/'crossentropy' \r\n    2. When an aliased function name is used eg. 'mse' \r\n    3. Removing the `weighted` prefix from weighted metric names.\r\n  * Allow non-Tensors through v2 losses.\r\n  * Add v2 sparse categorical crossentropy metric.\r\n  * Add v2 APIs for `AUCCurve` and `AUCSummationMethod` enums.\r\n  * `add_update` can now be passed a zero-arg callable in order to support turning off the update when setting `trainable=False` on a Layer of a Model compiled with `run_eagerly=True`.\r\n  * Standardize the LayerNormalization API by replacing the args `norm_axis` and `params_axis` with `axis`.\r\n  * Fixed critical bugs that help with DenseFeatures usability in TF2\r\n\r\n* `tf.lite`:\r\n  * Added evaluation script for `COCO` minival\r\n  * Add delegate support for `QUANTIZE`.\r\n  * Add `GATHER` support to NN API delegate.\r\n  * Added support for TFLiteConverter Python API in 2.0. Contains functions from_saved_model, from_keras_file, and from_concrete_functions.\r\n  * Add `EXPAND_DIMS` support to NN API delegate TEST.\r\n  * Add `narrow_range` attribute to QuantizeAndDequantizeV2 and V3.\r\n  * Added support for `tflite_convert` command line tool in 2.0.\r\n  * Post-training quantization tool supports quantizing weights shared by multiple operations. The models made with versions of this tool will use INT8 types for weights and will only be executable interpreters from this version onwards.\r\n  * Post-training quantization tool supports fp16 weights and GPU delegate acceleration for fp16.\r\n  * Add delegate support for `QUANTIZED_16BIT_LSTM`.\r\n  * Extracts `NNAPIDelegateKernel` from nnapi_delegate.cc\r\n\r\n* TensorRT\r\n  * Add TensorFlow 2.0-compatible `TrtGraphConverterV2` API for TensorRT conversion. TensorRT initialization arguments are now passed wrapped in a named-tuple,`TrtConversionParams`, rather than as separate arguments as in `TrtGraphConverter`.\r\n  * Changed API to optimize TensorRT engines during graph optimization. This is nowdone by calling `converter.build()` where previously `is_dynamic_op=False` would be set.\r\n  * `converter.convert()` no longer returns a `tf.function`. Now the function must beaccessed from the saved model.\r\n  * The `converter.calibrate()` method has been removed. To trigger calibration, a`calibration_input_fn` should be provided to `converter.convert()`.\r\n  \r\n* Other:\r\n  * Fix accidental quadratic graph construction cost in graph-mode `tf.gradients()`.\r\n  * ResourceVariable's gather op supports batch dimensions.\r\n  * ResourceVariable support for `gather_nd`.\r\n  * `ResourceVariable` and `Variable` no longer accepts `constraint` in the constructor, nor expose it as a @property.\r\n  * Added gradient for `SparseToDense` op.\r\n  * Expose a flag that allows the number of threads to vary across Python benchmarks.\r\n  * `image.resize` in 2.0 now supports gradients for the new resize kernels.\r\n  * `image.resize` now considers proper pixel centers and has new kernels (incl. anti-aliasing).\r\n  * Renamed `tf.image` functions to remove duplicate \"image\" where it is redundant.\r\n  * Variadic reduce is supported on CPU Variadic reduce is supported on CPU\r\n  * Remove unused `StringViewVariantWrapper`.\r\n  * Delete unused `Fingerprint64Map` op registration\r\n  * Add broadcasting support to `tf.matmul`.\r\n  * Add C++ Gradient for `BatchMatMulV2`.\r\n  * Add `tf.math.cumulative_logsumexp` operation.\r\n  * Add ellipsis (...) support for `tf.einsum()`.\r\n  * Add expand_composites argument to all `nest.*` methods.\r\n  * Added `strings.byte_split`.\r\n  * Add a new \"result_type\" parameter to `tf.strings.split`.\r\n  * Add name argument to `tf.string_split` and `tf.strings_split`.\r\n  * Extend `tf.strings.split` to support inputs with any rank.\r\n  * Added `tf.random.binomial`.\r\n  * Added `key` and `skip` methods to `random.experimental.Generator`.\r\n  * Extend `tf.function` with basic support for CompositeTensors arguments (such as `SparseTensor` and `RaggedTensor`).\r\n  * `parallel_for.pfor`: add converters for Softmax, LogSoftmax, IsNaN, All, Any, and MatrixSetDiag.\r\n  * `parallel_for`: add converters for LowerTriangularSolve and Cholesky.\r\n  * `parallel_for`: add converters for `LogMatrixDeterminant` and `MatrixBandPart`.\r\n  * `parallel_for`: Add converter for `MatrixDiag`.\r\n  * `parallel_for`: Add converters for `OneHot`, `LowerBound`, `UpperBound`.\r\n  * `parallel_for`: add converter for `BroadcastTo`.\r\n  * Add `pfor` converter for `Squeeze`.\r\n  * Add `RaggedTensor.placeholder()`.\r\n  * Add ragged tensor support to `tf.squeeze`.\r\n  * Update RaggedTensors to support int32 row_splits.\r\n  * Allow `LinearOperator.solve` to take a `LinearOperator`.\r\n  * Allow all dtypes for `LinearOperatorCirculant`.\r\n  * Introduce MaxParallelism method\r\n  * Add `LinearOperatorHouseholder`.\r\n  * Adds Philox support to new stateful RNG's XLA path.\r\n  * Add `TensorSpec` support for CompositeTensors.\r\n  * Add `tf.linalg.tridiagonal_solve` op.\r\n  * Added partial_pivoting input parameter to `tf.linalg.tridiagonal_solve`.\r\n  * Added gradient to `tf.linalg.tridiagonal_solve`.\r\n  * Add `tf.linalg.tridiagonal_mul op`.\r\n  * Added GPU implementation of `tf.linalg.tridiagonal_matmul`.\r\n  * Add `LinearOperatorToeplitz`.\r\n  * Upgraded LIBXSMM to version 1.11.\r\n  * Uniform processing of quantized embeddings by Gather and EmbeddingLookup Ops\r\n  * Correct a misstatement in the documentation of the sparse softmax cross entropy logit parameter.\r\n  * Add `tf.ragged.boolean_mask`.\r\n  * `tf.switch_case` added, which selects a branch_fn based on a branch_index.\r\n  * The C++ kernel of gather op supports batch dimensions.\r\n  * Fixed default value and documentation for `trainable` arg of tf.Variable.\r\n  * EagerTensor now supports numpy buffer interface for tensors.\r\n  * This change bumps the version number of the FullyConnected Op to 5.\r\n  * Added new op: `tf.strings.unsorted_segment_join`.\r\n  * Add HW acceleration support for `topK_v2`.\r\n  * CloudBigtable version updated to v0.10.0 BEGIN_PUBLIC CloudBigtable version updated to v0.10.0.\r\n  * Expose `Head` as public API.\r\n  * Added `tf.sparse.from_dense` utility function.\r\n  * Improved ragged tensor support in `TensorFlowTestCase`.\r\n  * Added a function `nested_value_rowids` for ragged tensors.\r\n  * Add `tf.ragged.stack`.\r\n  * Makes the a-normal form transformation in Pyct configurable as to which nodes are converted to variables and which are not.\r\n  * `ResizeInputTensor` now works for all delegates.\r\n  * `tf.cond` emits a StatelessIf op if the branch functions are stateless and do not touch any resources.\r\n  * Add support of local soft device placement for eager op.\r\n  * Pass partial_pivoting to the `_TridiagonalSolveGrad`.\r\n  * Add HW acceleration support for `LogSoftMax`.\r\n  * Add guard to avoid acceleration of L2 Normalization with input rank != 4\r\n  * Fix memory allocation problem when calling `AddNewInputConstantTensor`.\r\n  * Delegate application failure leaves interpreter in valid state\r\n  * `tf.while_loop` emits a StatelessWhile op if the cond and body functions are stateless and do not touch any resources.\r\n  * tf.cond, tf.while and if and while in AutoGraph now accept a nonscalar predicate if has a single element. This does not affec non-V2 control flow.\r\n  * Fix potential security vulnerability where decoding variant tensors from proto could result in heap out of bounds memory access.\r\n  * Only create a GCS directory object if the object does not already exist.\r\n  * Introduce `dynamic` constructor argument in Layer and Model, which should be set to True when using imperative control flow in the `call` method.\r\n  * Begin adding Go wrapper for C Eager API.\r\n  * XLA HLO graphs can be inspected with interactive_graphviz tool now.\r\n  * Add dataset ops to the graph (or create kernels in Eager execution) during the python Dataset object creation instead doing it during Iterator creation time.\r\n  * Add `batch_dims` argument to `tf.gather`.\r\n  * The behavior of `tf.gather` is now correct when axis=None and batch_dims<0.\r\n  * Update docstring for gather to properly describe the non-empty `batch_dims` case.\r\n  * Removing of dtype in the constructor of initializers and partition_info in call.\r\n  * Add `tf.math.nextafter` op.\r\n  * Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically dispatches the best kernel implementation based on CPU vector architecture. To disable them, build with `--define=tensorflow_mkldnn_contraction_kernel=0`.\r\n  * `tf.linspace(start, stop, num)` now always uses \"stop\" as last value (for num > 1)\r\n  * Added top-k to precision and recall to keras metrics.\r\n  * Add a ragged size op and register it to the op dispatcher\r\n  * Transitive dependencies on :`pooling_ops` were removed.  Some users may need to add explicit dependencies on :`pooling_ops` if they reference the operators from that library.\r\n  * Add `CompositeTensor` base class.\r\n  * Malformed gif images could result in an access out of bounds in the color palette of the frame. This has been fixed now\r\n  * Add templates and interfaces for creating lookup tables\r\n  * `Tensor::UnsafeCopyFromInternal` deprecated in favor `Tensor::BitcastFrom`.\r\n  * In `map_vectorization` optimization, reduce the degree of parallelism in the vectorized map node.\r\n  * Add variant wrapper for `absl::string_view`.\r\n  * Add OpKernels for some stateless maps.\r\n  * DType is no longer convertible to an int. Use `dtype.as_datatype_enum` instead of `int(dtype)` to get the same result.\r\n  * Support both binary and -1/1 label input in v2 hinge and squared hinge losses.\r\n  * Added `LinearOperator.adjoint` and `LinearOperator.H` (alias).\r\n  * Expose CriticalSection in core as `tf.CriticalSection`.\r\n  * Enhanced graphviz output.\r\n  * Add opkernel templates for common table operations.\r\n  * Fix callbacks do not log values in eager mode when a deferred build model is used.\r\n  * `SignatureDef` util functions have been deprecated.\r\n  * Update `Fingerprint64Map` to use aliases\r\n  * Add legacy string flat hash map op kernels.\r\n  * Add support for `add_metric` in the graph function mode.\r\n  * Updating cosine similarity loss - removed the negate sign from cosine similarity.\r\n  * Changed default for gradient accumulation for TPU embeddings to true.\r\n  * Adds summary trace API for collecting graph and profile information. \r\n  \r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n1e100, a6802739, Abolfazl Shahbazi, Adam Weiss, Ag Ramesh, Alan Du, Albin Joy, Amit, Amit Srivastava, Andy Craze, Anshuman Tripathy, Armen Poghosov, armenpoghosov, Arpit Shah, Ashwin Ramaswami, Aurelien Geron, Aur\u00e9Lien Geron, aweers, awesomealex1, Bairen Yi, Ben Barsdell, Bhavani Subramanian, Brandon Carter, candy.dc, Chao Liu, Clayne Robison, csukuangfj, Dan Jarvis, Dan Lazewatsky, Daniel Ingram, Dave Airlie, David Norman, Dayananda V, Denis Khalikov, Deven Desai, Dheeraj Rajaram Reddy, dmitrievanthony, Drew Szurko, Duncan Riach, Fei Hu, Felix Lemke, Filip Matzner, fo40225, frreiss, Gautam, gehring, Grzegorz George Pawelczak, Grzegorz Pawelczak, HanGuo97, Hari Shankar, hehongliang, Heungsub Lee, Hoeseong Kim, I-Hong Jhuo, Ilango R, Innovimax, Jacky Ko, Jakub Lipinski,  jcf94, Jeff Poznanovic, Jia Qingtong, Jiankang, Joe Q, Joe Quadrino, Jonas Rauber, Jonathan Kyl, Joppe Geluykens, Joseph Friedman, jtressle, jwu, K. Hodges, kaixih, Karl Lessard, Karl Weinmeister, Kashif Rasul, kjopek, Koan-Sin Tan, kouml, ktaebum, Laurent Le Brun, Li, Guizi, Loo Rong Jie, Lucas Hendren, Lukas Geiger, Luke Han, Mahmoud Abuzaina, manhyuk, Marco Gaido, Marek Drozdowski, Mark Ryan, mars20, Mateusz Chudyk, Matt Conley, MattConley, mbhuiyan, mdfaijul, Melissa Grueter, Michael K\u00e4Ufl, Micka\u00ebL Schoentgen, Miguel Morin, Mike Arpaia, nammbash, Natalia Gimelshein, Nayana-Ibm, neargye, Nehal J Wani, Niels Ole Salscheider, Niranjan Hasabnis, Nutti, olicht, P Sudeepam, Paige Bailey, Palmer Lao, Pariksheet Pinjari, Pavel Samolysov, Pooya Davoodi, Ryan Jiang, Samantha Andow, Sami Kama, Saurabh Deoras, Shahzad Lone, Shashi, Siju, Siju Samuel, Snease-Abq, Spencer Schaber, srinivasan.narayanamoorthy, Steve Lang, Steve Nesae, Supriya Rao, Taylor Jakobson, Taylor Thornton, ThisIsPIRI, Thomas Deegan, tomguluson92, Tongxuan Liu, Vagif, vcarpani, Vikram Tiwari, Vishwak Srinivasan, Vitor-Alves, wangsiyu, WeberXie, WeijieSun, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, Yan Facai (\u989c\u53d1\u624d), ymodak, Younes Khoudli, Yuan Lin, Yves-Noel Weweler, zjjott, \u535c\u5c45, \u738b\u632f\u534e (Wang Zhenhua), \r\n\r\n4d55397500, a6802739, Abdullah Selek, abenmao, Adam Richter, Ag Ramesh, Albin Joy, Alex, Alex Itkes, Alex Sergeev, Alexander Pivovarov, Alexey Romanov, alhkad, Aman Patel, Amit, Amit Kumar Jaiswal, Amit Srivastava, amoitra, Andreas Eberle, Andrew Lihonosov, Anthony Hsu, Anthony Platanios, Anuj Rawat, arp95, Arpit Shah, Astropeak, Augustina Ragwitz, Aurelien Geron, Aur\u00e9Lien Geron, avasid, aweers, Ayush Agrawal, Bas Aarts, Bastian Eichenberger, Bayberry Z, Ben Barsdell, Benjamin Peterson, bhack, Bharat Raghunathan, Bhavani Subramanian, Bin Fan, blairhan, Bl\u00e9Nesi Attila, Bodin-E, Bryan Cutler, Cao Zongyan, Casper Da Costa-Luis, Chen Guoyin, chenchc, chengchingwen, chie8842, Christian Hansen, Christoph Boeddeker, Christopher Yeh, Clayne Robison, Coady, Patrick, crafet, ctiijima, Daniel Rasmussen, Daniel Salvadori, David Norman, delock, Denis Khalikov, Deven Desai, Diego Caballero, Donovan Ong, Duncan Dean, Duncan Riach, Dustin Neighly, Dwight J Lyle, Eamon Ito-Fisher, eashtian3, Edward Forgacs, EFanZh, ejot, Elroy Ashtian Jr, Eric Schweitz, Evgeniy Polyakov, Fangjun Kuang, Federico Martinez, Fei Hu, Filip Matzner, FlashTek, fo40225, formath, Franc\u0327Ois Chollet, Fred Reiss, Frederic Bastien, Fredrik Knutsson, G. Hussain Chinoy, Gabriel, gehring, Geoffrey Irving, George Grzegorz Pawelczak, George Sterpu, Gianluca Varisco, Gleb Popov, Greg Peatfield, Guillaume Klein, Gurpreet Singh, Gustavo Lima Chaves, Gyoung-Yoon Ryoo, haison, Hanton Yang, Haraldur T\u00f3Mas Hallgr\u00edMsson, Huan Li (\u674e\u5353\u6853), H\u00e5Kon Sandsmark, I-Hong, Ilham Firdausi Putra, Imran Salam, Irene Dea, Ivan Habernal, Jacky, Jason Zaman, Jason Zavaglia, jayhpark530, jefby, Jeff Daily, Jeffrey Poznanovic, Jekyll Lai, jer, Jeroen B\u00e9Dorf, jerryyin, jhalakp, jiakai, JiangXIAO, Joe Bowser, Joel Shapiro, Johan Gunnarsson, Jojimon Varghese, Jonathan, Joon, Josh Beal, Julian Niedermeier, Junqin Zhang, Justin Dujardin, Justin Tunis, Kaixi Hou, Karthik Muthuraman, Kay Zhu, Kbhute-Ibm, KDR, Keno Fischer, Kevin Mader, khanhlvg, Kilaru Yasaswi Sri Chandra Gandhi, Koock Yoon, Kyuwon Kim, Lakshay Tokas, leike666666, leonard951, Leslie-Fang, Letian Kang, Li, Guizi, Lukas Folle, Lukas Geiger, luxupu, lvli, Ma, Guokai, Mahmoud Abuzaina, Maksym Kysylov, Mandar Deshpande, Manraj Singh Grover, Margaret Maynard-Reid, Mark Ryan, Matt Conley, mbhuiyan, mdfaijul, Mei Jie, merturl, MichaelKonobeev, Michal W. Tarnowski, Mihail Salnikov, Mikalai Drabovich, Mike Holcomb, minds, monklof, Moses Marin, mpppk, Mr. Metal, Mshr-H, musikisomorphie, nammbash, Nathan Luehr, Nayana Thorat, Neeraj Pradhan, Neil, Nick, Nick Lewycky, Niels Ole Salscheider, Niklas Silfverstr\u00f6M, Niranjan Hasabnis, Nuka-137, ocjosen, omeir1, P Sudeepam, Pan Daoxin, Pariksheet Pinjari, Pasquale Minervini, Patrick J. Lopresti, Patrik Gustavsson, Pavel Akhtyamov, PENGWA, per1234, PeterLee, Phan Van Nguyen Duc, Philipp Jund, Phillip Kravtsov, Pooya Davoodi, Pranav Marathe, Putra Manggala, Qingqing Cao, R S Nikhil Krishna, Rajeshwar Reddy T, Ramon Vi\u00f1As, Rasmus Diederichsen, Reuben Morais, robert, Rohit Gupta, Roland Zimmermann, Roman Soldatow, RonLek, Ruizhe, Ryan Jiang, saishruthi, Saleem Abdulrasool, Sami Kama, Sana-Damani, sdamani, Sean Morgan, seanshpark, Sebastien Iooss, Serv-Inc, Severen Redwood, Shashank Gupta, shashvat, Shashvat Chand Shahi, Shubham Goyal, Sigrid Keydana, Siju Samuel, sleighsoft, smilu97, Son Tran, sremedios, Srini511, srinivasan.narayanamoorthy, Subin, Sumesh Udayakumaran, Sungmann Cho, sunway513, sxwang, Tae-Hwan Jung, Taehoon Lee, Takeo Sawada, Taylor Jakobson, Ted Chang, TengLu, terryky, ThisIsIsaac, Thomas Deegan, Thomas Hagebols, tianyapiaozi, Till Hoffmann, Tim Zaman, Tongxuan Liu, Trent Lo, Trevor Morris, TungJerry, Tyorden, Uday Bondhugula, v1incent, Vasileios Lioutas, vbvg2008, Vijay Ravichandran, Viktor Gal, Vincent, Vishnuvardhan Janapati, Vivek Suryamurthy, wangsiyu, wateryzephyr, Wei Wang, Wen-Heng (Jack) Chung, wenxizhu, Will Battel, William D. Irons, winstonq, wyzhao, Xiaoming (Jason) Cui, Xiaoquan Kong, Xin, Xinping Wang, Yann-Yy, Yasir Modak, Yasuhiro Matsumoto, Yong Tang, Yongfeng Gu, Yuan (Terry) Tang, Yuchen Ying, zhangyujing, zyeric, \u738b\u632f\u534e (Zhenhua Wang), \u9ec4\u946b", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.0.0-rc2", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.0.0-rc2", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-rc2", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/20178937"}, {"tag_name": "v1.15.0-rc1", "name": "TensorFlow Release 1.15.0-rc1", "author_name": "goldiegadde", "body": "# Release 1.15.0-rc1\r\nThis is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year. \r\n\r\n## Major Features and Improvements\r\n* As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.\r\n* TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2 module`. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1 module`. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.\r\nThis enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.\r\n* EagerTensor now supports numpy buffer interface for tensors.\r\n* Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.\r\n* Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.\r\n* AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.\r\n* Adds `enable_tensor_equality()`, which switches the behavior such that: \r\n  * Tensors are no longer hashable.\r\n  * Tensors can be compared with == and !=, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.\r\n* Auto Mixed-Precision graph optimizer simplifies converting models to float16 for acceleration on Volta and Turing Tensor Cores. This feature can be enabled by wrapping an optimizer class with `tf.train.experimental.enable_mixed_precision_graph_rewrite()`.\r\n* Add environment variable `TF_CUDNN_DETERMINISTIC`. Setting to \"true\" or \"1\" forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic.\r\n* TensorRT\r\n  * Migrate TensorRT conversion sources from contrib to compiler directory in preparation for TF 2.0.\r\n  * Add additional, user friendly `TrtGraphConverter` API for TensorRT conversion.\r\n  * Expand support for TensorFlow operators in TensorRT conversion (e.g.\r\n    `Gather`, `Slice`, `Pack`, `Unpack`, `ArgMin`, `ArgMax`,`DepthSpaceShuffle`). \r\n  * Support TensorFlow operator `CombinedNonMaxSuppression` in TensorRT conversion which \r\n     significantly accelerates object detection models.\r\n\r\n## Breaking Changes\r\n* Tensorflow code now produces 2 different pip packages: tensorflow_core containing all the code (in the future it will contain only the private implementation) and tensorflow which is a virtual pip package doing forwarding to tensorflow_core (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.\r\n* TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.\r\n* Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.\r\n* `tf.keras`:\r\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.\r\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.\r\n  * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.\r\n  * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer \"layer-name\" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.\r\n  * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).\r\n\r\n## Bug Fixes and Other Changes\r\n* `tf.estimator`:\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint` format, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * Fix tests in canned estimators.\r\n  * Expose Head as public API.\r\n  * Fixes critical bugs that help with `DenseFeatures` usability in TF2\r\n* `tf.data`:\r\n  * Promoting `unbatch` from experimental to core API.\r\n  * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.\r\n* `tf.keras`:\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to tf.train.Checkpoint format, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.\r\n  * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.\r\n  * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.\r\n  * Enable the Keras compile API `experimental_run_tf_function` flag by default. This flag enables single training/eval/predict execution path. With this 1. All input types are converted to `Dataset`. 2. When distribution strategy is not specified this goes through the no-op distribution strategy path. 3. Execution is wrapped in tf.function unless `run_eagerly=True` is set in compile.\r\n  * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.\r\n* `tf.lite`\r\n  * Add `GATHER` support to NN API delegate.\r\n  * tflite object detection script has a debug mode.\r\n  * Add delegate support for `QUANTIZE`.\r\n  * Added evaluation script for COCO minival.\r\n  * Add delegate support for `QUANTIZED_16BIT_LSTM`.\r\n  * Converts hardswish subgraphs into atomic ops.\r\n* Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\r\n* `parallel_for`: Add converter for `MatrixDiag`.\r\n* Add `narrow_range` attribute to `QuantizeAndDequantizeV2` and V3.\r\n* Added new op: `tf.strings.unsorted_segment_join`.\r\n* Add HW acceleration support for `topK_v2`.\r\n* Add new `TypeSpec` classes.\r\n* CloudBigtable version updated to v0.10.0.\r\n* Expose `Head` as public API.\r\n* Update docstring for gather to properly describe the non-empty `batch_dims` case.\r\n* Added `tf.sparse.from_dense` utility function.\r\n* Improved ragged tensor support in `TensorFlowTestCase`.\r\n* Makes the a-normal form transformation in Pyct configurable as to which nodes are converted to variables and which are not.\r\n* `ResizeInputTensor` now works for all delegates.\r\n* Add `EXPAND_DIMS` support to NN API delegate TEST:  expand_dims_test\r\n* `tf.cond` emits a StatelessIf op if the branch functions are stateless and do not touch any resources.\r\n* `tf.cond`, `tf.while` and `if` and `while` in AutoGraph now accept a nonscalar predicate if has a single element. This does not affect non-V2 control flow.\r\n* `tf.while_loop` emits a StatelessWhile op if the cond and body functions are stateless and do not touch any resources.\r\n* Refactors code in Quant8 LSTM support to reduce TFLite binary size.\r\n* Add support of local soft device placement for eager op.\r\n* Add HW acceleration support for `LogSoftMax`.\r\n* Added a function `nested_value_rowids` for ragged tensors.\r\n* Add guard to avoid acceleration of L2 Normalization with input rank != 4\r\n* Add `tf.math.cumulative_logsumexp operation`.\r\n* Add `tf.ragged.stack`.\r\n* Fix memory allocation problem when calling `AddNewInputConstantTensor`.\r\n* Delegate application failure leaves interpreter in valid state.\r\n* Add check for correct memory alignment to `MemoryAllocation::MemoryAllocation()`.\r\n* Extracts `NNAPIDelegateKernel` from nnapi_delegate.cc\r\n* Added support for `FusedBatchNormV3` in converter.\r\n* A ragged to dense op for directly calculating tensors.\r\n* Fix accidental quadratic graph construction cost in graph-mode `tf.gradients()`.\r\n* The `precision_mode` argument to `TrtGraphConverter` is now case insensitive.\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\na6802739, Aaron Ma, Abdullah Selek, Abolfazl Shahbazi, Ag Ramesh, Albert Z. Guo, Albin Joy, Alex Itkes, Alex Sergeev, Alexander Pivovarov, Alexey Romanov, alhkad, Amit Srivastava, amoitra, Andrew Lihonosov, Andrii Prymostka, Anuj Rawat, Astropeak, Ayush Agrawal, Bairen Yi, Bas Aarts, Bastian Eichenberger, Ben Barsdell, Benjamin Peterson, bhack, Bharat Raghunathan, Bhavani Subramanian, Bryan Cutler, candy.dc, Cao Zongyan, Captain-Pool, Casper Da Costa-Luis, Chen Guoyin, Cheng Chang, chengchingwen, Chong Yan, Choong Yin Thong, Christopher Yeh, Clayne Robison, Coady, Patrick, Dan Ganea, David Norman, Denis Khalikov, Deven Desai, Diego Caballero, Duncan Dean, Duncan Riach, Dwight J Lyle, Eamon Ito-Fisher, eashtian3, EFanZh, ejot, Elroy Ashtian Jr, Eric Schweitz, Fangjun Kuang, Fei Hu, fo40225, formath, Fred Reiss, Frederic Bastien, Fredrik Knutsson, G. Hussain Chinoy, Gabriel, gehring, George Grzegorz Pawelczak, Gianluca Varisco, Gleb Popov, Greg Peatfield, Guillaume Klein, Gurpreet Singh, Gustavo Lima Chaves, haison, Haraldur T\u00f3Mas Hallgr\u00edMsson, HarikrishnanBalagopal, H\u00e5Kon Sandsmark, I-Hong, Ilham Firdausi Putra, Imran Salam, Jason Zaman, Jason Zavaglia, jayhpark530, jefby, Jeff Daily, Jeffrey Poznanovic, Jekyll Lai, Jeroen B\u00e9Dorf, Jerry Shih, jerryyin, jiakai, JiangXIAO, Joe Bowser, Joel Shapiro, Johan Gunnarsson, Jojimon Varghese, Joon, Josh Beal, Julian Niedermeier, Jun Wan, Junqin Zhang, Junyuan Xie, Justin Tunis, Kaixi Hou, Karl Lessard, Karthik Muthuraman, Kbhute-Ibm, khanhlvg, Koock Yoon, kstuedem, Kyuwon Kim, Lakshay Tokas, leike666666, leonard951, Leslie-Fang, Leslie-Fang-Intel, Li, Guizi, Lukas Folle, Lukas Geiger, Mahmoud Abuzaina, Manraj Singh Grover, Margaret Maynard-Reid, Mark Ryan, Matt Conley, Matthew Bentham, Matthew Denton, mbhuiyan, mdfaijul, Mei Jie, merturl, MichaelKonobeev, Michal W. Tarnowski, minds, mpppk, musikisomorphie, Nagy Mostafa, Nayana Thorat, Neil, Niels Ole Salscheider, Niklas Silfverstr\u00f6M, Niranjan Hasabnis, ocjosen, olramde, Pariksheet Pinjari, Patrick J. Lopresti, Patrik Gustavsson, per1234, PeterLee, Phan Van Nguyen Duc, Phillip Kravtsov, Pooya Davoodi, Pranav Marathe, Putra Manggala, Qingqing Cao, Rajeshwar Reddy T, Ramon Vi\u00f1As, Rasmus Diederichsen, Reuben Morais, richardbrks, robert, RonLek, Ryan Jiang, saishruthi, Saket Khandelwal, Saleem Abdulrasool, Sami Kama, Sana-Damani, Sergii Khomenko, Severen Redwood, Shubham Goyal, Sigrid Keydana, Siju Samuel, sleighsoft, smilu97, Son Tran, Srini511, srinivasan.narayanamoorthy, Sumesh Udayakumaran, Sungmann Cho, Tae-Hwan Jung, Taehoon Lee, Takeshi Watanabe, TengLu, terryky, TheMindVirus, ThisIsIsaac, Till Hoffmann, Timothy Liu, Tomer Gafner, Tongxuan Liu, Trent Lo, Trevor Morris, Uday Bondhugula, Vasileios Lioutas, vbvg2008, Vishnuvardhan Janapati, Vivek Suryamurthy, Wei Wang, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, winstonq, wyzhao, Xiaoming (Jason) Cui, Xinan Jiang, Xinping Wang, Yann-Yy, Yasir Modak, Yong Tang, Yongfeng Gu, Yuchen Ying, Yuxin Wu, zyeric, \u738b\u632f\u534e (Zhenhua Wang)", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.15.0-rc1", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.15.0-rc1", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.15.0-rc1", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/20036018"}, {"tag_name": "v2.0.0-rc1", "name": "TensorFlow 2.0.0-rc1", "author_name": "goldiegadde", "body": "# Release 2.0.0-rc1\r\n\r\n## Major Features and Improvements\r\n\r\nTensorFlow 2.0 focuses on **simplicity** and **ease of use**, featuring updates like:\r\n\r\n* Easy model building with Keras and eager execution.\r\n* Robust model deployment in production on any platform.\r\n* Powerful experimentation for research.\r\n* API simplification by reducing duplication and removing deprecated endpoints.\r\n\r\nFor details on best practices with 2.0, see [the Effective 2.0 guide](https://www.tensorflow.org/beta/guide/effective_tf2)\r\n\r\n\r\nFor information on upgrading your existing TensorFlow 1.x models, please refer to our [Upgrade](https://medium.com/tensorflow/upgrading-your-code-to-tensorflow-2-0-f72c3a4d83b5) and [Migration](https://www.tensorflow.org/beta/guide/migration_guide) guides. We have also released a collection of [tutorials and getting started guides](https://www.tensorflow.org/beta).\r\n\r\n## Highlights\r\n\r\n* TF 2.0 delivers Keras as the central high level API used to build and train models. Keras provides several model-building APIs such as Sequential, Functional, and Subclassing along with eager execution, for immediate iteration and intuitive debugging, and tf.data, for building scalable input pipelines. Checkout [guide](https://www.tensorflow.org/beta/guide/keras/overview) for additional details.\r\n* Distribution Strategy: TF 2.0 users will be able to use the [`tf.distribute.Strategy`](https://www.tensorflow.org/beta/guide/distribute_strategy) API to distribute training with minimal code changes, yielding great out-of-the-box performance. It supports distributed training with Keras model.fit, as well as with custom training loops. Multi-GPU support is available, along with experimental support for multi worker and Cloud TPUs. Check out the [guide](https://www.tensorflow.org/beta/guide/distribute_strategy) for more details.\r\n* Functions, not Sessions. The traditional declarative programming model of building a graph and executing it via a `tf.Session` is discouraged, and replaced by writing regular Python functions. Using the `tf.function` decorator, such functions can be turned into graphs which can be executed remotely, serialized, and optimized for performance.\r\n* Unification of tf.train.Optimizers and tf.keras.Optimizers. Use tf.keras.Optimizers for TF2.0. `compute_gradients` is removed as public API, and use GradientTape to compute gradients.\r\n* AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with tf.data, tf.distribute and tf.keras APIs. \r\n* Unification of exchange formats to SavedModel. All TensorFlow ecosystem projects (TensorFlow Lite, TensorFlow JS, TensorFlow Serving, TensorFlow Hub) accept SavedModels. Model state should be saved to and restored from SavedModels.\r\n* API Changes: Many API symbols have been renamed or removed, and argument names have changed. Many of these changes are motivated by consistency and clarity. The 1.x API remains available in the compat.v1 module. A list of all symbol changes can be found [here](https://docs.google.com/spreadsheets/d/1FLFJLzg7WNP6JHODX5q8BDgptKafq_slHpnHVbJIteQ/edit#gid=0). \r\n * API clean-up, included removing `tf.app`, `tf.flags`, and `tf.logging` in favor of [absl-py](https://github.com/abseil/abseil-py).\r\n* No more global variables with helper methods like `tf.global_variables_initializer` and `tf.get_global_step`.\r\n* Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.\r\n* Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.\r\n* Fixes autocomplete for most TensorFlow API references by switching to use relative imports in API `__init__.py` files.\r\n\r\n## Breaking Changes\r\n* Many backwards incompatible API changes have been made to clean up the APIs and make them more consistent.\r\n* Toolchains:\r\n  * TensorFlow 2.0.0 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.\r\n  * Tensorflow code now produces 2 different pip packages: tensorflow_core containing all the code (in the future it will contain only the private implementation) and tensorflow which is a virtual pip package doing forwarding to tensorflow_core (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.\r\n\r\n* `tf.contrib`:\r\n  * `tf.contrib` has been deprecated, and functionality has been either migrated to the core TensorFlow API, to an ecosystem project such as [tensorflow/addons](https://www.github.com/tensorflow/addons) or [tensorflow/io](https://www.github.com/tensorflow/io), or removed entirely.\r\n  * Remove `tf.contrib.timeseries` dependency on TF distributions.\r\n  * Replace contrib references with `tf.estimator.experimental.*` for apis in early_stopping.py.\r\n\r\n* `tf.estimator`:\r\n  * Premade estimators in the tf.estimator.DNN/Linear/DNNLinearCombined family have been updated to use `tf.keras.optimizers` instead of the `tf.compat.v1.train.Optimizer`s. If you do not pass in an `optimizer=` arg or if you use a string, the premade estimator will use the Keras optimizer. This is checkpoint breaking, as the optimizers have separate variables. A checkpoint converter tool for converting optimizers is included with the release,  but if you want to avoid any change, switch to the v1 version of the estimator:  `tf.compat.v1.estimator.DNN/Linear/DNNLinearCombined*`.\r\n  * Default aggregation for canned Estimators is now `SUM_OVER_BATCH_SIZE`. To maintain previous default behavior, please pass `SUM` as the loss aggregation method.\r\n  * Canned Estimators don\u2019t support `input_layer_partitioner` arg in the API. If you have this arg, you will have to switch to `tf.compat.v1 canned Estimators`.\r\n  * `Estimator.export_savedmodel` has been renamed `export_saved_model`.\r\n  * When saving to SavedModel, Estimators will strip default op attributes. This is almost always the correct behavior, as it is more forwards compatible, but if you require that default attributes are saved with the model, please use `tf.compat.v1.Estimator`.\r\n  * Feature Columns have been upgraded to be more Eager-friendly and to work with Keras. As a result, `tf.feature_column.input_layer` has been deprecated in favor of `tf.keras.layers.DenseFeatures`. v1 feature columns have direct analogues in v2 except for `shared_embedding_columns`, which are not cross-compatible with v1 and v2. Use `tf.feature_column.shared_embeddings` instead.\r\n\r\n* `tf.keras`:\r\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config.  To configure the number of threads, use `tf.config.threading` APIs.\r\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel. HDF5 files are still supported.\r\n  * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.\r\n  * Layers now default to float32, and automatically cast their inputs to the layer's dtype. If you had a model that used float64, it will probably silently use float32 in TensorFlow 2, and a warning will be issued that starts with \"Layer <layer-name> is casting an input tensor from dtype float64 to the layer's dtype of float32\". To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.\r\n\r\n* `tf.lite`:\r\n  * Removed `lite.OpHint`, `lite.experimental`, and `lite.constant` from 2.0 API.\r\n* Tensors are no longer hashable, but instead compare element-wise with `==` and `!=`. Use `tf.compat.v1.disable_tensor_equality()` to return to the previous behavior.\r\n* Performing equality operations on Tensors or Variables with incompatible shapes an exception is no longer thrown. Instead `__eq__` returns False and `__ne__` returns True.\r\n* Removed `tf.string_split` from v2 API.\r\n* Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.\r\n* Add `UnifiedGRU` as the new GRU implementation for tf2.0. Change the default recurrent activation function for GRU from `hard_sigmoid` to `sigmoid`, and `reset_after` to True in 2.0. Historically recurrent activation is `hard_sigmoid` since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default GRU will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with GRU(recurrent_activation='hard_sigmoid', reset_after=False) to fallback to 1.x behavior.\r\n* `CUDNN_INSTALL_PATH`, `TENSORRT_INSTALL_PATH`, `NCCL_INSTALL_PATH`, `NCCL_HDR_PATH` are deprecated. Use `TF_CUDA_PATHS` instead which supports a comma-separated list of base paths that are searched to find CUDA libraries and headers.\r\n\r\nRefer to our [public project status tracker](https://github.com/orgs/tensorflow/projects/4) and [issues tagged with `2.0`](https://github.com/tensorflow/tensorflow/issues?q=is%3Aopen+is%3Aissue+label%3A2.0) on GitHub for insight into recent issues and development progress.\r\n\r\nIf you experience any snags when using TF 2.0, please let us know at the [TF 2.0 Testing User Group](https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/testing). We have a support mailing list as well as weekly testing meetings, and would love to hear your migration feedback and questions.\r\n\r\n\r\n## Bug Fixes and Other Changes\r\n\r\n* `tf.contrib`:\r\n  * Expose `tf.contrib.proto.*` ops in `tf.io` (they will exist in TF2)\r\n  \r\n* `tf.data`:\r\n  * Add support for TensorArrays to `tf.data Dataset`.\r\n  * Integrate Ragged Tensors with `tf.data`.\r\n  * All core and experimental tf.data transformations that input user-defined functions can span multiple devices now.\r\n  * Extending the TF 2.0 support for `shuffle(..., reshuffle_each_iteration=True)` and `cache()` to work across different Python iterators for the same dataset.\r\n  * Removing the `experimental_numa_aware` option from `tf.data.Options`.\r\n  * Add `num_parallel_reads` and passing in a Dataset containing filenames into `TextLineDataset` and `FixedLengthRecordDataset`.\r\n  * Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\r\n  * Promoting `tf.data.experimental.enumerate_dataset` to core as `tf.data.Dataset.enumerate`.\r\n  * Promoting `tf.data.experimental.unbatch` to core as `tf.data.Dataset.unbatch`.\r\n  * Adds option for introducing slack in the pipeline to reduce CPU contention, via `tf.data.Options().experimental_slack = True`\r\n  * Added experimental support for parallel batching to `batch()` and `padded_batch()`. This functionality can be enabled through tf.data.Options()\r\n  * Support cancellation of long-running `reduce`.\r\n  * Now we use `dataset` node name as prefix instead of the op name, to identify the component correctly in metrics, for pipelines with repeated components.\r\n  * Improve the performance of datasets using `from_tensors()`.\r\n  * Promoting `unbatch` from experimental to core API.\r\n  * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.\r\n\r\n* `tf.distribute`:\r\n  * Enable `tf.distribute.experimental.MultiWorkerMirroredStrategy` working in eager mode.\r\n  * Callbacks are supported in `MultiWorkerMirroredStrategy`.\r\n  * Disable `run_eagerly` and distribution strategy if there are symbolic tensors added to the model using `add_metric` or `add_loss`.\r\n  * Loss and gradients should now more reliably be correctly scaled w.r.t. the global batch size when using a `tf.distribute.Strategy`.\r\n  * Set default loss reduction as `AUTO` for improving reliability of loss scaling with distribution strategy and custom training loops. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used in distribution strategy scope, outside of built-in training loops such as `tf.keras` `compile` and `fit`, we expect reduction value to be 'None' or 'SUM'. Using other values will raise an error.\r\n  * Support for multi-host `ncclAllReduce` in Distribution Strategy.\r\n\r\n* `tf.estimator`:\r\n  * Replace `tf.contrib.estimator.add_metrics` with `tf.estimator.add_metrics`\r\n  * Use `tf.compat.v1.estimator.inputs` instead of `tf.estimator.inputs`\r\n  * Replace contrib references with `tf.estimator.experimental.*` for apis in early_s in Estimator\r\n  * Canned Estimators will now use keras optimizers by default. An error will be raised if tf.train.Optimizers are used, and you will have to switch to tf.keras.optimizers or tf.compat.v1 canned Estimators.\r\n  * A checkpoint converter for canned Estimators has been provided to transition canned Estimators that are warm started from `tf.train.Optimizers` to `tf.keras.optimizers`.\r\n  * Losses are scaled in canned estimator v2 and not in the optimizers anymore. If you are using Estimator + distribution strategy + optimikzer v1 then the behavior does not change. This implies that if you are using custom estimator with optimizer v2, you have to scale losses. We have new utilities to help scale losses `tf.nn.compute_average_loss`, `tf.nn.scale_regularization_loss`.\r\n\r\n* `tf.keras`:\r\n  * Premade models (including Linear and WideDeep) have been introduced for the purpose of replacing Premade estimators.\r\n  * Model saving changes\r\n  * `model.save` and `tf.saved_model.save` may now save to the TensorFlow SavedModel format. The model can be restored using `tf.keras.models.load_model`. HDF5 files are still supported, and may be used by specifying `save_format=\"h5\"` when saving.\r\n  * Raw TensorFlow functions can now be used in conjunction with the Keras Functional API during model creation. This obviates the need for users to create Lambda layers in most cases when using the Functional API. Like Lambda layers, TensorFlow functions that result in Variable creation or assign ops are not supported.\r\n  * Add support for passing list of lists to the `metrics` argument in Keras `compile`.\r\n  * Add `tf.keras.layers.AbstractRNNCell` as the preferred implementation for RNN cells in TF v2. User can use it to implement RNN cells with custom behavior.\r\n  * Keras training and validation curves are shown on the same plot when using the TensorBoard callback.\r\n  * Switched Keras `fit/evaluate/predict` execution to use only a single unified path by default unless eager execution has been explicitly disabled, regardless of input type. This unified path places an eager-friendly training step inside of a `tf.function`. With this \r\n   1.  All input types are converted to `Dataset`.\r\n   2. The path assumes there is always a distribution strategy. when distribution strategy is not specified the path uses a no-op distribution strategy. \r\n   3. The training step is wrapped in `tf.function` unless `run_eagerly=True` is set in compile. The single path execution code does not yet support all use cases. We fallback to the existing v1 execution paths if your model contains the following: \r\n     1. `sample_weight_mode` in compile \r\n     2. `weighted_metrics` in compile \r\n     3. v1 optimizer \r\n     4. target tensors in compile\r\nIf you are experiencing any issues because of this change, please inform us (file an issue) about your use case and you can unblock yourself by setting `experimental_run_tf_function=False` in compile meanwhile. We have seen couple of use cases where the model usage pattern is not as expected and would not work with this change.\r\n   1. output tensors of one layer is used in the constructor of another.\r\n   2. symbolic tensors outside the scope of the model are used in custom loss functions.\r\n   The flag can be disabled for these cases and ideally the usage pattern will need to be fixed.\r\n  * Mark Keras `set_session` as `compat.v1` only.\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint format`, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.\r\n  * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.\r\n  * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.\r\n  * Update TF 2.0 `keras.backend.name_scope` to use TF 2.0 `name_scope`.\r\n  * Add v2 module aliases for losses, metrics, initializers and optimizers: `tf.losses = tf.keras.losses` & `tf.metrics = tf.keras.metrics` &  `tf.initializers = tf.keras.initializers` & `tf.optimizers = tf.keras.optimizers`.\r\n  * Updates binary cross entropy logic in Keras when input is probabilities. Instead of converting probabilities to logits, we are using the cross entropy formula for probabilities.\r\n  * Added public APIs for `cumsum` and `cumprod` keras backend functions.\r\n  * Add support for temporal sample weight mode in subclassed models.\r\n  * Raise ValueError if an integer is passed to the training APIs. \r\n  * Added fault-tolerance support for training Keras model via `model.fit()` with `MultiWorkerMirroredStrategy`, tutorial available.\r\n  * Custom Callback tutorial is now available.\r\n  * To train with `tf.distribute`, Keras api is recommended over estimator.\r\n  * `steps_per_epoch` and `steps` arguments are supported with numpy arrays.\r\n  * New error message when unexpected keys are used in sample_weight/class_weight dictionaries \r\n  * Losses are scaled in Keras compile/fit and not in the optimizers anymore. If you are using custom training loop, we have new utilities to help scale losses `tf.nn.compute_average_loss`, `tf.nn.scale_regularization_loss`.\r\n  * `Layer` apply and add_variable APIs are deprecated.\r\n  * Added support for channels first data format in cross entropy losses with logits and support for tensors with unknown ranks.\r\n  * Error messages will be raised if `add_update`, `add_metric`, `add_loss`, activity regularizers are used inside of a control flow branch.\r\n  * New loss reduction types: \r\n    1. `AUTO`: Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When    used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, we expect reduction value to be `SUM` or `NONE`. Using `AUTO` in that case will raise an error. \r\n    2. `NONE`: Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like `fit`/`evaluate`, the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. \r\n    3. `SUM`: Scalar sum of weighted losses. 4. `SUM_OVER_BATCH_SIZE`: Scalar `SUM` divided by number of elements in losses. This reduction type is not supported when used with `tf.distribute.Strategy` outside of built-in training loops like `tf.keras` `compile`/`fit`.\r\n  * Wraps losses passed to the `compile` API (strings and v1 losses) which are not instances of v2 `Loss` class in `LossWrapper` class. => All losses will now use `SUM_OVER_BATCH_SIZE` reduction as default.\r\n  * `model.add_loss(symbolic_tensor)` should work in ambient eager.\r\n  * Update metric name to always reflect what the user has given in compile. Affects following cases \r\n    1. When name is given as 'accuracy'/'crossentropy' \r\n    2. When an aliased function name is used eg. 'mse' \r\n    3. Removing the `weighted` prefix from weighted metric names.\r\n  * Allow non-Tensors through v2 losses.\r\n  * Add v2 sparse categorical crossentropy metric.\r\n  * Add v2 APIs for `AUCCurve` and `AUCSummationMethod` enums.\r\n  * `add_update` can now be passed a zero-arg callable in order to support turning off the update when setting `trainable=False` on a Layer of a Model compiled with `run_eagerly=True`.\r\n  * Standardize the LayerNormalization API by replacing the args `norm_axis` and `params_axis` with `axis`.\r\n  * Fixed critical bugs that help with DenseFeatures usability in TF2\r\n\r\n* `tf.lite`:\r\n  * Added evaluation script for `COCO` minival\r\n  * Add delegate support for `QUANTIZE`.\r\n  * Add `GATHER` support to NN API delegate.\r\n  * Added support for TFLiteConverter Python API in 2.0. Contains functions from_saved_model, from_keras_file, and from_concrete_functions.\r\n  * Add `EXPAND_DIMS` support to NN API delegate TEST.\r\n  * Add `narrow_range` attribute to QuantizeAndDequantizeV2 and V3.\r\n  * Added support for `tflite_convert` command line tool in 2.0.\r\n  * Post-training quantization tool supports quantizing weights shared by multiple operations. The models made with versions of this tool will use INT8 types for weights and will only be executable interpreters from this version onwards.\r\n  * Post-training quantization tool supports fp16 weights and GPU delegate acceleration for fp16.\r\n  * Add delegate support for `QUANTIZED_16BIT_LSTM`.\r\n  * Extracts `NNAPIDelegateKernel` from nnapi_delegate.cc\r\n  \r\n* Other:\r\n  * Fix accidental quadratic graph construction cost in graph-mode `tf.gradients()`.\r\n  * ResourceVariable's gather op supports batch dimensions.\r\n  * ResourceVariable support for `gather_nd`.\r\n  * `ResourceVariable` and `Variable` no longer accepts `constraint` in the constructor, nor expose it as a @property.\r\n  * Added gradient for `SparseToDense` op.\r\n  * Expose a flag that allows the number of threads to vary across Python benchmarks.\r\n  * `image.resize` in 2.0 now supports gradients for the new resize kernels.\r\n  * `image.resize` now considers proper pixel centers and has new kernels (incl. anti-aliasing).\r\n  * Renamed `tf.image` functions to remove duplicate \"image\" where it is redundant.\r\n  * Variadic reduce is supported on CPU Variadic reduce is supported on CPU\r\n  * Remove unused `StringViewVariantWrapper`.\r\n  * Delete unused `Fingerprint64Map` op registration\r\n  * Add broadcasting support to `tf.matmul`.\r\n  * Add C++ Gradient for `BatchMatMulV2`.\r\n  * Add `tf.math.cumulative_logsumexp` operation.\r\n  * Add ellipsis (...) support for `tf.einsum()`.\r\n  * Add expand_composites argument to all `nest.*` methods.\r\n  * Added `strings.byte_split`.\r\n  * Add a new \"result_type\" parameter to `tf.strings.split`.\r\n  * Add name argument to `tf.string_split` and `tf.strings_split`.\r\n  * Extend `tf.strings.split` to support inputs with any rank.\r\n  * Added `tf.random.binomial`.\r\n  * Added `key` and `skip` methods to `random.experimental.Generator`.\r\n  * Extend `tf.function` with basic support for CompositeTensors arguments (such as `SparseTensor` and `RaggedTensor`).\r\n  * `parallel_for.pfor`: add converters for Softmax, LogSoftmax, IsNaN, All, Any, and MatrixSetDiag.\r\n  * `parallel_for`: add converters for LowerTriangularSolve and Cholesky.\r\n  * `parallel_for`: add converters for `LogMatrixDeterminant` and `MatrixBandPart`.\r\n  * `parallel_for`: Add converter for `MatrixDiag`.\r\n  * `parallel_for`: Add converters for `OneHot`, `LowerBound`, `UpperBound`.\r\n  * `parallel_for`: add converter for `BroadcastTo`.\r\n  * Add `pfor` converter for `Squeeze`.\r\n  * Add `RaggedTensor.placeholder()`.\r\n  * Add ragged tensor support to `tf.squeeze`.\r\n  * Update RaggedTensors to support int32 row_splits.\r\n  * Allow `LinearOperator.solve` to take a `LinearOperator`.\r\n  * Allow all dtypes for `LinearOperatorCirculant`.\r\n  * Introduce MaxParallelism method\r\n  * Add `LinearOperatorHouseholder`.\r\n  * Adds Philox support to new stateful RNG's XLA path.\r\n  * Add `TensorSpec` support for CompositeTensors.\r\n  * Add `tf.linalg.tridiagonal_solve` op.\r\n  * Added partial_pivoting input parameter to `tf.linalg.tridiagonal_solve`.\r\n  * Added gradient to `tf.linalg.tridiagonal_solve`.\r\n  * Add `tf.linalg.tridiagonal_mul op`.\r\n  * Added GPU implementation of `tf.linalg.tridiagonal_matmul`.\r\n  * Add `LinearOperatorToeplitz`.\r\n  * Upgraded LIBXSMM to version 1.11.\r\n  * Uniform processing of quantized embeddings by Gather and EmbeddingLookup Ops\r\n  * Correct a misstatement in the documentation of the sparse softmax cross entropy logit parameter.\r\n  * Add `tf.ragged.boolean_mask`.\r\n  * `tf.switch_case` added, which selects a branch_fn based on a branch_index.\r\n  * The C++ kernel of gather op supports batch dimensions.\r\n  * Fixed default value and documentation for `trainable` arg of tf.Variable.\r\n  * EagerTensor now supports numpy buffer interface for tensors.\r\n  * This change bumps the version number of the FullyConnected Op to 5.\r\n  * Added new op: `tf.strings.unsorted_segment_join`.\r\n  * Add HW acceleration support for `topK_v2`.\r\n  * CloudBigtable version updated to v0.10.0 BEGIN_PUBLIC CloudBigtable version updated to v0.10.0.\r\n  * Expose `Head` as public API.\r\n  * Added `tf.sparse.from_dense` utility function.\r\n  * Improved ragged tensor support in `TensorFlowTestCase`.\r\n  * Added a function `nested_value_rowids` for ragged tensors.\r\n  * Add `tf.ragged.stack`.\r\n  * Makes the a-normal form transformation in Pyct configurable as to which nodes are converted to variables and which are not.\r\n  * `ResizeInputTensor` now works for all delegates.\r\n  * `tf.cond` emits a StatelessIf op if the branch functions are stateless and do not touch any resources.\r\n  * Add support of local soft device placement for eager op.\r\n  * Pass partial_pivoting to the `_TridiagonalSolveGrad`.\r\n  * Add HW acceleration support for `LogSoftMax`.\r\n  * Add guard to avoid acceleration of L2 Normalization with input rank != 4\r\n  * Fix memory allocation problem when calling `AddNewInputConstantTensor`.\r\n  * Delegate application failure leaves interpreter in valid state\r\n  * `tf.while_loop` emits a StatelessWhile op if the cond and body functions are stateless and do not touch any resources.\r\n  * tf.cond, tf.while and if and while in AutoGraph now accept a nonscalar predicate if has a single element. This does not affec non-V2 control flow.\r\n  * Fix potential security vulnerability where decoding variant tensors from proto could result in heap out of bounds memory access.\r\n  * Only create a GCS directory object if the object does not already exist.\r\n  * Introduce `dynamic` constructor argument in Layer and Model, which should be set to True when using imperative control flow in the `call` method.\r\n  * Begin adding Go wrapper for C Eager API.\r\n  * XLA HLO graphs can be inspected with interactive_graphviz tool now.\r\n  * Add dataset ops to the graph (or create kernels in Eager execution) during the python Dataset object creation instead doing it during Iterator creation time.\r\n  * Add `batch_dims` argument to `tf.gather`.\r\n  * The behavior of `tf.gather` is now correct when axis=None and batch_dims<0.\r\n  * Update docstring for gather to properly describe the non-empty `batch_dims` case.\r\n  * Removing of dtype in the constructor of initializers and partition_info in call.\r\n  * Add `tf.math.nextafter` op.\r\n  * Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically dispatches the best kernel implementation based on CPU vector architecture. To disable them, build with `--define=tensorflow_mkldnn_contraction_kernel=0`.\r\n  * `tf.linspace(start, stop, num)` now always uses \"stop\" as last value (for num > 1)\r\n  * Added top-k to precision and recall to keras metrics.\r\n  * Add a ragged size op and register it to the op dispatcher\r\n  * Transitive dependencies on :`pooling_ops` were removed.  Some users may need to add explicit dependencies on :`pooling_ops` if they reference the operators from that library.\r\n  * Add `CompositeTensor` base class.\r\n  * Malformed gif images could result in an access out of bounds in the color palette of the frame. This has been fixed now\r\n  * Add templates and interfaces for creating lookup tables\r\n  * `Tensor::UnsafeCopyFromInternal` deprecated in favor `Tensor::BitcastFrom`.\r\n  * In `map_vectorization` optimization, reduce the degree of parallelism in the vectorized map node.\r\n  * Add variant wrapper for `absl::string_view`.\r\n  * Add OpKernels for some stateless maps.\r\n  * DType is no longer convertible to an int. Use `dtype.as_datatype_enum` instead of `int(dtype)` to get the same result.\r\n  * Support both binary and -1/1 label input in v2 hinge and squared hinge losses.\r\n  * Added `LinearOperator.adjoint` and `LinearOperator.H` (alias).\r\n  * Expose CriticalSection in core as `tf.CriticalSection`.\r\n  * Enhanced graphviz output.\r\n  * Add opkernel templates for common table operations.\r\n  * Fix callbacks do not log values in eager mode when a deferred build model is used.\r\n  * `SignatureDef` util functions have been deprecated.\r\n  * Update `Fingerprint64Map` to use aliases\r\n  * Add legacy string flat hash map op kernels.\r\n  * Add support for `add_metric` in the graph function mode.\r\n  * Updating cosine similarity loss - removed the negate sign from cosine similarity.\r\n  * Changed default for gradient accumulation for TPU embeddings to true.\r\n  * Adds summary trace API for collecting graph and profile information. \r\n  \r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n1e100, a6802739, Abolfazl Shahbazi, Adam Weiss, Ag Ramesh, Alan Du, Albin Joy, Amit, Amit Srivastava, Andy Craze, Anshuman Tripathy, Armen Poghosov, armenpoghosov, Arpit Shah, Ashwin Ramaswami, Aurelien Geron, Aur\u00e9Lien Geron, aweers, awesomealex1, Bairen Yi, Ben Barsdell, Bhavani Subramanian, Brandon Carter, candy.dc, Chao Liu, Clayne Robison, csukuangfj, Dan Jarvis, Dan Lazewatsky, Daniel Ingram, Dave Airlie, David Norman, Dayananda V, Denis Khalikov, Deven Desai, Dheeraj Rajaram Reddy, dmitrievanthony, Drew Szurko, Duncan Riach, Fei Hu, Felix Lemke, Filip Matzner, fo40225, frreiss, Gautam, gehring, Grzegorz George Pawelczak, Grzegorz Pawelczak, HanGuo97, Hari Shankar, hehongliang, Heungsub Lee, Hoeseong Kim, I-Hong Jhuo, Ilango R, Innovimax, Jacky Ko, Jakub Lipinski,  jcf94, Jeff Poznanovic, Jia Qingtong, Jiankang, Joe Q, Joe Quadrino, Jonas Rauber, Jonathan Kyl, Joppe Geluykens, Joseph Friedman, jtressle, jwu, K. Hodges, kaixih, Karl Lessard, Karl Weinmeister, Kashif Rasul, kjopek, Koan-Sin Tan, kouml, ktaebum, Laurent Le Brun, Li, Guizi, Loo Rong Jie, Lucas Hendren, Lukas Geiger, Luke Han, Mahmoud Abuzaina, manhyuk, Marco Gaido, Marek Drozdowski, Mark Ryan, mars20, Mateusz Chudyk, Matt Conley, MattConley, mbhuiyan, mdfaijul, Melissa Grueter, Michael K\u00e4Ufl, Micka\u00ebL Schoentgen, Miguel Morin, Mike Arpaia, nammbash, Natalia Gimelshein, Nayana-Ibm, neargye, Nehal J Wani, Niels Ole Salscheider, Niranjan Hasabnis, Nutti, olicht, P Sudeepam, Paige Bailey, Palmer Lao, Pariksheet Pinjari, Pavel Samolysov, Pooya Davoodi, Ryan Jiang, Samantha Andow, Sami Kama, Saurabh Deoras, Shahzad Lone, Shashi, Siju, Siju Samuel, Snease-Abq, Spencer Schaber, srinivasan.narayanamoorthy, Steve Lang, Steve Nesae, Supriya Rao, Taylor Jakobson, Taylor Thornton, ThisIsPIRI, Thomas Deegan, tomguluson92, Tongxuan Liu, Vagif, vcarpani, Vikram Tiwari, Vishwak Srinivasan, Vitor-Alves, wangsiyu, WeberXie, WeijieSun, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, Yan Facai (\u989c\u53d1\u624d), ymodak, Yong Tang, Younes Khoudli, Yuan Lin, Yves-Noel Weweler, zjjott, \u535c\u5c45, \u738b\u632f\u534e (Wang Zhenhua), \r\n\r\n4d55397500, a6802739, Abdullah Selek, abenmao, Adam Richter, Ag Ramesh, Albin Joy, Alex, Alex Itkes, Alex Sergeev, Alexander Pivovarov, Alexey Romanov, alhkad, Aman Patel, Amit, Amit Kumar Jaiswal, Amit Srivastava, amoitra, Andreas Eberle, Andrew Lihonosov, Anthony Hsu, Anthony Platanios, Anuj Rawat, arp95, Arpit Shah, Astropeak, Augustina Ragwitz, Aurelien Geron, Aur\u00e9Lien Geron, avasid, aweers, Ayush Agrawal, Bas Aarts, Bastian Eichenberger, Bayberry Z, Ben Barsdell, Benjamin Peterson, bhack, Bharat Raghunathan, Bhavani Subramanian, Bin Fan, blairhan, Bl\u00e9Nesi Attila, Bodin-E, Bryan Cutler, Cao Zongyan, Casper Da Costa-Luis, Chen Guoyin, chenchc, chengchingwen, chie8842, Christian Hansen, Christoph Boeddeker, Christopher Yeh, Clayne Robison, Coady, Patrick, crafet, ctiijima, Daniel Rasmussen, Daniel Salvadori, David Norman, delock, Denis Khalikov, Deven Desai, Diego Caballero, Donovan Ong, Duncan Dean, Duncan Riach, Dustin Neighly, Dwight J Lyle, Eamon Ito-Fisher, eashtian3, Edward Forgacs, EFanZh, ejot, Elroy Ashtian Jr, Eric Schweitz, Evgeniy Polyakov, Fangjun Kuang, Federico Martinez, Fei Hu, Filip Matzner, FlashTek, fo40225, formath, Franc\u0327Ois Chollet, Fred Reiss, Frederic Bastien, Fredrik Knutsson, G. Hussain Chinoy, Gabriel, gehring, Geoffrey Irving, George Grzegorz Pawelczak, George Sterpu, Gianluca Varisco, Gleb Popov, Greg Peatfield, Guillaume Klein, Gurpreet Singh, Gustavo Lima Chaves, Gyoung-Yoon Ryoo, haison, Hanton Yang, Haraldur T\u00f3Mas Hallgr\u00edMsson, Huan Li (\u674e\u5353\u6853), H\u00e5Kon Sandsmark, I-Hong, Ilham Firdausi Putra, Imran Salam, Irene Dea, Ivan Habernal, Jacky, Jason Zaman, Jason Zavaglia, jayhpark530, jefby, Jeff Daily, Jeffrey Poznanovic, Jekyll Lai, jer, Jeroen B\u00e9Dorf, jerryyin, jhalakp, jiakai, JiangXIAO, Joe Bowser, Joel Shapiro, Johan Gunnarsson, Jojimon Varghese, Jonathan, Joon, Josh Beal, Julian Niedermeier, Junqin Zhang, Justin Dujardin, Justin Tunis, Kaixi Hou, Karthik Muthuraman, Kay Zhu, Kbhute-Ibm, KDR, Keno Fischer, Kevin Mader, khanhlvg, Kilaru Yasaswi Sri Chandra Gandhi, Koock Yoon, Kyuwon Kim, Lakshay Tokas, leike666666, leonard951, Leslie-Fang, Letian Kang, Li, Guizi, Lukas Folle, Lukas Geiger, luxupu, lvli, Ma, Guokai, Mahmoud Abuzaina, Maksym Kysylov, Mandar Deshpande, Manraj Singh Grover, Margaret Maynard-Reid, Mark Ryan, Matt Conley, mbhuiyan, mdfaijul, Mei Jie, merturl, MichaelKonobeev, Michal W. Tarnowski, Mihail Salnikov, Mikalai Drabovich, Mike Holcomb, minds, monklof, Moses Marin, mpppk, Mr. Metal, Mshr-H, musikisomorphie, nammbash, Nathan Luehr, Nayana Thorat, Neeraj Pradhan, Neil, Nick, Nick Lewycky, Niels Ole Salscheider, Niklas Silfverstr\u00f6M, Niranjan Hasabnis, Nuka-137, ocjosen, omeir1, P Sudeepam, Pan Daoxin, Pariksheet Pinjari, Pasquale Minervini, Patrick J. Lopresti, Patrik Gustavsson, Pavel Akhtyamov, PENGWA, per1234, PeterLee, Phan Van Nguyen Duc, Philipp Jund, Phillip Kravtsov, Pooya Davoodi, Pranav Marathe, Putra Manggala, Qingqing Cao, R S Nikhil Krishna, Rajeshwar Reddy T, Ramon Vi\u00f1As, Rasmus Diederichsen, Reuben Morais, robert, Rohit Gupta, Roland Zimmermann, Roman Soldatow, RonLek, Ruizhe, Ryan Jiang, saishruthi, Saleem Abdulrasool, Sami Kama, Sana-Damani, sdamani, Sean Morgan, seanshpark, Sebastien Iooss, Serv-Inc, Severen Redwood, Shashank Gupta, shashvat, Shashvat Chand Shahi, Shubham Goyal, Sigrid Keydana, Siju Samuel, sleighsoft, smilu97, Son Tran, sremedios, Srini511, srinivasan.narayanamoorthy, Subin, Sumesh Udayakumaran, Sungmann Cho, sunway513, sxwang, Tae-Hwan Jung, Taehoon Lee, Takeo Sawada, Taylor Jakobson, Ted Chang, TengLu, terryky, ThisIsIsaac, Thomas Deegan, Thomas Hagebols, tianyapiaozi, Till Hoffmann, Tim Zaman, Tongxuan Liu, Trent Lo, Trevor Morris, TungJerry, Tyorden, Uday Bondhugula, v1incent, Vasileios Lioutas, vbvg2008, Vijay Ravichandran, Viktor Gal, Vincent, Vishnuvardhan Janapati, Vivek Suryamurthy, wangsiyu, wateryzephyr, Wei Wang, Wen-Heng (Jack) Chung, wenxizhu, Will Battel, William D. Irons, winstonq, wyzhao, Xiaoming (Jason) Cui, Xiaoquan Kong, Xin, Xinping Wang, Yann-Yy, Yasir Modak, Yasuhiro Matsumoto, Yong Tang, Yongfeng Gu, Yuan (Terry) Tang, Yuchen Ying, zhangyujing, zyeric, \u738b\u632f\u534e (Zhenhua Wang), \u9ec4\u946b", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.0.0-rc1", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.0.0-rc1", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-rc1", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/19925830"}, {"tag_name": "v1.15.0-rc0", "name": "TensorFlow 1.15.0-rc0", "author_name": "goldiegadde", "body": "# Release 1.15.0-rc0\r\nThis is the last 1.x release for TensorFlow. We do not expect to update the 1.x branch with features, although we will issue patch releases to fix vulnerabilities for at least one year. \r\n\r\n## Major Features and Improvements\r\n* As [announced](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/iRCt5m4qUz0), `tensorflow` pip package will by default include GPU support (same as `tensorflow-gpu` now) for the platforms we currently have GPU support (Linux and Windows). It will work on machines with and without Nvidia GPUs. `tensorflow-gpu` will still be available, and CPU-only packages can be downloaded at `tensorflow-cpu` for users who are concerned about package size.\r\n* TensorFlow 1.15 contains a complete implementation of the 2.0 API in its `compat.v2 module`. It contains a copy of the 1.15 main module (without `contrib`) in the `compat.v1 module`. TensorFlow 1.15 is able to emulate 2.0 behavior using the `enable_v2_behavior()` function.\r\nThis enables writing forward compatible code: by explicitly importing either `tensorflow.compat.v1` or `tensorflow.compat.v2`, you can ensure that your code works without modifications against an installation of 1.15 or 2.0.\r\n* EagerTensor now supports numpy buffer interface for tensors.\r\n* Add toggles `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()` for enabling/disabling v2 control flow.\r\n* Enable v2 control flow as part of `tf.enable_v2_behavior()` and `TF2_BEHAVIOR=1`.\r\n* AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with `tf.data`, `tf.distribute` and `tf.keras` APIS.\r\n* Adds `enable_tensor_equality()`, which switches the behavior such that: \r\n  * Tensors are no longer hashable.\r\n  * Tensors can be compared with == and !=, yielding a Boolean Tensor with element-wise comparison results. This will be the default behavior in 2.0.\r\n\r\n## Breaking Changes\r\n* Tensorflow code now produces 2 different pip packages: tensorflow_core containing all the code (in the future it will contain only the private implementation) and tensorflow which is a virtual pip package doing forwarding to tensorflow_core (and in the future will contain only the public API of tensorflow). We don't expect this to be breaking, unless you were importing directly from the implementation.\r\n* TensorFlow 1.15 is built using devtoolset7 (GCC7) on Ubuntu 16. This may lead to ABI incompatibilities with extensions built against earlier versions of TensorFlow.\r\n* Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.\r\n* `tf.keras`:\r\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config. To configure the number of threads, use `tf.config.threading` APIs.\r\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.\r\n  * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.\r\n  * Layers now default to `float32`, and automatically cast their inputs to the layer's dtype. If you had a model that used `float64`, it will probably silently use `float32` in TensorFlow2, and a warning will be issued that starts with Layer \"layer-name\" is casting an input tensor from dtype float64 to the layer's dtype of float32. To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.\r\n  * Some `tf.assert_*` methods now raise assertions at operation creation time (i.e. when this Python line executes) if the input tensors' values are known at that time, not during the session.run(). When this happens, a noop is returned and the input tensors are marked non-feedable. In other words, if they are used as keys in `feed_dict` argument to `session.run()`, an error will be raised. Also, because some assert ops don't make it into the graph, the graph structure changes. A different graph can result in different per-op random seeds when they are not given explicitly (most often).\r\n\r\n## Bug Fixes and Other Changes\r\n* `tf.data`:\r\n  * Promoting `unbatch` from experimental to core API.\r\n  * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.\r\n* `tf.keras`:\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to tf.train.Checkpoint format, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.\r\n  * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and `tf.keras.models.load_model` instead.\r\n  * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.\r\n  * Enable the Keras compile API `experimental_run_tf_function` flag by default. This flag enables single training/eval/predict execution path. With this 1. All input types are converted to `Dataset`. 2. When distribution strategy is not specified this goes through the no-op distribution strategy path. 3. Execution is wrapped in tf.function unless `run_eagerly=True` is set in compile.\r\n  * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.\r\n* `tf.lite`\r\n  * Add `GATHER` support to NN API delegate.\r\n  * tflite object detection script has a debug mode.\r\n  * Add delegate support for `QUANTIZE`.\r\n  * Added evaluation script for COCO minival.\r\n  * Add delegate support for `QUANTIZED_16BIT_LSTM`.\r\n  * Converts hardswish subgraphs into atomic ops.\r\n* Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\r\n* `parallel_for`: Add converter for `MatrixDiag`.\r\n* Add `narrow_range` attribute to `QuantizeAndDequantizeV2` and V3.\r\n* Added new op: `tf.strings.unsorted_segment_join`.\r\n* Add HW acceleration support for `topK_v2`.\r\n* Add new `TypeSpec` classes.\r\n* CloudBigtable version updated to v0.10.0.\r\n* Expose `Head` as public API.\r\n* Update docstring for gather to properly describe the non-empty `batch_dims` case.\r\n* Added `tf.sparse.from_dense` utility function.\r\n* Improved ragged tensor support in `TensorFlowTestCase`.\r\n* Makes the a-normal form transformation in Pyct configurable as to which nodes are converted to variables and which are not.\r\n* `ResizeInputTensor` now works for all delegates.\r\n* Add `EXPAND_DIMS` support to NN API delegate TEST:  expand_dims_test\r\n* `tf.cond` emits a StatelessIf op if the branch functions are stateless and do not touch any resources.\r\n* `tf.cond`, `tf.while` and `if` and `while` in AutoGraph now accept a nonscalar predicate if has a single element. This does not affect non-V2 control flow.\r\n* `tf.while_loop` emits a StatelessWhile op if the cond and body functions are stateless and do not touch any resources.\r\n* Refactors code in Quant8 LSTM support to reduce TFLite binary size.\r\n* Add support of local soft device placement for eager op.\r\n* Add HW acceleration support for `LogSoftMax`.\r\n* Added a function `nested_value_rowids` for ragged tensors.\r\n* Add guard to avoid acceleration of L2 Normalization with input rank != 4\r\n* Add `tf.math.cumulative_logsumexp operation`.\r\n* Add `tf.ragged.stack`.\r\n* Fix memory allocation problem when calling `AddNewInputConstantTensor`.\r\n* Delegate application failure leaves interpreter in valid state.\r\n* Add check for correct memory alignment to `MemoryAllocation::MemoryAllocation()`.\r\n* Extracts `NNAPIDelegateKernel` from nnapi_delegate.cc\r\n* Added support for `FusedBatchNormV3` in converter.\r\n* A ragged to dense op for directly calculating tensors.\r\n* Fix accidental quadratic graph construction cost in graph-mode `tf.gradients()`.\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\na6802739, Aaron Ma, Abdullah Selek, Abolfazl Shahbazi, Ag Ramesh, Albert Z. Guo, Albin Joy, Alex Itkes, Alex Sergeev, Alexander Pivovarov, Alexey Romanov, alhkad, Amit Srivastava, amoitra, Andrew Lihonosov, Andrii Prymostka, Anuj Rawat, Astropeak, Ayush Agrawal, Bairen Yi, Bas Aarts, Bastian Eichenberger, Ben Barsdell, Benjamin Peterson, bhack, Bharat Raghunathan, Bhavani Subramanian, Bryan Cutler, candy.dc, Cao Zongyan, Captain-Pool, Casper Da Costa-Luis, Chen Guoyin, Cheng Chang, chengchingwen, Chong Yan, Choong Yin Thong, Christopher Yeh, Clayne Robison, Coady, Patrick, Dan Ganea, David Norman, Denis Khalikov, Deven Desai, Diego Caballero, Duncan Dean, Duncan Riach, Dwight J Lyle, Eamon Ito-Fisher, eashtian3, EFanZh, ejot, Elroy Ashtian Jr, Eric Schweitz, Fangjun Kuang, Fei Hu, fo40225, formath, Fred Reiss, Frederic Bastien, Fredrik Knutsson, G. Hussain Chinoy, Gabriel, gehring, George Grzegorz Pawelczak, Gianluca Varisco, Gleb Popov, Greg Peatfield, Guillaume Klein, Gurpreet Singh, Gustavo Lima Chaves, haison, Haraldur T\u00f3Mas Hallgr\u00edMsson, HarikrishnanBalagopal, H\u00e5Kon Sandsmark, I-Hong, Ilham Firdausi Putra, Imran Salam, Jason Zaman, Jason Zavaglia, jayhpark530, jefby, Jeff Daily, Jeffrey Poznanovic, Jekyll Lai, Jeroen B\u00e9Dorf, Jerry Shih, jerryyin, jiakai, JiangXIAO, Joe Bowser, Joel Shapiro, Johan Gunnarsson, Jojimon Varghese, Joon, Josh Beal, Julian Niedermeier, Jun Wan, Junqin Zhang, Junyuan Xie, Justin Tunis, Kaixi Hou, Karl Lessard, Karthik Muthuraman, Kbhute-Ibm, khanhlvg, Koock Yoon, kstuedem, Kyuwon Kim, Lakshay Tokas, leike666666, leonard951, Leslie-Fang, Leslie-Fang-Intel, Li, Guizi, Lukas Folle, Lukas Geiger, Mahmoud Abuzaina, Manraj Singh Grover, Margaret Maynard-Reid, Mark Ryan, Matt Conley, Matthew Bentham, Matthew Denton, mbhuiyan, mdfaijul, Mei Jie, merturl, MichaelKonobeev, Michal W. Tarnowski, minds, mpppk, musikisomorphie, Nagy Mostafa, Nayana Thorat, Neil, Niels Ole Salscheider, Niklas Silfverstr\u00f6M, Niranjan Hasabnis, ocjosen, olramde, Pariksheet Pinjari, Patrick J. Lopresti, Patrik Gustavsson, per1234, PeterLee, Phan Van Nguyen Duc, Phillip Kravtsov, Pooya Davoodi, Pranav Marathe, Putra Manggala, Qingqing Cao, Rajeshwar Reddy T, Ramon Vi\u00f1As, Rasmus Diederichsen, Reuben Morais, richardbrks, robert, RonLek, Ryan Jiang, saishruthi, Saket Khandelwal, Saleem Abdulrasool, Sami Kama, Sana-Damani, Sergii Khomenko, Severen Redwood, Shubham Goyal, Sigrid Keydana, Siju Samuel, sleighsoft, smilu97, Son Tran, Srini511, srinivasan.narayanamoorthy, Sumesh Udayakumaran, Sungmann Cho, Tae-Hwan Jung, Taehoon Lee, Takeshi Watanabe, TengLu, terryky, TheMindVirus, ThisIsIsaac, Till Hoffmann, Timothy Liu, Tomer Gafner, Tongxuan Liu, Trent Lo, Trevor Morris, Uday Bondhugula, Vasileios Lioutas, vbvg2008, Vishnuvardhan Janapati, Vivek Suryamurthy, Wei Wang, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, winstonq, wyzhao, Xiaoming (Jason) Cui, Xinan Jiang, Xinping Wang, Yann-Yy, Yasir Modak, Yong Tang, Yongfeng Gu, Yuchen Ying, Yuxin Wu, zyeric, \u738b\u632f\u534e (Zhenhua Wang)", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.15.0-rc0", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.15.0-rc0", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.15.0-rc0", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/19866891"}, {"tag_name": "v2.0.0-rc0", "name": "TensorFlow 2.0.0-rc0", "author_name": "goldiegadde", "body": "# Release 2.0.0-rc0\r\n\r\n## Major Features and Improvements\r\n\r\nTensorFlow 2.0 focuses on **simplicity** and **ease of use**, featuring updates like:\r\n\r\n* Easy model building with Keras and eager execution.\r\n* Robust model deployment in production on any platform.\r\n* Powerful experimentation for research.\r\n* API simplification by reducing duplication and removing deprecated endpoints.\r\n\r\nFor details on best practices with 2.0, see [the Effective 2.0 guide](https://www.tensorflow.org/beta/guide/effective_tf2)\r\n\r\n\r\nFor information on upgrading your existing TensorFlow 1.x models, please refer to our [Upgrade](https://medium.com/tensorflow/upgrading-your-code-to-tensorflow-2-0-f72c3a4d83b5) and [Migration](https://www.tensorflow.org/beta/guide/migration_guide) guides. We have also released a collection of [tutorials and getting started guides](https://www.tensorflow.org/beta).\r\n\r\n## Highlights\r\n\r\n* TF 2.0 delivers Keras as the central high level API used to build and train models. Keras provides several model-building APIs such as Sequential, Functional, and Subclassing along with eager execution, for immediate iteration and intuitive debugging, and tf.data, for building scalable input pipelines. Checkout [guide](https://www.tensorflow.org/beta/guide/keras/overview) for additional details.\r\n* Distribution Strategy: TF 2.0 users will be able to use the [`tf.distribute.Strategy`](https://www.tensorflow.org/beta/guide/distribute_strategy) API to distribute training with minimal code changes, yielding great out-of-the-box performance. It supports distributed training with Keras model.fit, as well as with custom training loops. Multi-GPU support is available, along with experimental support for multi worker and Cloud TPUs. Check out the [guide](https://www.tensorflow.org/beta/guide/distribute_strategy) for more details.\r\n* Functions, not Sessions. The traditional declarative programming model of building a graph and executing it via a `tf.Session` is discouraged, and replaced with by writing regular Python functions. Using the `tf.function` decorator, such functions can be turned into graphs which can be executed remotely, serialized, and optimized for performance.\r\n* Unification of tf.train.Optimizers and tf.keras.Optimizers. Use tf.keras.Optimizers for TF2.0. `compute_gradients` is removed as public API, and use GradientTape to compute gradients.\r\n* AutoGraph translates Python control flow into TensorFlow expressions, allowing users to write regular Python inside `tf.function`-decorated functions. AutoGraph is also applied in functions used with tf.data, tf.distribute and tf.keras APIs. \r\n* Unification of exchange formats to SavedModel. All TensorFlow ecosystem projects (TensorFlow Lite, TensorFlow JS, TensorFlow Serving, TensorFlow Hub) accept SavedModels. Model state should be saved to and restored from SavedModels.\r\n* API Changes: Many API symbols have been renamed or removed, and argument names have changed. Many of these changes are motivated by consistency and clarity. The 1.x API remains available in the compat.v1 module. A list of all symbol changes can be found [here](https://docs.google.com/spreadsheets/d/1FLFJLzg7WNP6JHODX5q8BDgptKafq_slHpnHVbJIteQ/edit#gid=0). \r\n * API clean-up, included removing `tf.app`, `tf.flags`, and `tf.logging` in favor of [absl-py](https://github.com/abseil/abseil-py).\r\n* No more global variables with helper methods like `tf.global_variables_initializer` and `tf.get_global_step`.\r\n\r\n## Breaking Changes\r\n* Many backwards incompatible API changes have been made to clean up the APIs and make them more consistent.\r\n* `tf.contrib` has been deprecated, and functionality has been either migrated to the core TensorFlow API, to an ecosystem project such as [tensorflow/addons](https://www.github.com/tensorflow/addons) or [tensorflow/io](https://www.github.com/tensorflow/io), or removed entirely.\r\n* Premade estimators in the tf.estimator.DNN/Linear/DNNLinearCombined family have been updated to use `tf.keras.optimizers` instead of the `tf.compat.v1.train.Optimizer`s. If you do not pass in an `optimizer=` arg or if you use a string, the premade estimator will use the Keras optimizer. This is checkpoint breaking, as the optimizers have separate variables. A checkpoint converter tool for converting optimizers is included with the release,  but if you want to avoid any change, switch to the v1 version of the estimator:  `tf.compat.v1.estimator.DNN/Linear/DNNLinearCombined*`.\r\n* The equality operation on Tensors & Variables now compares on value instead of `id()`. As a result, both Tensors & Variables are no longer hashable types.\r\n* Layers now default to float32, and automatically cast their inputs to the layer's dtype. If you had a model that used float64, it will probably silently use float32 in TensorFlow 2, and a warning will be issued that starts with \"Layer <layer-name> is casting an input tensor from dtype float64 to the layer's dtype of float32\". To fix, either set the default dtype to float64 with `tf.keras.backend.set_floatx('float64')`, or pass `dtype='float64'` to each of the Layer constructors. See `tf.keras.layers.Layer` for more information.\r\n\r\nRefer to our [public project status tracker](https://github.com/orgs/tensorflow/projects/4) and [issues tagged with `2.0`](https://github.com/tensorflow/tensorflow/issues?q=is%3Aopen+is%3Aissue+label%3A2.0) on GitHub for insight into recent issues and development progress.\r\n\r\nIf you experience any snags when using TF 2.0, please let us know at the [TF 2.0 Testing User Group](https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/testing). We have a support mailing list as well as weekly testing meetings, and would love to hear your migration feedback and questions.\r\n\r\n\r\n## Bug Fixes and Other Changes\r\n\r\n* `tf.data`:\r\n  * Add support for TensorArrays to `tf.data Dataset`.\r\n  * Integrate Ragged Tensors with `tf.data`.\r\n  * All core and experimental tf.data transformations that input user-defined functions can span multiple devices now.\r\n  * Extending the TF 2.0 support for `shuffle(..., reshuffle_each_iteration=True)` and `cache()` to work across different Python iterators for the same dataset.\r\n  * Removing the `experimental_numa_aware` option from `tf.data.Options`.\r\n  * Add `num_parallel_reads` and passing in a Dataset containing filenames into `TextLineDataset` and `FixedLengthRecordDataset`.\r\n  * Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\r\n  * Promoting `tf.data.experimental.enumerate_dataset` to core as `tf.data.Dataset.enumerate`.\r\n  * Promoting `tf.data.experimental.unbatch` to core as `tf.data.Dataset.unbatch`.\r\n  * Adds option for introducing slack in the pipeline to reduce CPU contention, via `tf.data.Options().experimental_slack = True`\r\n  * Added experimental support for parallel batching to `batch()` and `padded_batch()`. This functionality can be enabled through tf.data.Options()\r\n  * Support cancellation of long-running `reduce`.\r\n  * Now we use `dataset` node name as prefix instead of the op name, to identify the component correctly in metrics, for pipelines with repeated components.\r\n\r\n* `tf.distribute`:\r\n  * Enable `tf.distribute.experimental.MultiWorkerMirroredStrategy` working in eager mode.\r\n  * Disable `run_eagerly` and distribution strategy if there are symbolic tensors added to the model using `add_metric` or `add_loss`.\r\n  * Bug fix: loss and gradients should now more reliably be correctly scaled w.r.t. the global batch size when using a `tf.distribute.Strategy`.\r\n  * Set default loss reduction as `AUTO` for improving reliability of loss scaling with distribution strategy and custom training loops. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used in distribution strategy scope, outside of built-in training loops such as `tf.keras` `compile` and `fit`, we expect reduction value to be 'None' or 'SUM'. Using other values will raise an error.\r\n  * Support for multi-host `ncclAllReduce` in Distribution Strategy.\r\n\r\n* `tf.estimator`:\r\n  * Replace `tf.contrib.estimator.add_metrics` with `tf.estimator.add_metrics`\r\n  * Use `tf.compat.v1.estimator.inputs` instead of `tf.estimator.inputs`\r\n  * Replace contrib references with tf.estimator.experimental.* for apis in early_s in Estimator\r\n  * Canned Estimators will now use keras optimizers by default. An error will be raised if tf.train.Optimizers are used, and you will have to switch to tf.keras.optimizers or tf.compat.v1 canned Estimators.\r\n  * A checkpoint converter for canned Estimators has been provided to transition canned Estimators that are warm started from tf.train.Optimizers to tf.keras.optimizers.\r\n  * Default aggregation for canned Estimators is now SUM_OVER_BATCH_SIZE. To maintain previous default behavior, please pass SUM as the loss aggregation method.\r\n  * Canned Estimators don\u2019t support `input_layer_partitioner` arg in the API. If you have this arg, you will have to switch to tf.compat.v1 canned Estimators.\r\n  * Estimator.export_savedmodel has been renamed export_saved_model\r\n  * When saving to SavedModel, Estimators will strip default op attributes. This is almost always the correct behavior, as it is more forwards compatible, but if you require that default attributes are saved with the model, please use tf.compat.v1.Estimator\r\n  * Feature Columns have been upgraded to be more Eager-friendly and to work with Keras. As a result, tf.feature_column.input_layer has been deprecated in favor of tf.keras.layers.DenseFeatures. v1 feature columns have direct analogues in v2 except for shared_embedding_columns, which are not cross-compatible with v1 and v2. Use tf.feature_column.shared_embeddings instead.\r\n  * Losses are scaled in canned estimator v2 and not in the optimizers anymore. If you are using Estimator + distribution strategy + optimikzer v1 then the behavior does not change. This implies that if you are using custom estimator with optimizer v2, you have to scale losses. We have new utilities to help scale losses `tf.nn.compute_average_loss`, `tf.nn.scale_regularization_loss`.\r\n\r\n* `tf.keras`:\r\n  * Premade models (including Linear and WideDeep) have been introduced for the purpose of replacing Premade estimators.\r\n  * Model saving changes\r\n  * `model.save` and `tf.saved_model.save` may now save to the TensorFlow SavedModel format. The model can be restored using `tf.keras.models.load_model`. HDF5 files are still supported, and may be used by specifying `save_format=\"h5\"` when saving.\r\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel. HDF5 files are still supported.\r\n  * Deprecated `tf.keras.experimental.export_saved_model` and `tf.keras.experimental.function`. Please use `tf.keras.models.save_model(..., save_format='tf')` and tf.keras.models.load_model` instead.\r\n  * Raw TensorFlow functions can now be used in conjunction with the Keras Functional API during model creation. This obviates the need for users to create Lambda layers in most cases when using the Functional API. Like Lambda layers, TensorFlow functions that result in Variable creation or assign ops are not supported.\r\n  * Add support for passing list of lists to the `metrics` argument in Keras `compile. \r\n  * Add `tf.keras.layers.AbstractRNNCell` as the preferred implementation for RNN cells in TF v2. User can use it to implement RNN cells with custom behavior.\r\n  * Keras training and validation curves are shown on the same plot when using the TensorBoard callback.\r\n  * Switched Keras `fit/evaluate/predict` execution to use only a single unified path by default unless eager execution has been explicitly disabled, regardless of input type. This unified path places an eager-friendly training step inside of a `tf.function`. With this 1. All input types are converted to `Dataset`. 2. The path assumes there is always a distribution strategy. when distribution strategy is not specified the path uses a no-op distribution strategy. 3. The training step is wrapped in tf.function unless `run_eagerly=True` is set in compile. The single path execution code does not yet support all use cases. We fallback to the existing v1 execution paths if your model contains the following: 1. sample_weight_mode in compile 2. weighted_metrics in compile 3. v1 optimizer 4. target tensors in compile. If you are experiencing any issues because of this change, please inform us (file an issue) about your use case and you can unblock yourself by setting `experimental_run_tf_function=False` in compile meanwhile. We have seen couple of use cases where the model usage pattern is not as expected and would not work with this change. 1. output tensors of one layer is used in the constructor of another. 2. symbolic tensors outside the scope of the model are used in custom loss functions. The flag can be disabled for these cases and ideally the usage pattern will need to be fixed.\r\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config.  To configure the number of threads, use `tf.config.threading` APIs.\r\n  * Mark Keras `set_session` as `compat.v1` only.\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint format`, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * `keras.backend.resize_images` (and consequently, `keras.layers.Upsampling2D`) behavior has changed, a bug in the resizing implementation was fixed.\r\n  * Add an `implementation=3` mode for `tf.keras.layers.LocallyConnected2D` and `tf.keras.layers.LocallyConnected1D` layers using `tf.SparseTensor` to store weights,  allowing a dramatic speedup for large sparse models.\r\n  * Raise error if `batch_size` argument is used when input is dataset/generator/keras sequence.\r\n  * Update TF 2.0 `keras.backend.name_scope` to use TF 2.0 `name_scope`.\r\n  * Add v2 module aliases for losses, metrics, initializers and optimizers: `tf.losses = tf.keras.losses` & `tf.metrics = tf.keras.metrics` &  `tf.initializers = tf.keras.initializers` & `tf.optimizers = tf.keras.optimizers`.\r\n  * Updates binary cross entropy logic in Keras when input is probabilities. Instead of converting probabilities to logits, we are using the cross entropy formula for probabilities.\r\n  * Added public APIs for `cumsum` and `cumprod` keras backend functions.\r\n  * Add support for temporal sample weight mode in subclassed models.\r\n  * Raise ValueError if an integer is passed to the training APIs. \r\n  * Added fault-tolerance support for training Keras model via `model.fit()` with `MultiWorkerMirroredStrategy`, tutorial available.\r\n  * Callbacks are supported in `MultiWorkerMirroredStrategy`.\r\n  * Custom Callback tutorial is now available.\r\n  * To train with `tf.distribute`, Keras api is recommended over estimator.\r\n  * `steps_per_epoch` and `steps` arguments are supported with numpy arrays.\r\n  * New error message when unexpected keys are used in sample_weight/class_weight dictionaries \r\n  * Losses are scaled in Keras compile/fit and not in the optimizers anymore. If you are using custom training loop, we have new utilities to help scale losses `tf.nn.compute_average_loss`, `tf.nn.scale_regularization_loss`.\r\n  * `Layer` apply and add_variable APIs are deprecated.\r\n  * Added support for channels first data format in cross entropy losses with logits and support for tensors with unknown ranks.\r\n  * Error messages will be raised if `add_update`, `add_metric`, `add_loss`, activity regularizers are used inside of a control flow branch.\r\n  * New loss reduction types: 1. `AUTO`: Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When    used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, we expect reduction value to be `SUM` or `NONE`. Using `AUTO` in that case will raise an error. 2. `NONE`: Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like `fit`/`evaluate`, the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. 3. `SUM`: Scalar sum of weighted losses. 4. `SUM_OVER_BATCH_SIZE`: Scalar `SUM` divided by number of elements in losses. This reduction type is not supported when used with `tf.distribute.Strategy` outside of built-in training loops like `tf.keras` `compile`/`fit`.\r\n\r\n* `tf.lite`:\r\n  * Added support for TFLiteConverter Python API in 2.0. Contains functions from_saved_model, from_keras_file, and from_concrete_functions.\r\n  * Removed `lite.OpHint`, `lite.experimental`, and `lite.constant` from 2.0 API.\r\n  * Added support for `tflite_convert` command line tool in 2.0.\r\n  * Post-training quantization tool supports quantizing weights shared by multiple operations. The models made with versions of this tool will use INT8 types for weights and will only be executable interpreters from this version onwards.\r\n  * Post-training quantization tool supports fp16 weights and GPU delegate acceleration for fp16.\r\n\r\n* `tf.contrib`:\r\n  * Expose `tf.contrib.proto.*` ops in `tf.io` (they will exist in TF2)\r\n  * Remove `tf.contrib.timeseries` dependency on TF distributions.\r\n  * Replace contrib references with `tf.estimator.experimental.*` for apis in early_stopping.py\r\n\r\n* Other:\r\n  * Bug fix for `tf.tile gradient`.\r\n  * TF code now resides in `tensorflow_core` and `tensorflow` is just a virtual pip package. No code changes are needed for projects using TensorFlow, the change is transparent\r\n  * Added gradient for `SparseToDense` op.\r\n  * Expose a flag that allows the number of threads to vary across Python benchmarks.\r\n  * ResourceVariable's gather op supports batch dimensions.\r\n  * `image.resize` in 2.0 now supports gradients for the new resize kernels.\r\n  * removed `tf.string_split` from v2 API\r\n  * Variadic reduce is supported on CPU Variadic reduce is supported on CPU\r\n  * Added GPU implementation of `tf.linalg.tridiagonal_solve`.\r\n  * Delete unused lookup table code\r\n  * Remove unused `StringViewVariantWrapper`.\r\n  * Delete unused `Fingerprint64Map` op registration\r\n  * Add broadcasting support to `tf.matmul`.\r\n  * Add ellipsis (...) support for `tf.einsum()`.\r\n  * ResourceVariable support for `gather_nd`.\r\n  * Add expand_composites argument to all nest.* methods.\r\n  * Standardize the LayerNormalization API by replacing the args `norm_axis` and `params_axis` with `axis`.\r\n  * Add a new \"result_type\" parameter to `tf.strings.split`\r\n  * `add_update` can now be passed a zero-arg callable in order to support turning off the update when setting `trainable=False` on a Layer of a Model compiled with `run_eagerly=True`.\r\n  * Added `tf.random.binomial`.\r\n  * Extend `tf.function` with basic support for CompositeTensors arguments (such as SparseTensor and RaggedTensor).\r\n  * Add name argument to `tf.string_split` and `tf.strings_split`.\r\n  * Added `strings.byte_split`.\r\n  * CUDNN_INSTALL_PATH, TENSORRT_INSTALL_PATH, NCCL_INSTALL_PATH, NCCL_HDR_PATH are deprecated. Use TF_CUDA_PATHS instead which supports a comma-separated list of base paths that are searched to find CUDA libraries and headers.\r\n  * Add `RaggedTensor.placeholder()`.\r\n  * Add pfor converter for `Squeeze`.\r\n  * Renamed `tf.image` functions to remove duplicate \"image\" where it is redundant.\r\n  * Add C++ Gradient for BatchMatMulV2.\r\n  * `parallel_for.pfor`: add converters for Softmax, LogSoftmax, IsNaN, All, Any, and MatrixSetDiag.\r\n  * `parallel_for`: add converters for LowerTriangularSolve and Cholesky.\r\n  * Add ragged tensor support to `tf.squeeze`.\r\n  * Allow `LinearOperator.solve` to take a `LinearOperator`.\r\n  * Allow all dtypes for `LinearOperatorCirculant`.\r\n  * Introduce MaxParallelism method\r\n  * `parallel_for`: add converter for `BroadcastTo`.\r\n  * Add `LinearOperatorHouseholder`.\r\n  * Added `key` and `skip` methods to `random.experimental.Generator`.\r\n  * Adds Philox support to new stateful RNG's XLA path.\r\n  * Update RaggedTensors to support int32 row_splits.\r\n  * Add `TensorSpec` support for CompositeTensors.\r\n  * Added partial_pivoting input parameter to `tf.linalg.tridiagonal_solve`.\r\n  * Extend `tf.strings.split` to support inputs with any rank\r\n  * Removing the `experimental_numa_aware` option from `tf.data.Options`.\r\n  * Improve the performance of datasets using `from_tensors()`.\r\n  * Add `tf.linalg.tridiagonal_mul op`.\r\n  * Add `LinearOperatorToeplitz`.\r\n  * Added gradient to `tf.linalg.tridiagonal_solve`.\r\n  * Upgraded LIBXSMM to version 1.11.\r\n  * `parallel_for`: add converters for `LogMatrixDeterminant` and `MatrixBandPart`.\r\n  * Uniform processing of quantized embeddings by Gather and EmbeddingLookup Ops\r\n  * Correct a misstatement in the documentation of the sparse softmax cross entropy logit parameter.\r\n  * `parallel_for`: Add converters for `OneHot`, `LowerBound`, `UpperBound`.\r\n  * Added GPU implementation of `tf.linalg.tridiagonal_matmul`.\r\n  * Add gradient to `tf.linalg.tridiagonal_matmul`.\r\n  * Add `tf.ragged.boolean_mask`.\r\n  * `tf.switch_case` added, which selects a branch_fn based on a branch_index.\r\n  * The C++ kernel of gather op supports batch dimensions.\r\n  * Promoting `unbatch` from experimental to core API.\r\n  * Fixed default value and documentation for `trainable` arg of tf.Variable.\r\n  * Adds `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()`.\r\n  * EagerTensor now supports buffer interface for tensors.\r\n  * This change bumps the version number of the FullyConnected Op to 5.\r\n  * tensorflow : crash when pointer become nullptr.\r\n  * `parallel_for`: Add converter for `MatrixDiag`.\r\n  * Add 'narrow_range' attribute to QuantizeAndDequantizeV2 and V3.\r\n  * Added new op: `tf.strings.unsorted_segment_join`.\r\n  * Tensorflow code now produces 2 different pip packages: tensorflow_core containing all the code (in the future it will contain only the private implementation) and tensorflow which is a virtual pip package doing forwarding to tensorflow_core (and in the future will contain only the public API of tensorflow)\r\n  * Adding support for datasets as inputs to `from_tensors` and `from_tensor_slices` and batching and unbatching of nested datasets.\r\n  * Add HW acceleration support for topK_v2\r\n  * Add new TypeSpec classes\r\n  * CloudBigtable version updated to v0.10.0 BEGIN_PUBLIC CloudBigtable version updated to v0.10.0\r\n  * Deprecated the use of `constraint=` and `.constraint` with ResourceVariable.\r\n  * Expose Head as public API.\r\n   * Update docstring for gather to properly describe the non-empty batch_dims case.\r\n  * Added `tf.sparse.from_dense` utility function.\r\n  * Add `GATHER` support to NN API delegate\r\n  * Improved ragged tensor support in `TensorFlowTestCase`.\r\n  * Makes the a-normal form transformation in Pyct configurable as to which nodes are converted to variables and which are not.\r\n  * `ResizeInputTensor` now works for all delegates\r\n  * Start of open development of TF, TFLite, XLA MLIR dialects.\r\n  * Add `EXPAND_DIMS` support to NN API delegate TEST:  expand_dims_test\r\n  * `tf.cond` emits a StatelessIf op if the branch functions are stateless and do not touch any resources.\r\n  * Add support of local soft device placement for eager op.\r\n  * Pass partial_pivoting to the `_TridiagonalSolveGrad`.\r\n  * Add HW acceleration support for LogSoftMax\r\n  * Added a function nested_value_rowids for ragged tensors.\r\n  * fixed a bug in histogram_op.cc.\r\n  * Add guard to avoid acceleration of L2 Normalization with input rank != 4\r\n  * Added evaluation script for COCO minival\r\n  * Add delegate support for `QUANTIZE`\r\n  * add `tf.math.cumulative_logsumexp` operation.\r\n  * Add `tf.ragged.stack`.\r\n  * Add delegate support for `QUANTIZED_16BIT_LSTM`.\r\n  * `tf.while_loop` emits a StatelessWhile op if the cond and body functions are stateless and do not touch any resources.\r\n  * Fix memory allocation problem when calling `AddNewInputConstantTensor`.\r\n  * Delegate application failure leaves interpreter in valid state\r\n  * tf.cond, tf.while and if and while in AutoGraph now accept a nonscalar predicate if has a single element. This does not affec non-V2 control flow.\r\n  * Enables v2 control flow as part of tf.enable_v2_behavior() and TF2_BEHAVIOR=1.\r\n  * Fix potential security vulnerability where decoding variant tensors from proto could result in heap out of bounds memory access.\r\n  * Extracts NNAPIDelegateKernel from nnapi_delegate.cc\r\n  * Only create a GCS directory object if the object does not already exist.\r\n  * Introduce `dynamic` constructor argument in Layer and Model, which should be set to True when using imperative control flow in the `call` method.\r\n  * `ResourceVariable` and `Variable` no longer accepts `constraint` in the constructor, nor expose it as a @property.\r\n  * Add UnifiedGRU as the new GRU implementation for tf2.0. Change the default recurrent activation function for GRU from 'hard_sigmoid' to 'sigmoid', and 'reset_after' to True in 2.0. Historically recurrent activation is 'hard_sigmoid' since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default GRU will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with GRU(recurrent_activation='hard_sigmoid', reset_after=False) to fallback to 1.x behavior.\r\n  * Begin adding Go wrapper for C Eager API\r\n  * XLA HLO graphs can be inspected with interactive_graphviz tool now.\r\n  * Add dataset ops to the graph (or create kernels in Eager execution) during the python Dataset object creation instead doing it during Iterator creation time.\r\n  * Add batch_dims argument to tf.gather.\r\n  * Removing of dtype in the constructor of initializers and partition_info in call.\r\n  * Add `tf.math.nextafter` op.\r\n  * Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically dispatches the best kernel implementation based on CPU vector architecture. To disable them, build with --define=tensorflow_mkldnn_contraction_kernel=0.\r\n  * `tf.linspace(start, stop, num)` now always uses \"stop\" as last value (for num > 1)\r\n  * Added top-k to precision and recall to keras metrics.\r\n  * Add a ragged size op and register it to the op dispatcher\r\n  * Transitive dependencies on :pooling_ops were removed.  Some users may need to add explicit dependencies on :pooling_ops if they reference the operators from that library.\r\n  * Add CompositeTensor base class.\r\n  * Malformed gif images could result in an access out of bounds in the color palette of the frame. This has been fixed now\r\n  * Add templates and interfaces for creating lookup tables\r\n  * Tensor::UnsafeCopyFromInternal deprecated in favor Tensor::BitcastFrom\r\n  * In `map_vectorization` optimization, reduce the degree of parallelism in the vectorized map node.\r\n  * Add variant wrapper for absl::string_view\r\n  * Wraps losses passed to the `compile` API (strings and v1 losses) which are not instances of v2 `Loss` class in `LossWrapper` class. => All losses will now use `SUM_OVER_BATCH_SIZE` reduction as default.\r\n  * Add OpKernels for some stateless maps\r\n  * Add v2 APIs for AUCCurve and AUCSummationMethod enums. #tf-metrics-convergence\r\n  * Allow non-Tensors through v2 losses.\r\n  * Add v2 sparse categorical crossentropy metric. GITHUB_PR_OR_BUG=b/123431691\r\n  * DType is no longer convertible to an int. Use dtype.as_datatype_enum instead of int(dtype) to get the same result.\r\n  * Support both binary and -1/1 label input in v2 hinge and squared hinge losses.\r\n  * Added LinearOperator.adjoint and LinearOperator.H (alias).\r\n  * Expose CriticalSection in core as `tf.CriticalSection`.\r\n  * Enhanced graphviz output.\r\n  * The behavior of `tf.gather` is now correct when axis=None and batch_dims<0.\r\n  * Add `tf.linalg.tridiagonal_solve` op.\r\n  * Add opkernel templates for common table operations.\r\n  * Fix issue: Callbacks do not log values in eager mode when a deferred build model is used.\r\n  * SignatureDef util functions have been deprecated.\r\n  * Update Fingerprint64Map to use aliases\r\n  * Add legacy string flat hash map op kernels\r\n  * Fix: `model.add_loss(symbolic_tensor)` should work in ambient eager.\r\n  * Adding `clear_losses` API to be able to clear losses at the end of forward pass in a custom training loop in eager. GITHUB_PR_OR_BUG=b/123431691\r\n  * Add support for `add_metric` in the graph function mode. GITHUB_PR_OR_BUG=tf_only\r\n  * Updating cosine similarity loss - removed the negate sign from cosine similarity. GITHUB_PR_OR_BUG=b/123431691\r\n  * TF 2.0 - Update metric name to always reflect what the user has given in compile. Affects following cases 1. When name is given as 'accuracy'/'crossentropy' 2. When an aliased function name is used eg. 'mse' 3. Removing the `weighted` prefix from weighted metric names.\r\n  * Workaround for compiler bug(?)\r\n  * Changed default for gradient accumulation for TPU embeddings to true.\r\n  * Adds summary trace API for collecting graph and profile information.\r\n  * `image.resize` now considers proper pixel centers and has new kernels (incl. anti-aliasing).\r\n \r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n1e100, a6802739, Abolfazl Shahbazi, Adam Weiss, Ag Ramesh, Alan Du, Albin Joy, Amit, Amit Srivastava, Andy Craze, Anshuman Tripathy, Armen Poghosov, armenpoghosov, Arpit Shah, Ashwin Ramaswami, Aurelien Geron, Aur\u00e9Lien Geron, aweers, awesomealex1, Bairen Yi, Ben Barsdell, Bhavani Subramanian, Brandon Carter, candy.dc, Chao Liu, Clayne Robison, csukuangfj, Dan Jarvis, Dan Lazewatsky, Daniel Ingram, Dave Airlie, David Norman, Dayananda V, Denis Khalikov, Deven Desai, Dheeraj Rajaram Reddy, dmitrievanthony, Drew Szurko, Duncan Riach, Fei Hu, Felix Lemke, Filip Matzner, fo40225, frreiss, Gautam, gehring, Grzegorz George Pawelczak, Grzegorz Pawelczak, HanGuo97, Hari Shankar, hehongliang, Heungsub Lee, Hoeseong Kim, I-Hong Jhuo, Ilango R, Innovimax, Jacky Ko, Jakub Lipinski,  jcf94, Jeff Poznanovic, Jia Qingtong, Jiankang, Joe Q, Joe Quadrino, Jonas Rauber, Jonathan Kyl, Joppe Geluykens, Joseph Friedman, jtressle, jwu, K. Hodges, kaixih, Karl Lessard, Karl Weinmeister, Kashif Rasul, kjopek, Koan-Sin Tan, kouml, ktaebum, Laurent Le Brun, Li, Guizi, Loo Rong Jie, Lucas Hendren, Lukas Geiger, Luke Han, Mahmoud Abuzaina, manhyuk, Marco Gaido, Marek Drozdowski, Mark Ryan, mars20, Mateusz Chudyk, Matt Conley, MattConley, mbhuiyan, mdfaijul, Melissa Grueter, Michael K\u00e4Ufl, Micka\u00ebL Schoentgen, Miguel Morin, Mike Arpaia, nammbash, Natalia Gimelshein, Nayana-Ibm, neargye, Nehal J Wani, Niels Ole Salscheider, Niranjan Hasabnis, Nutti, olicht, P Sudeepam, Paige Bailey, Palmer Lao, Pariksheet Pinjari, Pavel Samolysov, Pooya Davoodi, Ryan Jiang, Samantha Andow, Sami Kama, Saurabh Deoras, Shahzad Lone, Shashi, Siju, Siju Samuel, Snease-Abq, Spencer Schaber, srinivasan.narayanamoorthy, Steve Lang, Steve Nesae, Supriya Rao, Taylor Jakobson, Taylor Thornton, ThisIsPIRI, Thomas Deegan, tomguluson92, Tongxuan Liu, Vagif, vcarpani, Vikram Tiwari, Vishwak Srinivasan, Vitor-Alves, wangsiyu, WeberXie, WeijieSun, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, Yan Facai (\u989c\u53d1\u624d), ymodak, Yong Tang, Younes Khoudli, Yuan Lin, Yves-Noel Weweler, zjjott, \u535c\u5c45, \u738b\u632f\u534e (Wang Zhenhua), \r\n\r\n4d55397500, a6802739, Abdullah Selek, abenmao, Adam Richter, Ag Ramesh, Albin Joy, Alex, Alex Itkes, Alex Sergeev, Alexander Pivovarov, Alexey Romanov, alhkad, Aman Patel, Amit, Amit Kumar Jaiswal, Amit Srivastava, amoitra, Andreas Eberle, Andrew Lihonosov, Anthony Hsu, Anthony Platanios, Anuj Rawat, arp95, Arpit Shah, Astropeak, Augustina Ragwitz, Aurelien Geron, Aur\u00e9Lien Geron, avasid, aweers, Ayush Agrawal, Bas Aarts, Bastian Eichenberger, Bayberry Z, Ben Barsdell, Benjamin Peterson, bhack, Bharat Raghunathan, Bhavani Subramanian, Bin Fan, blairhan, Bl\u00e9Nesi Attila, Bodin-E, Bryan Cutler, Cao Zongyan, Casper Da Costa-Luis, Chen Guoyin, chenchc, chengchingwen, chie8842, Christian Hansen, Christoph Boeddeker, Christopher Yeh, Clayne Robison, Coady, Patrick, crafet, ctiijima, Daniel Rasmussen, Daniel Salvadori, David Norman, delock, Denis Khalikov, Deven Desai, Diego Caballero, Donovan Ong, Duncan Dean, Duncan Riach, Dustin Neighly, Dwight J Lyle, Eamon Ito-Fisher, eashtian3, Edward Forgacs, EFanZh, ejot, Elroy Ashtian Jr, Eric Schweitz, Evgeniy Polyakov, Fangjun Kuang, Federico Martinez, Fei Hu, Filip Matzner, FlashTek, fo40225, formath, Franc\u0327Ois Chollet, Fred Reiss, Frederic Bastien, Fredrik Knutsson, G. Hussain Chinoy, Gabriel, gehring, Geoffrey Irving, George Grzegorz Pawelczak, George Sterpu, Gianluca Varisco, Gleb Popov, Greg Peatfield, Guillaume Klein, Gurpreet Singh, Gustavo Lima Chaves, Gyoung-Yoon Ryoo, haison, Hanton Yang, Haraldur T\u00f3Mas Hallgr\u00edMsson, Huan Li (\u674e\u5353\u6853), H\u00e5Kon Sandsmark, I-Hong, Ilham Firdausi Putra, Imran Salam, Irene Dea, Ivan Habernal, Jacky, Jason Zaman, Jason Zavaglia, jayhpark530, jefby, Jeff Daily, Jeffrey Poznanovic, Jekyll Lai, jer, Jeroen B\u00e9Dorf, jerryyin, jhalakp, jiakai, JiangXIAO, Joe Bowser, Joel Shapiro, Johan Gunnarsson, Jojimon Varghese, Jonathan, Joon, Josh Beal, Julian Niedermeier, Junqin Zhang, Justin Dujardin, Justin Tunis, Kaixi Hou, Karthik Muthuraman, Kay Zhu, Kbhute-Ibm, KDR, Keno Fischer, Kevin Mader, khanhlvg, Kilaru Yasaswi Sri Chandra Gandhi, Koock Yoon, Kyuwon Kim, Lakshay Tokas, leike666666, leonard951, Leslie-Fang, Letian Kang, Li, Guizi, Lukas Folle, Lukas Geiger, luxupu, lvli, Ma, Guokai, Mahmoud Abuzaina, Maksym Kysylov, Mandar Deshpande, Manraj Singh Grover, Margaret Maynard-Reid, Mark Ryan, Matt Conley, mbhuiyan, mdfaijul, Mei Jie, merturl, MichaelKonobeev, Michal W. Tarnowski, Mihail Salnikov, Mikalai Drabovich, Mike Holcomb, minds, monklof, Moses Marin, mpppk, Mr. Metal, Mshr-H, musikisomorphie, nammbash, Nathan Luehr, Nayana Thorat, Neeraj Pradhan, Neil, Nick, Nick Lewycky, Niels Ole Salscheider, Niklas Silfverstr\u00f6M, Niranjan Hasabnis, Nuka-137, ocjosen, omeir1, P Sudeepam, Pan Daoxin, Pariksheet Pinjari, Pasquale Minervini, Patrick J. Lopresti, Patrik Gustavsson, Pavel Akhtyamov, PENGWA, per1234, PeterLee, Phan Van Nguyen Duc, Philipp Jund, Phillip Kravtsov, Pooya Davoodi, Pranav Marathe, Putra Manggala, Qingqing Cao, R S Nikhil Krishna, Rajeshwar Reddy T, Ramon Vi\u00f1As, Rasmus Diederichsen, Reuben Morais, robert, Rohit Gupta, Roland Zimmermann, Roman Soldatow, RonLek, Ruizhe, Ryan Jiang, saishruthi, Saleem Abdulrasool, Sami Kama, Sana-Damani, sdamani, Sean Morgan, seanshpark, Sebastien Iooss, Serv-Inc, Severen Redwood, Shashank Gupta, shashvat, Shashvat Chand Shahi, Shubham Goyal, Sigrid Keydana, Siju Samuel, sleighsoft, smilu97, Son Tran, sremedios, Srini511, srinivasan.narayanamoorthy, Subin Modeel, Sumesh Udayakumaran, Sungmann Cho, sunway513, sxwang, Tae-Hwan Jung, Taehoon Lee, Takeo Sawada, Taylor Jakobson, Ted Chang, TengLu, terryky, ThisIsIsaac, Thomas Deegan, Thomas Hagebols, tianyapiaozi, Till Hoffmann, Tim Zaman, Tongxuan Liu, Trent Lo, Trevor Morris, TungJerry, Tyorden, Uday Bondhugula, v1incent, Vasileios Lioutas, vbvg2008, Vijay Ravichandran, Viktor Gal, Vincent, Vishnuvardhan Janapati, Vivek Suryamurthy, wangsiyu, wateryzephyr, Wei Wang, Wen-Heng (Jack) Chung, wenxizhu, Will Battel, William D. Irons, winstonq, wyzhao, Xiaoming (Jason) Cui, Xiaoquan Kong, Xin, Xinping Wang, Yann-Yy, Yasir Modak, Yasuhiro Matsumoto, Yong Tang, Yongfeng Gu, Yuan (Terry) Tang, Yuchen Ying, zhangyujing, zyeric, \u738b\u632f\u534e (Zhenhua Wang), \u9ec4\u946b\r\n\r\n", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.0.0-rc0", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.0.0-rc0", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-rc0", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/19501173"}, {"tag_name": "v1.13.2", "name": "TensorFlow 1.13.2", "author_name": "tensorflow-jenkins", "body": "# Release 1.13.2\r\n\r\n## Bug Fixes and Other Changes\r\n\r\n* Updates `png_archive` dependency to 1.6.37 to not be affected by CVE-2019-7317, CVE-2018-13785, and CVE-2018-14048.\r\n* Updates `sqlite` dependency to 3.28.0 to not be affected by CVE-2018-20506, CVE-2018-20346, and CVE-2018-20505.", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.13.2", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.13.2", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.13.2", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/18635353"}, {"tag_name": "v1.12.3", "name": "TensorFlow 1.12.3", "author_name": "tensorflow-jenkins", "body": "# Release 1.12.3\r\n\r\n## Bug Fixes and Other Changes\r\n\r\n* Updates `png_archive` dependency to 1.6.37 to not be affected by CVE-2019-7317, CVE-2018-13785, and CVE-2018-14048.\r\n* Updates `sqlite` depenency to 3.28.0 to not be affected by CVE-2018-20506, CVE-2018-20346, and CVE-2018-20505.", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.12.3", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.12.3", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.12.3", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/18155514"}, {"tag_name": "v1.14.0", "name": "TensorFlow 1.14.0", "author_name": "tensorflow-jenkins", "body": "# Release 1.14.0\r\n\r\n## Major Features and Improvements\r\n\r\n* This is the first 1.x release containing the compat.v2 module. This module is required to allow libraries to publish code which works in both 1.x and 2.x. After this release, no backwards incompatible changes are allowed in the 2.0 Python API.\r\n* Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically dispatches the best kernel implementation based on CPU vector architecture. To disable them, build with --define=tensorflow_mkldnn_contraction_kernel=0.\r\n* Non-Windows system libraries are now versioned. This should be a no-op for most users as it affects only system package maintainers or those building extensions to TensorFlow:\r\n  * Python wheels (Pip packages) contain one library file.\r\n    * Linux: `libtensorflow_framework.so.1`\r\n    * MacOS: `libtensorflow_framework.1.dylib`\r\n  * Our `libtensorflow` tarball archives contain the `libtensorflow` library and two symlinks. MacOS `.dylib` libraries are the same, but match MacOS library naming requirements (i.e. `libtensorflow.1.dylib`):\r\n    * `libtensorflow.so.1.14.0`, the main library\r\n    * `libtensorflow.so.1`, symlinked to the main library\r\n    * `libtensorflow.so`, symlinked to `.so.1`\r\n\r\n## Behavioral changes\r\n\r\n* Set default loss reduction as `AUTO` for improving reliability of loss scaling with distribution strategy and custom training loops. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used in distribution strategy scope, outside of built-in training loops such as `tf.keras` `compile` and `fit`, we expect reduction value to be 'None' or 'SUM'. Using other values will raise an error.\r\n* Wraps losses passed to the `compile` API (strings and v1 losses) which are not instances of v2 `Loss` class in `LossWrapper` class. => All losses will now use `SUM_OVER_BATCH_SIZE` reduction as default.\r\n* Disable `run_eagerly` and distribution strategy if there are symbolic tensors added to the model using `add_metric` or `add_loss`.\r\n* tf.linspace(start, stop, num) now always uses \"stop\" as last value (for num > 1)\r\n* The behavior of tf.gather is now correct when axis=None and batch_dims<0.\r\n* Only create a GCS directory object if the object does not already exist.\r\n* In `map_vectorization` optimization, reduce the degree of parallelism in the vectorized map node.\r\n* Bug fix: loss and gradients should now more reliably be correctly scaled w.r.t. the global batch size when using a tf.distribute.Strategy.\r\n* Updating cosine similarity loss - removed the negate sign from cosine similarity.\r\n* DType is no longer convertible to an int. Use dtype.as_datatype_enum instead of int(dtype) to get the same result.\r\n* Changed default for gradient accumulation for TPU embeddings to true.\r\n* Callbacks now log values in eager mode when a deferred build model is used.\r\n* Transitive dependencies on :pooling_ops were removed.  Some users may need to add explicit dependencies on :pooling_ops if they reference the operators from that library.\r\n\r\n## Bug Fixes and Other Changes\r\n* Documentation\r\n* Deprecations and Symbol renames.\r\n  * The GPU configuration env parameter `TF_CUDA_HOST_MEM_LIMIT_IN_MB` has been changed to `TF_GPU_HOST_MEM_LIMIT_IN_MB`.\r\n  * Remove unused StringViewVariantWrapper\r\n  * Delete unused Fingerprint64Map op registration\r\n  * SignatureDef util functions have been deprecated.\r\n  * Renamed tf.image functions to remove duplicate \"image\" where it is redundant.\r\n  * tf.keras.experimental.export renamed to tf.keras.experimental.export_saved_model\r\n  * Standardize the LayerNormalization API by replacing the args `norm_axis` and `params_axis` with `axis`.\r\n  * Tensor::UnsafeCopyFromInternal deprecated in favor Tensor::BitcastFrom\r\n* Keras & Python API\r\n  * Add v2 module aliases for:\r\n    * tf.initializers => tf.keras.initializers\r\n    * tf.losses => tf.keras.losses & tf.metrics => tf.keras.metrics\r\n    * tf.optimizers => tf.keras.optimizers\r\n  * Add tf.keras.layers.AbstractRNNCell as the preferred implementation of RNN cell for TF v2. User can use it to implement RNN cell with custom behavior.\r\n  * Adding `clear_losses` API to be able to clear losses at the end of forward pass in a custom training loop in eager.\r\n  * Add support for passing list of lists to the `metrics` param in Keras `compile`.\r\n  * Added top-k to precision and recall to keras metrics.\r\n  * Adding public APIs for `cumsum` and `cumprod` keras backend functions.\r\n  * Fix: model.add_loss(symbolic_tensor) should work in ambient eager.\r\n  * Add name argument to tf.string_split and tf.strings_split\r\n  * Minor change to SavedModels exported from Keras using tf.keras.experimental.export. (SignatureDef key for evaluation mode is now \"eval\" instead of \"test\"). This will be reverted back to \"test\" in the near future.\r\n  * Updates binary cross entropy logic in Keras when input is probabilities. Instead of converting probabilities to logits, we are using the cross entropy formula for probabilities.\r\n  * Raw TensorFlow functions can now be used in conjunction with the Keras Functional API during model creation. This obviates the need for users to create Lambda layers in most cases when using the Functional API. Like Lambda layers, TensorFlow functions that result in Variable creation or assign ops are not supported.\r\n  * Keras training and validation curves are shown on the same plot.\r\n  * Introduce `dynamic` constructor argument in Layer and Model, which should be set to True when using imperative control flow in the `call` method.\r\n  * Removing of dtype in the constructor of initializers and partition_info in call.\r\n* New ops and improved op functionality\r\n  * Add OpKernels for some stateless maps\r\n  * Add v2 APIs for AUCCurve and AUCSummationMethod enums. #tf-metrics-convergence\r\n  * Add tf.math.nextafter op.\r\n  * Add CompositeTensor base class.\r\n  * Add tf.linalg.tridiagonal_solve op.\r\n  * Add opkernel templates for common table operations.\r\n  * Added GPU implementation of tf.linalg.tridiagonal_solve.\r\n  * Added support for TFLite in TensorFlow 2.0.\r\n  * Adds summary trace API for collecting graph and profile information.\r\n  * Add batch_dims argument to tf.gather.\r\n  * Add support for `add_metric` in the graph function mode.\r\n  * Add C++ Gradient for BatchMatMulV2.\r\n  * Added tf.random.binomial\r\n  * Added gradient for SparseToDense op.\r\n  * Add legacy string flat hash map op kernels\r\n  * Add a ragged size op and register it to the op dispatcher\r\n  * Add broadcasting support to tf.matmul.\r\n  * Add ellipsis (...) support for tf.einsum()\r\n  * Added LinearOperator.adjoint and LinearOperator.H (alias).\r\n  * Added GPU implementation of tf.linalg.tridiagonal_solve.\r\n  * Added strings.byte_split\r\n  * Add RaggedTensor.placeholder()\r\n  * Add a new \"result_type\" parameter to tf.strings.split\r\n  * `add_update` can now be passed a zero-arg callable in order to support turning off the update when setting `trainable=False` on a Layer of a Model compiled with `run_eagerly=True`.\r\n  * Add variant wrapper for absl::string_view\r\n  * Add expand_composites argument to all nest.* methods.\r\n  * Add pfor converter for Squeeze.\r\n  * Bug fix for tf.tile gradient\r\n  * Expose CriticalSection in core as tf.CriticalSection.\r\n  * Update Fingerprint64Map to use aliases\r\n  * ResourceVariable support for gather_nd.\r\n  * ResourceVariable's gather op supports batch dimensions.\r\n  * Variadic reduce is supported on CPU\r\n  * Extend tf.function with basic support for CompositeTensors arguments (such as SparseTensor and RaggedTensor).\r\n  * Add templates and interfaces for creating lookup tables\r\n  * Post-training quantization tool supports quantizing weights shared by multiple operations. The models made with versions of this tool will use INT8 types for weights and will only be executable interpreters from this version onwards.\r\n  * Malformed gif images could result in an access out of bounds in the color palette of the frame. This has been fixed now\r\n  * image.resize now considers proper pixel centers and has new kernels (incl. anti-aliasing).\r\n* Performance\r\n  * Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically dispatches the best kernel implementation based on CPU vector architecture. To disable them, build with --define=tensorflow_mkldnn_contraction_kernel=0.\r\n  * Support for multi-host ncclAllReduce in Distribution Strategy.\r\n  * Expose a flag that allows the number of threads to vary across Python benchmarks.\r\n* TensorFlow 2.0 Development\r\n  * Add v2 sparse categorical crossentropy metric.\r\n  * Allow non-Tensors through v2 losses.\r\n  * Add UnifiedGRU as the new GRU implementation for tf2.0. Change the default recurrent activation function for GRU from 'hard_sigmoid' to 'sigmoid', and 'reset_after' to True in 2.0. Historically recurrent activation is 'hard_sigmoid' since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default GRU will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with GRU(recurrent_activation='hard_sigmoid', reset_after=False) to fallback to 1.x behavior.\r\n  * TF 2.0 - Update metric name to always reflect what the user has given in compile. Affects following cases 1. When name is given as 'accuracy'/'crossentropy' 2. When an aliased function name is used eg. 'mse' 3. Removing the `weighted` prefix from weighted metric names.\r\n  * Begin adding Go wrapper for C Eager API\r\n  * image.resize in 2.0 now supports gradients for the new resize kernels.\r\n  * removed tf.string_split from v2 API\r\n  * Expose tf.contrib.proto.* ops in tf.io (they will exist in TF2)\r\n  * \"Updates the TFLiteConverter API in 2.0. Changes from_concrete_function to from_concrete_functions.\"\r\n  * Enable tf.distribute.experimental.MultiWorkerMirroredStrategy working in eager mode.\r\n  * Support both binary and -1/1 label input in v2 hinge and squared hinge losses.\r\n* TensorFlow Lite\r\n  * \"Adds support for tflite_convert in 2.0.\"\r\n  * \"Remove lite.OpHint, lite.experimental, and lite.constant from 2.0 API.\"\r\n* tf.contrib\r\n  * Added Neural Turing Implementation as described in https://arxiv.org/abs/1807.08518.\r\n  * Remove tf.contrib.timeseries dependency on TF distributions.\r\n* tf.data\r\n  * Add num_parallel_reads and passing in a Dataset containing filenames into TextLineDataset and FixedLengthRecordDataset\r\n  * Going forward we operate in TF 2.0, this change is part of the effort to slowly converting XYZDataset to DatasetV2 type which is the official version going to be used in TF 2.0 and motivated by some compatibility issue found, _BigtableXYZDataset (of type DatasetV2) does not implement the _as_variant_tensor() of DatasetV1, when moving contrib.bigtable to tensorflow_io. Converting into DatasetV2 removes the overheads to maintain V1 while we are moving into TF 2.0.\r\n  * Add dataset ops to the graph (or create kernels in Eager execution) during the python Dataset object creation instead doing it during Iterator creation time.\r\n  * Add support for TensorArrays to tf.data Dataset.\r\n  * Switching tf.data functions to use `defun`, providing an escape hatch to continue using the legacy `Defun`.\r\n* Toolchains\r\n  * CUDNN_INSTALL_PATH, TENSORRT_INSTALL_PATH, NCCL_INSTALL_PATH, NCCL_HDR_PATH are deprecated. Use TF_CUDA_PATHS instead which supports a comma-separated list of base paths that are searched to find CUDA libraries and headers.\r\n  * TF code now resides in `tensorflow_core` and `tensorflow` is just a virtual pip package. No code changes are needed for projects using TensorFlow, the change is transparent\r\n* XLA\r\n  * XLA HLO graphs can be inspected with interactive_graphviz tool now.\r\n  * Adds Philox support to new stateful RNG's XLA path.\r\n* Estimator\r\n  * Use `tf.compat.v1.estimator.inputs` instead of `tf.estimator.inputs`\r\n  * Replace `contrib` references with `tf.estimator.experimental.*` for APIs in `early_stopping.py`\r\n  * Determining the \u201ccorrect\u201d value of the `--iterations_per_loop` for TPUEstimator or DistributionStrategy continues to be a challenge for our users. We propose dynamically tuning the `--iterations_per_loop` variable, specifically for using TPUEstimator in training mode, based on a user target TPU execution time. Users might specify a value such as: `--iterations_per_loop=300s`, which will result in roughly 300 seconds being spent on the TPU between host side operations.\r\n\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n1e100, 4d55397500, a6802739, abenmao, Adam Richter, Adam Weiss, Ag Ramesh, Alan Du, Albin Joy, Alex, Aman Patel, Amit, Amit Kumar Jaiswal, Amit Srivastava, Andreas Eberle, Andy Craze, Anthony Hsu, Anthony Platanios, Anuj Rawat, Armen Poghosov, armenpoghosov, arp95, Arpit Shah, Ashwin Ramaswami, Augustina Ragwitz, Aurelien Geron, Aur\u00e9Lien Geron, avasid, aweers, awesomealex1, Ayush Agrawal, Bayberry Z, Ben Barsdell, Bharat Raghunathan, Bhavani Subramanian, Bin Fan, blairhan, Bl\u00e9Nesi Attila, Bodin-E, Brandon Carter, candy.dc, Cheng Chang, Chao Liu, chenchc, chie8842, Christian Hansen, Christian Sigg,  Christoph Boeddeker, Clayne Robison, crafet, csukuangfj, ctiijima, Dan Jarvis, Dan Lazewatsky, Daniel Ingram, Daniel Rasmussen, Daniel Salvadori, Dave Airlie, David Norman, Dayananda V, Dayananda-V, delock, Denis Khalikov, Deven Desai, Dheeraj Rajaram Reddy, dmitrievanthony, Donovan Ong, Drew Szurko, Duncan Riach, Dustin Neighly, Edward Forgacs, EFanZh, Evgeniy Polyakov, Fangjun Kuang, Federico Martinez, Fei Hu, Felix Lemke, Filip Matzner, fo40225, Fred Reiss, Gautam, gehring, Geoffrey Irving, George Sterpu, Grzegorz George Pawelczak, Grzegorz Pawelczak, Gurpreet Singh, Gyoung-Yoon Ryoo, HanGuo97, Hanton Yang, Hari Shankar, hehongliang, Heungsub Lee, Hoeseong Kim, Huan Li (\u674e\u5353\u6853), I-Hong Jhuo, Ilango R, Innovimax, Irene Dea, Jacky Ko, Jakub Lipinski, Jason Zaman, jcf94, Jeffrey Poznanovic, Jens Elofsson, Jeroen B\u00e9Dorf, jhalakp, Jia Qingtong, Jiankang, Joe Q, Joe Quadrino, Joeran Beel, Jonas Rauber, Jonathan, Jonathan Kyl, Joppe Geluykens, Joseph Friedman, jtressle, jwu, K Yasaswi Sri Chandra Gandhi, K. Hodges, Kaixi Hou, Karl Lessard, Karl Weinmeister, Karthik Muthuraman, Kashif Rasul, KDR, Keno Fischer, Kevin Mader, Kilaru Yasaswi Sri Chandra Gandhi, kjopek, Koan-Sin Tan, kouml, ktaebum, Lakshay Tokas, Laurent Le Brun, Letian Kang, Li, Guizi, Loo Rong Jie, Lucas Hendren, Lukas Geiger, Luke Han, luxupu, lvli, Ma, Guokai, Mahmoud Abuzaina, Maksym Kysylov, Mandar Deshpande, manhyuk, Marco Gaido, Marek Drozdowski, Mark Collier, Mark Ryan, mars20, Mateusz Chudyk, Matt Conley, MattConley, mbhuiyan, mdfaijul, Melissa Grueter, Michael K\u00e4Ufl, Micka\u00ebL Schoentgen, Miguel Morin, Mihail Salnikov, Mikalai Drabovich, Mike Arpaia, Mike Holcomb, monklof, Moses Marin, Mr. Metal, Mshr-H, nammbash, Natalia Gimelshein, Nathan Luehr, Nayana-Ibm, neargye, Neeraj Pradhan, Nehal J Wani, Nick, Nick Lewycky, Niels Ole Salscheider, Niranjan Hasabnis, nlewycky, Nuka-137, Nutti, olicht, omeir1, P Sudeepam, Palmer Lao, Pan Daoxin, Pariksheet Pinjari, Pasquale Minervini, Pavel Akhtyamov, Pavel Samolysov, PENGWA, Philipp Jund, Pooya Davoodi, Pranav Marathe, R S Nikhil Krishna, Rohit Gupta, Roland Zimmermann, Roman Soldatow, rthadur, Ruizhe, Ryan Jiang, saishruthi, Samantha Andow, Sami Kama, Sana-Damani, Saurabh Deoras, sdamani, Sean Morgan, seanshpark, Sebastien Iooss, Sergii Khomenko, Serv-Inc, Shahzad Lone, Shashank Gupta, Shashi, shashvat, Shashvat Chand Shahi, Siju, Siju Samuel, Snease-Abq, Spencer Schaber, sremedios, srinivasan.narayanamoorthy, Steve Lang, Steve Nesae, Subin, Sumesh Udayakumaran, sunway513, Supriya Rao, sxwang, Takeo Sawada, Taylor Jakobson, Taylor Thornton, Ted Chang, ThisIsPIRI, Thomas Deegan, Thomas Hagebols, tianyapiaozi, Tim Zaman, tomguluson92, Tongxuan Liu, Trent Lo, TungJerry, Tyorden, v1incent, Vagif, vcarpani, Vijay Ravichandran, Vikram Tiwari, Viktor Gal, Vincent, Vishnuvardhan Janapati, Vishwak Srinivasan, Vitor-Alves, wangsiyu, wateryzephyr, WeberXie, WeijieSun, Wen-Heng (Jack) Chung, wenxizhu, Will Battel, William D. Irons, wyzhao, Xiaoming (Jason) Cui, Xiaoquan Kong, Xin, Yasuhiro Matsumoto, ymodak, Yong Tang, Younes Khoudli, Yuan (Terry) Tang, Yuan Lin, Yves-Noel Weweler, Zantares, zhuoryin, zjjott, \u535c\u5c45, \u738b\u632f\u534e (Zhenhua Wang), \u9ec4\u946b", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.14.0", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.14.0", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.14.0", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/18081222"}, {"tag_name": "v2.0.0-beta1", "name": "TensorFlow 2.0.0-beta1", "author_name": "annarev", "body": "# Release 2.0.0-beta1\r\n\r\nTensorflow 2.0.0-beta1 is a minor update to 2.0.0-beta0 with a few important bug\r\nfixes. Please refer to [2.0.0-beta0 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-beta0) for a complete list of changes in 2.0.0-beta0.\r\n\r\n## Bug Fixes and Other Changes\r\n* Partially fix the function inlining and performance regression for LSTM/GRU.\r\n* Replace training tensor argument with python boolean. Required for TFLite, which does not yet support control flow ops.\r\n* Allow SavedModel serialization to accept `None` InputSpec values.\r\n", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.0.0-beta1", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.0.0-beta1", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-beta1", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/17990750"}, {"tag_name": "v1.14.0-rc1", "name": "TensorFlow 1.14.0-rc1", "author_name": "tensorflow-jenkins", "body": "# Release 1.14.0\r\n\r\n## Major Features and Improvements\r\n\r\n* This is the first 1.x release containing the compat.v2 module. This module is required to allow libraries to publish code which works in both 1.x and 2.x. After this release, no backwards incompatible changes are allowed in the 2.0 Python API.\r\n* Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically dispatches the best kernel implementation based on CPU vector architecture. To disable them, build with --define=tensorflow_mkldnn_contraction_kernel=0.\r\n* Non-Windows system libraries are now versioned. This should be a no-op for most users as it affects only system package maintainers or those building extensions to TensorFlow:\r\n * Python wheels (Pip packages) contain one library file.\r\n   * Linux: `libtensorflow_framework.so.1`\r\n   * MacOS: `libtensorflow_framework.1.dylib`\r\n * Our `libtensorflow` tarball archives contain the `libtensorflow` library and two symlinks. MacOS `.dylib` libraries are the same, but match MacOS library naming requirements (i.e. `libtensorflow.1.dylib`):\r\n   * `libtensorflow.so.1.14.0`, the main library\r\n   * `libtensorflow.so.1`, symlinked to the main library\r\n   * `libtensorflow.so`, symlinked to `.so.1`\r\n\r\n## Behavioral changes\r\n\r\n* Set default loss reduction as `AUTO` for improving reliability of loss scaling with distribution strategy and custom training loops. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used in distribution strategy scope, outside of built-in training loops such as `tf.keras` `compile` and `fit`, we expect reduction value to be 'None' or 'SUM'. Using other values will raise an error.\r\n* Wraps losses passed to the `compile` API (strings and v1 losses) which are not instances of v2 `Loss` class in `LossWrapper` class. => All losses will now use `SUM_OVER_BATCH_SIZE` reduction as default.\r\n* Disable `run_eagerly` and distribution strategy if there are symbolic tensors added to the model using `add_metric` or `add_loss`.\r\n* tf.linspace(start, stop, num) now always uses \"stop\" as last value (for num > 1)\r\n* The behavior of tf.gather is now correct when axis=None and batch_dims<0.\r\n* Only create a GCS directory object if the object does not already exist.\r\n* In `map_vectorization` optimization, reduce the degree of parallelism in the vectorized map node.\r\n* Bug fix: loss and gradients should now more reliably be correctly scaled w.r.t. the global batch size when using a tf.distribute.Strategy.\r\n* Updating cosine similarity loss - removed the negate sign from cosine similarity.\r\n* DType is no longer convertible to an int. Use dtype.as_datatype_enum instead of int(dtype) to get the same result.\r\n* Changed default for gradient accumulation for TPU embeddings to true.\r\n* Callbacks now log values in eager mode when a deferred build model is used.\r\n* Transitive dependencies on :pooling_ops were removed.  Some users may need to add explicit dependencies on :pooling_ops if they reference the operators from that library.\r\n\r\n## Bug Fixes and Other Changes\r\n* Documentation\r\n* Deprecations and Symbol renames.\r\n  * Remove unused StringViewVariantWrapper\r\n  * Delete unused Fingerprint64Map op registration\r\n  * SignatureDef util functions have been deprecated.\r\n  * Renamed tf.image functions to remove duplicate \"image\" where it is redundant.\r\n  * tf.keras.experimental.export renamed to tf.keras.experimental.export_saved_model\r\n  * Standardize the LayerNormalization API by replacing the args `norm_axis` and `params_axis` with `axis`.\r\n  * Tensor::UnsafeCopyFromInternal deprecated in favor Tensor::BitcastFrom\r\n* Keras & Python API\r\n  * Add v2 module aliases for:\r\n    * tf.initializers => tf.keras.initializers\r\n    * tf.losses => tf.keras.losses & tf.metrics => tf.keras.metrics\r\n    * tf.optimizers => tf.keras.optimizers\r\n  * Add tf.keras.layers.AbstractRNNCell as the preferred implementation of RNN cell for TF v2. User can use it to implement RNN cell with custom behavior.\r\n  * Adding `clear_losses` API to be able to clear losses at the end of forward pass in a custom training loop in eager.\r\n  * Add support for passing list of lists to the `metrics` param in Keras `compile`.\r\n  * Added top-k to precision and recall to keras metrics.\r\n  * Adding public APIs for `cumsum` and `cumprod` keras backend functions.\r\n  * Fix: model.add_loss(symbolic_tensor) should work in ambient eager.\r\n  * Add name argument to tf.string_split and tf.strings_split\r\n  * Minor change to SavedModels exported from Keras using tf.keras.experimental.export. (SignatureDef key for evaluation mode is now \"eval\" instead of \"test\"). This will be reverted back to \"test\" in the near future.\r\n  * Updates binary cross entropy logic in Keras when input is probabilities. Instead of converting probabilities to logits, we are using the cross entropy formula for probabilities.\r\n  * Raw TensorFlow functions can now be used in conjunction with the Keras Functional API during model creation. This obviates the need for users to create Lambda layers in most cases when using the Functional API. Like Lambda layers, TensorFlow functions that result in Variable creation or assign ops are not supported.\r\n  * Keras training and validation curves are shown on the same plot.\r\n  * Introduce `dynamic` constructor argument in Layer and Model, which should be set to True when using imperative control flow in the `call` method.\r\n  * Removing of dtype in the constructor of initializers and partition_info in call.\r\n* New ops and improved op functionality\r\n  * Add OpKernels for some stateless maps\r\n  * Add v2 APIs for AUCCurve and AUCSummationMethod enums. #tf-metrics-convergence\r\n  * Add tf.math.nextafter op.\r\n  * Add CompositeTensor base class.\r\n  * Add tf.linalg.tridiagonal_solve op.\r\n  * Add opkernel templates for common table operations.\r\n  * Added support for TFLite in TensorFlow 2.0.\r\n  * Adds summary trace API for collecting graph and profile information.\r\n  * Add batch_dims argument to tf.gather.\r\n  * Add support for `add_metric` in the graph function mode.\r\n  * Add C++ Gradient for BatchMatMulV2.\r\n  * Added tf.random.binomial\r\n  * Added gradient for SparseToDense op.\r\n  * Add legacy string flat hash map op kernels\r\n  * Add a ragged size op and register it to the op dispatcher\r\n  * Add broadcasting support to tf.matmul.\r\n  * Add ellipsis (...) support for tf.einsum()\r\n  * Added LinearOperator.adjoint and LinearOperator.H (alias).\r\n  * Added GPU implementation of tf.linalg.tridiagonal_solve.\r\n  * Added strings.byte_split\r\n  * Add RaggedTensor.placeholder()\r\n  * Add a new \"result_type\" parameter to tf.strings.split\r\n  * `add_update` can now be passed a zero-arg callable in order to support turning off the update when setting `trainable=False` on a Layer of a Model compiled with `run_eagerly=True`.\r\n  * Add variant wrapper for absl::string_view\r\n  * Add expand_composites argument to all nest.* methods.\r\n  * Add pfor converter for Squeeze.\r\n  * Bug fix for tf.tile gradient\r\n  * Expose CriticalSection in core as tf.CriticalSection.\r\n  * Update Fingerprint64Map to use aliases\r\n  * ResourceVariable support for gather_nd.\r\n  * ResourceVariable's gather op supports batch dimensions.\r\n  * Variadic reduce is supported on CPU\r\n  * Extend tf.function with basic support for CompositeTensors arguments (such as SparseTensor and RaggedTensor).\r\n  * Add templates and interfaces for creating lookup tables\r\n  * Post-training quantization tool supports quantizing weights shared by multiple operations. The models made with versions of this tool will use INT8 types for weights and will only be executable interpreters from this version onwards.\r\n  * Malformed gif images could result in an access out of bounds in the color palette of the frame. This has been fixed now\r\n  * image.resize now considers proper pixel centers and has new kernels (incl. anti-aliasing).\r\n* Performance\r\n  * Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically dispatches the best kernel implementation based on CPU vector architecture. To disable them, build with --define=tensorflow_mkldnn_contraction_kernel=0.\r\n  * Support for multi-host ncclAllReduce in Distribution Strategy.\r\n  * Expose a flag that allows the number of threads to vary across Python benchmarks.\r\n* TensorFlow 2.0 Development\r\n  * Add v2 sparse categorical crossentropy metric.\r\n  * Allow non-Tensors through v2 losses.\r\n  * Add UnifiedGRU as the new GRU implementation for tf2.0. Change the default recurrent activation function for GRU from 'hard_sigmoid' to 'sigmoid', and 'reset_after' to True in 2.0. Historically recurrent activation is 'hard_sigmoid' since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default GRU will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with GRU(recurrent_activation='hard_sigmoid', reset_after=False) to fallback to 1.x behavior.\r\n  * TF 2.0 - Update metric name to always reflect what the user has given in compile. Affects following cases 1. When name is given as 'accuracy'/'crossentropy' 2. When an aliased function name is used eg. 'mse' 3. Removing the `weighted` prefix from weighted metric names.\r\n  * Begin adding Go wrapper for C Eager API\r\n  * image.resize in 2.0 now supports gradients for the new resize kernels.\r\n  * removed tf.string_split from v2 API\r\n  * Expose tf.contrib.proto.* ops in tf.io (they will exist in TF2)\r\n  * \"Updates the TFLiteConverter API in 2.0. Changes from_concrete_function to from_concrete_functions.\"\r\n  * Enable tf.distribute.experimental.MultiWorkerMirroredStrategy working in eager mode.\r\n  * Support both binary and -1/1 label input in v2 hinge and squared hinge losses.\r\n* TensorFlow Lite\r\n  * \"Adds support for tflite_convert in 2.0.\"\r\n  * \"Remove lite.OpHint, lite.experimental, and lite.constant from 2.0 API.\"\r\n* tf.contrib\r\n  * Added Neural Turing Implementation as described in https://arxiv.org/abs/1807.08518.\r\n  * Remove tf.contrib.timeseries dependency on TF distributions.\r\n* tf.data\r\n  * Add num_parallel_reads and passing in a Dataset containing filenames into TextLineDataset and FixedLengthRecordDataset\r\n  * Going forward we operate in TF 2.0, this change is part of the effort to slowly converting XYZDataset to DatasetV2 type which is the official version going to be used in TF 2.0 and motivated by some compatibility issue found, _BigtableXYZDataset (of type DatasetV2) does not implement the _as_variant_tensor() of DatasetV1, when moving contrib.bigtable to tensorflow_io. Converting into DatasetV2 removes the overheads to maintain V1 while we are moving into TF 2.0.\r\n  * Add dataset ops to the graph (or create kernels in Eager execution) during the python Dataset object creation instead doing it during Iterator creation time.\r\n  * Add support for TensorArrays to tf.data Dataset.\r\n  * Switching tf.data functions to use `defun`, providing an escape hatch to continue using the legacy `Defun`.\r\n* Toolchains\r\n  * CUDNN_INSTALL_PATH, TENSORRT_INSTALL_PATH, NCCL_INSTALL_PATH, NCCL_HDR_PATH are deprecated. Use TF_CUDA_PATHS instead which supports a comma-separated list of base paths that are searched to find CUDA libraries and headers.\r\n  * TF code now resides in `tensorflow_core` and `tensorflow` is just a virtual pip package. No code changes are needed for projects using TensorFlow, the change is transparent\r\n* XLA\r\n  * XLA HLO graphs can be inspected with interactive_graphviz tool now.\r\n* Estimator\r\n  * Use `tf.compat.v1.estimator.inputs` instead of `tf.estimator.inputs`\r\n  * Replace `contrib` references with `tf.estimator.experimental.*` for APIs in `early_stopping.py`\r\n  * Determining the \u201ccorrect\u201d value of the `--iterations_per_loop` for TPUEstimator or DistributionStrategy continues to be a challenge for our users. We propose dynamically tuning the `--iterations_per_loop` variable, specifically for using TPUEstimator in training mode, based on a user target TPU execution time. Users might specify a value such as: `--iterations_per_loop=300s`, which will result in roughly 300 seconds being spent on the TPU between host side operations.\r\n\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n1e100, 4d55397500, a6802739, abenmao, Adam Weiss, Ag Ramesh, Alan Du, Albin Joy, Alex, Aman Patel, Amit, Amit Kumar Jaiswal, Amit Srivastava, Andreas Eberle, Andy Craze, Anthony Platanios, Armen Poghosov, armenpoghosov, arp95, Arpit Shah, Ashwin Ramaswami, Aurelien Geron, Aur\u00e9Lien Geron, aweers, awesomealex1, Ayush Agrawal, Ben Barsdell, Bharat Raghunathan, Bhavani Subramanian, blairhan, Bl\u00e9Nesi Attila, Brandon Carter, candy.dc, Chao Liu, chenchc, chie8842, Christian Hansen, Christian Sigg, Clayne Robison, crafet, csukuangfj, ctiijima, Dan Jarvis, Dan Lazewatsky, Daniel Ingram, Daniel Salvadori, Dave Airlie, David Norman, Dayananda V, Dayananda-V, delock, Denis Khalikov, Deven Desai, Dheeraj Rajaram Reddy, dmitrievanthony, Donovan Ong, Drew Szurko, Duncan Riach, Dustin Neighly, Edward Forgacs, EFanZh, Fei Hu, Felix Lemke, Filip Matzner, fo40225, frreiss, Gautam, gehring, Geoffrey Irving, Grzegorz George Pawelczak, Grzegorz Pawelczak, Gyoung-Yoon Ryoo, HanGuo97, Hanton Yang, Hari Shankar, hehongliang, Heungsub Lee, Hoeseong Kim, I-Hong Jhuo, Ilango R, Innovimax, Irene Dea, Jacky Ko, Jakub Lipinski, Jason Zaman, jcf94, Jeffrey Poznanovic, Jens Elofsson, Jeroen B\u00e9Dorf, Jia Qingtong, Jiankang, Joe Q, Joe Quadrino, Joeran Beel, Jonas Rauber, Jonathan, Jonathan Kyl, Joppe Geluykens, Joseph Friedman, jtressle, jwu, K Yasaswi Sri Chandra Gandhi, K. Hodges, Kaixi Hou, Karl Lessard, Karl Weinmeister, Karthik Muthuraman, Kashif Rasul, KDR, Keno Fischer, Kevin Mader, kjopek, Koan-Sin Tan, kouml, ktaebum, Lakshay Tokas, Laurent Le Brun, Letian Kang, Li, Guizi, Loo Rong Jie, Lucas Hendren, Lukas Geiger, Luke Han, luxupu, Ma, Guokai, Mahmoud Abuzaina, Mandar Deshpande, manhyuk, Marco Gaido, Marek Drozdowski, Mark Collier, Mark Ryan, mars20, Mateusz Chudyk, Matt Conley, MattConley, mbhuiyan, mdfaijul, Melissa Grueter, Michael K\u00e4Ufl, Micka\u00ebL Schoentgen, Miguel Morin, Mihail Salnikov, Mike Arpaia, Mike Holcomb, monklof, Moses Marin, Mshr-H, nammbash, Natalia Gimelshein, Nayana-Ibm, neargye, Neeraj Pradhan, Nehal J Wani, Nick, Niels Ole Salscheider, Niranjan Hasabnis, nlewycky, Nuka-137, Nutti, olicht, P Sudeepam, Palmer Lao, Pan Daoxin, Pariksheet Pinjari, Pavel Samolysov, PENGWA, Pooya Davoodi, R S Nikhil Krishna, Rohit Gupta, Roman Soldatow, rthadur, Ruizhe, Ryan Jiang, Samantha Andow, Sami Kama, Sana-Damani, Saurabh Deoras, sdamani, seanshpark, Sebastien Iooss, Serv-Inc, Shahzad Lone, Shashank Gupta, Shashi, shashvat, shashvatshahi1998, Siju, Siju Samuel, Snease-Abq, Spencer Schaber, sremedios, srinivasan.narayanamoorthy, Steve Lang, Steve Nesae, Sumesh Udayakumaran, Supriya Rao, Taylor Jakobson, Taylor Thornton, Ted Chang, ThisIsPIRI, Thomas Deegan, Thomas Hagebols, tianyapiaozi, Tim Zaman, tomguluson92, Tongxuan Liu, TungJerry, v1incent, Vagif, vcarpani, Vikram Tiwari, Vishwak Srinivasan, Vitor-Alves, wangsiyu, wateryzephyr, WeberXie, WeijieSun, Wen-Heng (Jack) Chung, wenxizhu, Will Battel, William D. Irons, wyzhao, Xin, Xinan Jiang, Yasuhiro Matsumoto, ymodak, Yong Tang, Younes Khoudli, Yuan Lin, Yves-Noel Weweler, Zantares, zjjott, \u535c\u5c45, \u738b\u632f\u534e (Wang Zhenhua), \u9ec4\u946b", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.14.0-rc1", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.14.0-rc1", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.14.0-rc1", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/17865742"}, {"tag_name": "v2.0.0-beta0", "name": "TensorFlow 2.0.0-beta0", "author_name": "goldiegadde", "body": "# Release 2.0.0-beta0\r\n\r\n## Major Features and Improvements\r\n\r\nTensorFlow 2.0 focuses on **simplicity** and **ease of use**, featuring updates like:\r\n\r\n* Easy model building with Keras and eager execution\r\n* Robust model deployment in production on any platform\r\n* Powerful experimentation for research\r\n* API simplification by reducing duplication and removing deprecated endpoints\r\n\r\nThe feature improvements, fixes noted here are post TF 2.0 Alpha release. Please refer to [Alpha release notes](https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-alpha0) in case you missed it.\r\n\r\nFor information on upgrading your existing TensorFlow 1.x models, please refer to our [Upgrade](https://medium.com/tensorflow/upgrading-your-code-to-tensorflow-2-0-f72c3a4d83b5) and [Migration](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/migration_guide.ipynb) guides.\r\nWe have also released a collection of [tutorials and getting started guides](https://www.tensorflow.org/beta), and an [Effective Style Guide for TF 2.0](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/effective_tf2.md)\r\nFor more information on these community-driven changes, be sure to check out the [RFCs](https://github.com/tensorflow/community/tree/master/rfcs) we have on Github. If you care about details, all of the RFCs are important.\r\n\r\n## Highlights\r\n\r\n* Distribution Strategy: TF 2.0 users will be able to use the new [`tf.distribute.Strategy`](https://www.tensorflow.org/beta/guide/distribute_strategy) API to distribute training with minimal code changes, yielding good out-of-the-box performance. We have more strategies supported in the beta release, as well as improved support for custom training loops and Keras subclassed models. Check out the [guide](https://www.tensorflow.org/beta/guide/distribute_strategy) to see what\u2019s supported now.\r\n* API Freeze: Symbol renaming/deprecation and 2.0 API changes are complete. 2.0 API is final and is also available as part of the [TensorFlow 1.14 release](https://github.com/tensorflow/tensorflow/releases/tag/v1.14.0-rc0) in compat.v2 module. A list of all symbol changes can be found [here](https://docs.google.com/spreadsheets/d/1FLFJLzg7WNP6JHODX5q8BDgptKafq_slHpnHVbJIteQ/edit#gid=0)\r\n\r\n## Breaking Changes\r\n\r\n* `tf.contrib` has been deprecated, and functionality has been either migrated to the core TensorFlow API, to [tensorflow/addons](https://www.github.com/tensorflow/addons), or removed entirely.\r\n* Premade estimators in the tf.estimator.DNN/Linear/DNNLinearCombined family have been updated to use `tf.keras.optimizers` instead of the `tf.compat.v1.train.Optimizer`s. If you do not pass in an `optimizer=` arg or if you use a string, the premade estimator will use the keras optimizer. This is checkpoint breaking, as the optimizers have separate variables. A checkpoint converter tool for converting optimizers is included with the release,  but if you want to avoid any change, switch to the v1 version of the estimator:  `tf.compat.v1.estimator.DNN/Linear/DNNLinearCombined*`.\r\n\r\nRefer to our [public project status tracker](https://github.com/orgs/tensorflow/projects/4) and [issues tagged with `2.0`](https://github.com/tensorflow/tensorflow/issues?q=is%3Aopen+is%3Aissue+label%3A2.0) on GitHub for insight into recent issues and development progress.\r\n\r\nIf you experience any snags when using TF 2.0, please let us know at the [TF 2.0 Testing User Group](https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/testing). We have a support mailing list as well as weekly testing meetings, and would love to hear your migration feedback and questions.\r\n\r\n\r\n## Bug Fixes and Other Changes\r\n\r\n* `tf.data`:\r\n  * Adds option for introducing slack in the pipeline to reduce CPU contention, via `options =    `tf.data.Options()`; `options.experimental_slack = True; dataset = dataset.with_options(options)`\r\n  * Removing the `experimental_numa_aware` option from `tf.data.Options`.\r\n  * Add support for TensorArrays to `tf.data Dataset`.\r\n* `tf.keras`:\r\n  * Saving a Keras Model using `tf.saved_model.save` now saves the list of variables, trainable variables, regularization losses, and the call function.\r\n  * `tf.keras.estimator.model_to_estimator` now supports exporting to `tf.train.Checkpoint format`, which allows the saved checkpoints to be compatible with `model.load_weights`.\r\n  * `tf.keras.model.save_model` and `model.save` now defaults to saving a TensorFlow SavedModel.\r\n  * `OMP_NUM_THREADS` is no longer used by the default Keras config.  To configure the number of threads, use `tf.config.threading` APIs.\r\n  * Mark Keras `set_session` as `compat.v1` only\r\n* `tf.estimator`:\r\n  * Replace `tf.contrib.estimator.add_metrics` with `tf.estimator.add_metrics`\r\n* `tf.lite`:\r\n  * \"Update the TFLiteConverter API in 2.0. Changes from_concrete_function to from_concrete_functions.\"\r\n  * \"Add support for tflite_convert in 2.0.\"\r\n* `tf.contrib`:\r\n  * Expose tf.contrib.proto.* ops in tf.io (they will exist in TF2)\r\n* Other:\r\n  * Eliminate race condition during XLA convolution autotuning.\r\n  * Bug fix for `tf.tile` gradient\r\n  * TF code now resides in `tensorflow_core` and `tensorflow` is just a virtual pip package. No code changes are needed for projects using TensorFlow, the change is transparent.\r\n  * Added gradient for SparseToDense op.\r\n  * Expose a flag that allows the number of threads to vary across Python benchmarks.\r\n  * ResourceVariable's gather op supports batch dimensions.\r\n  * `image.resize` in 2.0 now supports gradients for the new resize kernels.\r\n  * removed `tf.string_split` from v2 API.\r\n  * Variadic reduce is supported on CPU Variadic reduce is supported on CPU\r\n  * Added GPU implementation of `tf.linalg.tridiagonal_solve`.\r\n  * Delete unused lookup table code.\r\n  * Going forward we operate in TF 2.0, this change is part of the effort to slowly converting XYZDataset to DatasetV2 type which is the official version going to be used in TF 2.0 and motivated by some compatibility issue found, `_BigtableXYZDataset` (of type `DatasetV2`) does not implement the `_as_variant_tensor()` of DatasetV1, when moving `contrib.bigtable` to `tensorflow_io`. Converting into `DatasetV2` removes the overheads to maintain V1 while we are moving into TF 2.0.\r\n  * Remove unused `StringViewVariantWrapper`.\r\n  * Delete unused Fingerprint64Map op registration.\r\n  * Add broadcasting support to `tf.matmul`.\r\n  * Add ellipsis (...) support for `tf.einsum()`.\r\n  * ResourceVariable support for `gather_nd`.\r\n  * Add expand_composites argument to all nest.* methods.\r\n  * Standardize the LayerNormalization API by replacing the args `norm_axis` and `params_axis` with `axis`.\r\n  * Add a new \"result_type\" parameter to `tf.strings.split`.\r\n  * `add_update` can now be passed a zero-arg callable in order to support turning off the update when setting `trainable=False` on a Layer of a Model compiled with `run_eagerly=True`.\r\n  * Added `tf.random.binomial`.\r\n  * Extend `tf.function` with basic support for CompositeTensors arguments (such as SparseTensor and RaggedTensor).\r\n  * Add name argument to `tf.string_split` and `tf.strings_split`\r\n  * Added `strings.byte_split`\r\n  * Add num_parallel_reads and passing in a Dataset containing filenames into TextLineDataset and FixedLengthRecordDataset\r\n  * \"Remove `lite.OpHint`, `lite.experimental`, and `lite.constant` from 2.0 API.\"\r\n  * CUDNN_INSTALL_PATH, TENSORRT_INSTALL_PATH, NCCL_INSTALL_PATH, NCCL_HDR_PATH are deprecated. Use TF_CUDA_PATHS instead which supports a comma-separated list of base paths that are searched to find CUDA libraries and headers.\r\n  * Add RaggedTensor.placeholder()\r\n  * Add pfor converter for Squeeze.\r\n  * Renamed `tf.image` functions to remove duplicate \"image\" where it is redundant.\r\n  * Add C++ Gradient for BatchMatMulV2.\r\n  * Set default loss reduction as `AUTO` for improving reliability of loss scaling with distribution strategy and custom training loops. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used in distribution strategy scope, outside of built-in training loops such as `tf.keras` `compile` and `fit`, we expect reduction value to be 'None' or 'SUM'. Using other values will raise an error.\r\n  * `parallel_for.pfor`: add converters for Softmax, LogSoftmax, IsNaN, All, Any, and MatrixSetDiag.\r\n  * parallel_for: add converters for LowerTriangularSolve and Cholesky.\r\n  * Add ragged tensor support to `tf.squeeze`\r\n  * Allow `LinearOperator.solve` to take a `LinearOperator`.\r\n  * Allow all dtypes for `LinearOperatorCirculant`.\r\n  * Introduce MaxParallelism method\r\n  * parallel_for: add converter for `BroadcastTo`.\r\n  * Add LinearOperatorHouseholder.\r\n  * Added `key` and `skip` methods to `random.experimental.Generator`.\r\n  * Adds Philox support to new stateful RNG's XLA path.\r\n  * Update RaggedTensors to support int32 row_splits.\r\n  * Add TensorSpec support for CompositeTensors.\r\n  * Added partial_pivoting input parameter to tf.linalg.tridiagonal_solve.\r\n  * Extend `tf.strings.split` to support inputs with any rank\r\n  * Improve the performance of datasets using `from_tensors()`.\r\n  * Add `tf.linalg.tridiagonal_mul` op.\r\n  * Add `LinearOperatorToeplitz`.\r\n  * Added gradient to `tf.linalg.tridiagonal_solve`.\r\n  * Removed TensorFlow Lite Android example (moved to new examples repo).\r\n  * Updating TF 2.0 `keras.backend.name_scope` to use TF 2.0 `name_scope`.\r\n  * Upgraded LIBXSMM to version 1.11.\r\n  * parallel_for: add converters for `LogMatrixDeterminant` and `MatrixBandPart`.\r\n  * Promoting `tf.data.experimental.enumerate_dataset` to core as `tf.data.Dataset.enumerate`.\r\n  * Uniform processing of quantized embeddings by Gather and EmbeddingLookup Ops\r\n  * Integrate Ragged Tensors with `tf.data`.\r\n  * Correct a misstatement in the documentation of the sparse softmax cross entropy logit parameter.\r\n  * parallel_for: Add converters for `OneHot`, `LowerBound`, `UpperBound`.\r\n  * Added GPU implementation of `tf.linalg.tridiagonal_matmul`.\r\n  * Add gradient to `tf.linalg.tridiagonal_matmul`.\r\n  * Add `tf.ragged.boolean_mask`.\r\n  * `tf.switch_case` added, which selects a branch_fn based on a branch_index.\r\n  * The C++ kernel of gather op supports batch dimensions.\r\n  * Promoting `unbatch` from experimental to core API.\r\n  * Fixed default value and documentation for `trainable` arg of tf.Variable.\r\n  * Adds `tf.enable_control_flow_v2()` and `tf.disable_control_flow_v2()`\r\n  * EagerTensor now support buffer interface for tensors.\r\n  * This change bumps the version number of the FullyConnected Op to 5.\r\n  * tensorflow : crash when pointer become nullptr.\r\n  * Add support for defaulting the value of `cycle_length` argument of `tf.data.Dataset.interleave` to the number of schedulable CPU cores.\r\n* Minor docs fix for `is_gpu_available`.\r\n* Fix multiline magic.\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n4d55397500, a6802739, abenmao, Adam Richter, Ag Ramesh, Albin Joy, Alex, Aman Patel, Amit, Amit Kumar Jaiswal, Amit Srivastava, Andreas Eberle, Anthony Hsu, Anthony Platanios, Anuj Rawat, arp95, Arpit Shah, Astropeak, Augustina Ragwitz, Aurelien Geron, Aur\u00e9Lien Geron, avasid, aweers, Ayush Agrawal, Bairen Yi, Bayberry Z, Ben Barsdell, bhack, Bharat Raghunathan, Bhavani Subramanian, Bin Fan, blairhan, Bl\u00e9Nesi Attila, Bodin-E, Bryan Cutler, Cao Zongyan, chenchc, Cheng Chang, chengchingwen, chie8842, Christian Hansen, Christian Sigg, Christoph Boeddeker, Christopher Yeh, Clayne Robison, crafet, ctiijima, Daniel Rasmussen, Daniel Salvadori, David Norman, delock, Deven Desai, Donovan Ong, Duncan Dean, Duncan Riach, Dustin Neighly, Edward Forgacs, EFanZh, Evgeniy Polyakov, FAIJUL, Fangjun Kuang, Federico Martinez, Fei Hu, Filip Matzner, FlashTek, Fred Reiss, Fredrik Knutsson, Geoffrey Irving, George Sterpu, Grzegorz Pawelczak, Guozhong Zhuang, Gu\r\nrpreet Singh, Gyoung-Yoon Ryoo, Hanton Yang, Haraldur T\u00f3Mas Hallgr\u00edMsson, Huan Li (\u674e\u5353\u6853), I-Hong, Irene Dea, Jacky, Jason Zaman, Jason Zavaglia, Jeff Daily, Jeffrey Poznanovic, jer, Jeroen B\u00e9Dorf, jerryyin, jhalakp, jiakai, Jonathan, Justin Dujardin, Justin Tunis, Kaixi Hou, Karthik Muthuraman, Kay Zhu, KDR, Keno Fischer, Kevin Mader, Kilaru Yasaswi Sri Chandra Gandhi, Koan-Sin Tan, Lakshay Tokas, leonard951, Letian Kang, Li, Guizi, Lukas Geiger, luxupu, lvli, Ma, Guokai, Mahmoud Abuzaina, Maksym Kysylov, Mandar Deshpande, Margaret Maynard-Reid, Mark Ryan, Matt Conley, Mihail Salnikov, Mikalai Drabovich, Mike Holcomb, monklof, Moses Marin, Mr. Metal, Mshr-H, nammbash, Nathan Luehr, Neeraj Pradhan, Nick, Nick Lewycky, Niels Ole Salscheider, Niklas Silfverstr\u00f6M, Niranjan Hasabnis, Nuka-137, omeir1, P Sudeepam, Pan Daoxin, Pariksheet Pinjari, Pasquale Minervini, Patrick J. Lopresti, Pavel Akhtyamov, PENGWA, PeterLee, Philipp Jund, Pooya Davoodi, Pranav Marathe, R S Nikhil Krishna, Rohit Gupta, Roland Zimmermann, Roman Soldatow, rthadur, Ruizhe, saishruthi, Sami Kama, Sana-Damani, sdamani, Sean Morgan, seanshpark, Sebastien Iooss, Sergii Khomenko, Serv-Inc, Shashank Gupta, shashvat, Shashvat Chand Shahi, Siju Samuel, smilu97, sremedios, srinivasan.narayanamoorthy, Subin, Sumesh Udayakumaran, sunway513, sxwang, Takeo Sawada, Taylor Jakobson, Ted Chang, ThisIsIsaac, Thomas Deegan, Thomas Hagebols, Tim Zaman, Tongxuan Liu, Trent Lo, Trevor Morris, TungJerry, Tyorden, v1incent, Vijay Ravichandran, Viktor Gal, Vincent, Vishnuvardhan Janapati, wangsiyu, wateryzephyr, Wen-Heng (Jack) Chung, wenxizhu, Will Battel, William D. Irons, wyzhao, Xiaoming (Jason) Cui, Xiaoquan Kong, Xin, Yann-Yy, Yasuhiro Matsumoto, ymodak, Yong Tang, Yuan (Terry) Tang, Zantares, \u738b\u632f\u534e (Zhenhua Wang), \u9ec4\r\n", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.0.0-beta0", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.0.0-beta0", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-beta0", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/17852985"}, {"tag_name": "v1.14.0-rc0", "name": "TensorFlow 1.14.0-rc0", "author_name": "tensorflow-jenkins", "body": "# Release 1.14.0\r\n\r\n## Major Features and Improvements\r\n\r\n* This is the first 1.x release containing the compat.v2 module. This module is required to allow libraries to publish code which works in both 1.x and 2.x. After this release, no backwards incompatible changes are allowed in the 2.0 Python API.\r\n* Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically dispatches the best kernel implementation based on CPU vector architecture. To disable them, build with --define=tensorflow_mkldnn_contraction_kernel=0.\r\n\r\n## Behavioral changes\r\n\r\n* Set default loss reduction as `AUTO` for improving reliability of loss scaling with distribution strategy and custom training loops. `AUTO` indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used in distribution strategy scope, outside of built-in training loops such as `tf.keras` `compile` and `fit`, we expect reduction value to be 'None' or 'SUM'. Using other values will raise an error.\r\n* Wraps losses passed to the `compile` API (strings and v1 losses) which are not instances of v2 `Loss` class in `LossWrapper` class. => All losses will now use `SUM_OVER_BATCH_SIZE` reduction as default.\r\n* Disable `run_eagerly` and distribution strategy if there are symbolic tensors added to the model using `add_metric` or `add_loss`.\r\n* tf.linspace(start, stop, num) now always uses \"stop\" as last value (for num > 1)\r\n* `ResourceVariable` and `Variable` no longer accepts `constraint` in the constructor, nor expose it as a @property.\r\n* The behavior of tf.gather is now correct when axis=None and batch_dims<0.\r\n* Only create a GCS directory object if the object does not already exist.\r\n* In `map_vectorization` optimization, reduce the degree of parallelism in the vectorized map node.\r\n* Bug fix: loss and gradients should now more reliably be correctly scaled w.r.t. the global batch size when using a tf.distribute.Strategy.\r\n* Updating cosine similarity loss - removed the negate sign from cosine similarity.\r\n* DType is no longer convertible to an int. Use dtype.as_datatype_enum instead of int(dtype) to get the same result.\r\n* Changed default for gradient accumulation for TPU embeddings to true.\r\n* Callbacks now log values in eager mode when a deferred build model is used.\r\n* Transitive dependencies on :pooling_ops were removed.  Some users may need to add explicit dependencies on :pooling_ops if they reference the operators from that library.\r\n\r\n## Bug Fixes and Other Changes\r\n* Documentation\r\n* Deprecations and Symbol renames.\r\n  * Remove unused StringViewVariantWrapper\r\n  * Delete unused Fingerprint64Map op registration\r\n  * SignatureDef util functions have been deprecated.\r\n  * Renamed tf.image functions to remove duplicate \"image\" where it is redundant.\r\n  * tf.keras.experimental.export renamed to tf.keras.experimental.export_saved_model\r\n  * Standardize the LayerNormalization API by replacing the args `norm_axis` and `params_axis` with `axis`.\r\n  * Tensor::UnsafeCopyFromInternal deprecated in favor Tensor::BitcastFrom\r\n* Keras & Python API\r\n  * Add v2 module aliases for:\r\n    * tf.initializers => tf.keras.initializers\r\n    * tf.losses => tf.keras.losses & tf.metrics => tf.keras.metrics\r\n    * tf.optimizers => tf.keras.optimizers\r\n  * Add tf.keras.layers.AbstractRNNCell as the preferred implementation of RNN cell for TF v2. User can use it to implement RNN cell with custom behavior.\r\n  * Adding `clear_losses` API to be able to clear losses at the end of forward pass in a custom training loop in eager.\r\n  * Add support for passing list of lists to the `metrics` param in Keras `compile`.\r\n  * Added top-k to precision and recall to keras metrics.\r\n  * Adding public APIs for `cumsum` and `cumprod` keras backend functions.\r\n  * Fix: model.add_loss(symbolic_tensor) should work in ambient eager.\r\n  * Add name argument to tf.string_split and tf.strings_split\r\n  * Minor change to SavedModels exported from Keras using tf.keras.experimental.export. (SignatureDef key for evaluation mode is now \"eval\" instead of \"test\"). This will be reverted back to \"test\" in the near future.\r\n  * Updates binary cross entropy logic in Keras when input is probabilities. Instead of converting probabilities to logits, we are using the cross entropy formula for probabilities.\r\n  * Raw TensorFlow functions can now be used in conjunction with the Keras Functional API during model creation. This obviates the need for users to create Lambda layers in most cases when using the Functional API. Like Lambda layers, TensorFlow functions that result in Variable creation or assign ops are not supported.\r\n  * Keras training and validation curves are shown on the same plot.\r\n  * Introduce `dynamic` constructor argument in Layer and Model, which should be set to True when using imperative control flow in the `call` method.\r\n  * Removing of dtype in the constructor of initializers and partition_info in call.\r\n* New ops and improved op functionality\r\n  * Add OpKernels for some stateless maps\r\n  * Add v2 APIs for AUCCurve and AUCSummationMethod enums. #tf-metrics-convergence\r\n  * Add tf.math.nextafter op.\r\n  * Add CompositeTensor base class.\r\n  * Add tf.linalg.tridiagonal_solve op.\r\n  * Add opkernel templates for common table operations.\r\n  * Added support for TFLite in TensorFlow 2.0.\r\n  * Adds summary trace API for collecting graph and profile information.\r\n  * Add batch_dims argument to tf.gather.\r\n  * Add support for `add_metric` in the graph function mode.\r\n  * Add C++ Gradient for BatchMatMulV2.\r\n  * Added tf.random.binomial\r\n  * Added gradient for SparseToDense op.\r\n  * Add legacy string flat hash map op kernels\r\n  * Add a ragged size op and register it to the op dispatcher\r\n  * Add broadcasting support to tf.matmul.\r\n  * Add ellipsis (...) support for tf.einsum()\r\n  * Added LinearOperator.adjoint and LinearOperator.H (alias).\r\n  * Added GPU implementation of tf.linalg.tridiagonal_solve.\r\n  * Added strings.byte_split\r\n  * Add RaggedTensor.placeholder()\r\n  * Add a new \"result_type\" parameter to tf.strings.split\r\n  * `add_update` can now be passed a zero-arg callable in order to support turning off the update when setting `trainable=False` on a Layer of a Model compiled with `run_eagerly=True`.\r\n  * Add variant wrapper for absl::string_view\r\n  * Add expand_composites argument to all nest.* methods.\r\n  * Add pfor converter for Squeeze.\r\n  * Bug fix for tf.tile gradient\r\n  * Expose CriticalSection in core as tf.CriticalSection.\r\n  * Update Fingerprint64Map to use aliases\r\n  * ResourceVariable support for gather_nd.\r\n  * ResourceVariable's gather op supports batch dimensions.\r\n  * Variadic reduce is supported on CPU\r\n  * Extend tf.function with basic support for CompositeTensors arguments (such as SparseTensor and RaggedTensor).\r\n  * Add templates and interfaces for creating lookup tables\r\n  * Post-training quantization tool supports quantizing weights shared by multiple operations. The models made with versions of this tool will use INT8 types for weights and will only be executable interpreters from this version onwards.\r\n  * Malformed gif images could result in an access out of bounds in the color palette of the frame. This has been fixed now\r\n  * image.resize now considers proper pixel centers and has new kernels (incl. anti-aliasing).\r\n* Performance\r\n  * Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically dispatches the best kernel implementation based on CPU vector architecture. To disable them, build with --define=tensorflow_mkldnn_contraction_kernel=0.\r\n  * Support for multi-host ncclAllReduce in Distribution Strategy.\r\n  * Expose a flag that allows the number of threads to vary across Python benchmarks.\r\n* TensorFlow 2.0 Development\r\n  * Add v2 sparse categorical crossentropy metric.\r\n  * Allow non-Tensors through v2 losses.\r\n  * Add UnifiedGRU as the new GRU implementation for tf2.0. Change the default recurrent activation function for GRU from 'hard_sigmoid' to 'sigmoid', and 'reset_after' to True in 2.0. Historically recurrent activation is 'hard_sigmoid' since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default GRU will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with GRU(recurrent_activation='hard_sigmoid', reset_after=False) to fallback to 1.x behavior.\r\n  * TF 2.0 - Update metric name to always reflect what the user has given in compile. Affects following cases 1. When name is given as 'accuracy'/'crossentropy' 2. When an aliased function name is used eg. 'mse' 3. Removing the `weighted` prefix from weighted metric names.\r\n  * Begin adding Go wrapper for C Eager API\r\n  * image.resize in 2.0 now supports gradients for the new resize kernels.\r\n  * removed tf.string_split from v2 API\r\n  * Expose tf.contrib.proto.* ops in tf.io (they will exist in TF2)\r\n  * \"Updates the TFLiteConverter API in 2.0. Changes from_concrete_function to from_concrete_functions.\"\r\n  * Enable tf.distribute.experimental.MultiWorkerMirroredStrategy working in eager mode.\r\n  * Support both binary and -1/1 label input in v2 hinge and squared hinge losses.\r\n* TensorFlow Lite\r\n  * \"Adds support for tflite_convert in 2.0.\"\r\n  * \"Remove lite.OpHint, lite.experimental, and lite.constant from 2.0 API.\"\r\n* tf.contrib\r\n  * Added Neural Turing Implementation as described in https://arxiv.org/abs/1807.08518.\r\n  * Remove tf.contrib.timeseries dependency on TF distributions.\r\n* tf.data\r\n  * Add num_parallel_reads and passing in a Dataset containing filenames into TextLineDataset and FixedLengthRecordDataset\r\n  * Going forward we operate in TF 2.0, this change is part of the effort to slowly converting XYZDataset to DatasetV2 type which is the official version going to be used in TF 2.0 and motivated by some compatibility issue found, _BigtableXYZDataset (of type DatasetV2) does not implement the _as_variant_tensor() of DatasetV1, when moving contrib.bigtable to tensorflow_io. Converting into DatasetV2 removes the overheads to maintain V1 while we are moving into TF 2.0.\r\n  * Add dataset ops to the graph (or create kernels in Eager execution) during the python Dataset object creation instead doing it during Iterator creation time.\r\n  * Add support for TensorArrays to tf.data Dataset.\r\n  * Switching tf.data functions to use `defun`, providing an escape hatch to continue using the legacy `Defun`.\r\n* Toolchains\r\n  * CUDNN_INSTALL_PATH, TENSORRT_INSTALL_PATH, NCCL_INSTALL_PATH, NCCL_HDR_PATH are deprecated. Use TF_CUDA_PATHS instead which supports a comma-separated list of base paths that are searched to find CUDA libraries and headers.\r\n  * TF code now resides in `tensorflow_core` and `tensorflow` is just a virtual pip package. No code changes are needed for projects using TensorFlow, the change is transparent\r\n* XLA\r\n  * XLA HLO graphs can be inspected with interactive_graphviz tool now.\r\n* Estimator\r\n  * Use tf.compat.v1.estimator.inputs instead of tf.estimator.inputs\r\n  * Replace contrib references with tf.estimator.experimental.* for apis in early_stopping.py\r\n\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n1e100, 4d55397500, a6802739, abenmao, Adam Weiss, Ag Ramesh, Alan Du, Albin Joy, Alex, Aman Patel, Amit, Amit Kumar Jaiswal, Amit Srivastava, Andreas Eberle, Andy Craze, Anthony Platanios, Armen Poghosov, armenpoghosov, arp95, Arpit Shah, Ashwin Ramaswami, Aurelien Geron, Aur\u00e9Lien Geron, aweers, awesomealex1, Ayush Agrawal, Ben Barsdell, Bharat Raghunathan, Bhavani Subramanian, blairhan, Bl\u00e9Nesi Attila, Brandon Carter, candy.dc, Chao Liu, chenchc, chie8842, Christian Hansen, Christian Sigg, Clayne Robison, crafet, csukuangfj, ctiijima, Dan Jarvis, Dan Lazewatsky, Daniel Ingram, Daniel Salvadori, Dave Airlie, David Norman, Dayananda V, Dayananda-V, delock, Denis Khalikov, Deven Desai, Dheeraj Rajaram Reddy, dmitrievanthony, Donovan Ong, Drew Szurko, Duncan Riach, Dustin Neighly, Edward Forgacs, EFanZh, Fei Hu, Felix Lemke, Filip Matzner, fo40225, frreiss, Gautam, gehring, Geoffrey Irving, Grzegorz George Pawelczak, Grzegorz Pawelczak, Gyoung-Yoon Ryoo, HanGuo97, Hanton Yang, Hari Shankar, hehongliang, Heungsub Lee, Hoeseong Kim, I-Hong Jhuo, Ilango R, Innovimax, Irene Dea, Jacky Ko, Jakub Lipinski, Jason Zaman, jcf94, Jeffrey Poznanovic, Jens Elofsson, Jeroen B\u00e9Dorf, Jia Qingtong, Jiankang, Joe Q, Joe Quadrino, Joeran Beel, Jonas Rauber, Jonathan, Jonathan Kyl, Joppe Geluykens, Joseph Friedman, jtressle, jwu, K Yasaswi Sri Chandra Gandhi, K. Hodges, Kaixi Hou, Karl Lessard, Karl Weinmeister, Karthik Muthuraman, Kashif Rasul, KDR, Keno Fischer, Kevin Mader, kjopek, Koan-Sin Tan, kouml, ktaebum, Lakshay Tokas, Laurent Le Brun, Letian Kang, Li, Guizi, Loo Rong Jie, Lucas Hendren, Lukas Geiger, Luke Han, luxupu, Ma, Guokai, Mahmoud Abuzaina, Mandar Deshpande, manhyuk, Marco Gaido, Marek Drozdowski, Mark Collier, Mark Ryan, mars20, Mateusz Chudyk, Matt Conley, MattConley, mbhuiyan, mdfaijul, Melissa Grueter, Michael K\u00e4Ufl, Micka\u00ebL Schoentgen, Miguel Morin, Mihail Salnikov, Mike Arpaia, Mike Holcomb, monklof, Moses Marin, Mshr-H, nammbash, Natalia Gimelshein, Nayana-Ibm, neargye, Neeraj Pradhan, Nehal J Wani, Nick, Niels Ole Salscheider, Niranjan Hasabnis, nlewycky, Nuka-137, Nutti, olicht, P Sudeepam, Palmer Lao, Pan Daoxin, Pariksheet Pinjari, Pavel Samolysov, PENGWA, Pooya Davoodi, R S Nikhil Krishna, Rohit Gupta, Roman Soldatow, rthadur, Ruizhe, Ryan Jiang, Samantha Andow, Sami Kama, Sana-Damani, Saurabh Deoras, sdamani, seanshpark, Sebastien Iooss, Serv-Inc, Shahzad Lone, Shashank Gupta, Shashi, shashvat, shashvatshahi1998, Siju, Siju Samuel, Snease-Abq, Spencer Schaber, sremedios, srinivasan.narayanamoorthy, Steve Lang, Steve Nesae, Sumesh Udayakumaran, Supriya Rao, Taylor Jakobson, Taylor Thornton, Ted Chang, ThisIsPIRI, Thomas Deegan, Thomas Hagebols, tianyapiaozi, Tim Zaman, tomguluson92, Tongxuan Liu, TungJerry, v1incent, Vagif, vcarpani, Vikram Tiwari, Vishwak Srinivasan, Vitor-Alves, wangsiyu, wateryzephyr, WeberXie, WeijieSun, Wen-Heng (Jack) Chung, wenxizhu, Will Battel, William D. Irons, wyzhao, Xin, Xinan Jiang, Yasuhiro Matsumoto, ymodak, Yong Tang, Younes Khoudli, Yuan Lin, Yves-Noel Weweler, Zantares, zjjott, \u535c\u5c45, \u738b\u632f\u534e (Wang Zhenhua), \u9ec4\u946b", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.14.0-rc0", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.14.0-rc0", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.14.0-rc0", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/17563160"}, {"tag_name": "v1.12.2", "name": "TensorFlow 1.12.2", "author_name": "mihaimaruseac", "body": "# Release 1.12.2\r\n## Bug Fixes and Other Changes\r\n\r\n* Fixes a potential security vulnerability where carefully crafted GIF images can produce a null pointer dereference during decoding", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.12.2", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.12.2", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.12.2", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/16858424"}, {"tag_name": "v2.0.0-alpha0", "name": "TensorFlow 2.0.0-alpha0", "author_name": "goldiegadde", "body": "# Release 2.0.0-alpha0\r\n\r\n## Major Features and Improvements\r\n\r\nTensorFlow 2.0 focuses on **simplicity** and **ease of use**, featuring updates like:\r\n\r\n* Easy model building with Keras and eager execution.\r\n* Robust model deployment in production on any platform.\r\n* Powerful experimentation for research.\r\n* API simplification by reducing duplication removing deprecated endpoints.\r\n\r\nFor information on upgrading your existing TensorFlow 1.x models, please refer to our [Upgrade](https://medium.com/tensorflow/upgrading-your-code-to-tensorflow-2-0-f72c3a4d83b5) and [Migration](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/migration_guide.ipynb) guides.\r\nWe have also released a collection of [tutorials and getting started guides](https://github.com/tensorflow/docs/tree/master/site/en/r2), and an [Effective Style Guide for TF 2.0](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/effective_tf2.md).\r\n\r\nFor more information on these community-driven changes, be sure to check out the [RFCs](https://github.com/tensorflow/community/tree/master/rfcs) we have on Github. If you care about details, all of the RFCs are important.\r\nRefer to our [public project status tracker](https://github.com/orgs/tensorflow/projects/4) and [issues tagged with `2.0`](https://github.com/tensorflow/tensorflow/issues?q=is%3Aopen+is%3Aissue+label%3A2.0) on GitHub for insight into recent issues and development progress.\r\n\r\nAnd, of course: we would love to have your feedback! If you experience any snags when using TF 2.0, be sure to let us know at the [TF 2.0 Testing User Group](https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!forum/testing). We have a support mailing list as well as weekly testing meetings, and would love to hear your migration feedback and questions.\r\n\r\n### Some highlights:\r\n\r\n  * API clean-up, included removing `tf.app`, `tf.flags`, and `tf.logging` in favor of [absl-py](https://github.com/abseil/abseil-py).\r\n  * No more global variables with helper methods like `tf.global_variables_initializer` and `tf.get_global_step`.\r\n  * Functions, not sessions (`tf.Session` and `session.run` -> `tf.function`).\r\n  * Added support for TensorFlow Lite in TensorFlow 2.0.\r\n\r\n## Breaking Changes\r\n\r\n  * `tf.contrib` has been deprecated, and functionality has been either migrated to the core TensorFlow API, to [`tensorflow/addons`](https://www.github.com/tensorflow/addons), or removed entirely.\r\n  * Checkpoint breakage for [RNNs](https://github.com/tensorflow/tensorflow/issues/26350) and for [Optimizers](https://github.com/tensorflow/tensorflow/issues/26349).\r\n\r\n## Bug Fixes and Other Changes\r\n\r\n* `tf.estimator`\r\n  * Use `tf.compat.v1.estimator.inputs` instead of `tf.estimator.inputs` in Estimator.\r\n  * Replace contrib references with `tf.estimator.experimental.*` for apis in `early_stopping.py` in Estimator.\r\n* Keras & Python API\r\n  * Added top-k to precision and recall to keras metrics.\r\n  * Adding public APIs for `cumsum` and `cumprod` keras backend functions.\r\n  * Minor change to SavedModels exported from Keras using `tf.keras.experimental.export`. (SignatureDef key for evaluation mode is now \"eval\" instead of \"test\"). This will be reverted back to \"test\" in the near future.\r\n  * Add v2 module aliases for losses and metrics: `tf.losses = tf.keras.losses` & `tf.metrics = tf.keras.metrics`\r\n  * Add v2 module aliases for optimizers: `tf.optimizers = tf.keras.optimizers`\r\n  * `tf.keras.experimental.export` renamed to `tf.keras.experimental.export_saved_model`\r\n  * Add v2 module aliases for initializers: tf.initializers = tf.keras.initializers\r\n  * Add `tf.keras.layers.AbstractRNNCell` as the preferred implementation of RNN cell for TF v2. User can use it to implement RNN cell with custom behavior.\r\n  * Keras training and validation curves are shown on the same plot.\r\n  * Disable `run_eagerly` and distribution strategy if there are symbolic tensors added to the model using `add_metric` or `add_loss`.\r\n* Other:\r\n  * Only create a GCS directory object if the object does not already exist.\r\n  * Introduce `dynamic` constructor argument in Layer and Model, which should be set to True when using imperative control flow in the `call` method.\r\n  * `ResourceVariable` and `Variable` no longer accepts `constraint` in the constructor, nor expose it as a @property.\r\n  * `ResourceVariab...\r\n  * Add UnifiedGRU as the new GRU implementation for tf2.0. Change the default recurrent activation function for GRU from 'hard_sigmoid' to 'sigmoid', and 'reset_after' to True in 2.0. Historically recurrent activation is 'hard_sigmoid' since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default GRU will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with GRU(recurrent_activation='hard_sigmoid', reset_after=False) to fallback to 1.x behavior.\r\n  * Begin adding Go wrapper for C Eager API\r\n  * XLA HLO graphs can be inspected with interactive_graphviz tool now.\r\n  * Add dataset ops to the graph (or create kernels in Eager execution) during the python Dataset object creation instead doing it during Iterator creation time.\r\n  * Add batch_dims argument to tf.gather.\r\n  * Removing of dtype in the constructor of initializers and partition_info in call.\r\n  * Add `tf.math.nextafter` op.\r\n  * Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically dispatches the best kernel implementation based on CPU vector architecture. To disable them, build with --define=tensorflow_mkldnn_contraction_kernel=0.\r\n  * Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically dispatches the best kernel implementation based on CPU vector architecture. To disable them, build with --define=tensorflow_mkldnn_contraction_kernel=0.\r\n  * Turn on MKL-DNN contraction kernels by default. MKL-DNN dynamically dispatches the best kernel implementation based on CPU vector architecture. To disable them, build with --define=tensorflow_mkldnn_contraction_kernel=0.\r\n  * `tf.linspace(start, stop, num)` now always uses \"stop\" as last value (for num > 1)\r\n  * Raw TensorFlow functions can now be used in conjunction with the Keras Functional API during model creation. This obviates the need for users to create Lambda layers in most cases when using the Functional API. Like Lambda layers, TensorFlow functions that result in Variable creation or assign ops are not supported.\r\n  * Add a ragged size op and register it to the op dispatcher\r\n  * Transitive dependencies on :pooling_ops were removed.  Some users may need to add explicit dependencies on :pooling_ops if they reference the operators from that library.\r\n  * Updates binary cross entropy logic in Keras when input is probabilities. Instead of converting probabilities to logits, we are using the cross entropy formula for probabilities.\r\n  * Add CompositeTensor base class.\r\n  * Malformed gif images could result in an access out of bounds in the color palette of the frame. This has been fixed now\r\n  * Add templates and interfaces for creating lookup tables\r\n  * `Tensor::UnsafeCopyFromInternal` deprecated in favor `Tensor::BitcastFrom`.\r\n  * In `map_vectorization` optimization, reduce the degree of parallelism in the vectorized map node.\r\n  * Add variant wrapper for `absl::string_view`.\r\n  * Post-training quantization tool supports quantizing weights shared by multiple operations. The models made with versions of this tool will use INT8 types for weights and will only be executable interpreters from this version onwards.\r\n  * Wraps losses passed to the `compile` API (strings and v1 losses) which are not instances of v2 `Loss` class in `LossWrapper` class. => All losses will now use `SUM_OVER_BATCH_SIZE` reduction as default.\r\n  * Add OpKernels for some stateless maps\r\n  * Add v2 APIs for AUCCurve and AUCSummationMethod enums.\r\n  * Add v2 APIs for AUCCurve and AUCSummationMethod enums.\r\n  * Allow non-Tensors through v2 losses.\r\n  * Add v2 sparse categorical crossentropy metric.\r\n  * `DType` is no longer convertible to an int. Use `dtype.as_datatype_enum` instead of `int(dtype)` to get the same result.\r\n  * Support both binary and -1/1 label input in v2 hinge and squared hinge losses.\r\n  * Bug fix: loss and gradients should now more reliably be correctly scaled w.r.t. the global batch size when using a tf.distribute.Strategy.\r\n  * Added LinearOperator.adjoint and LinearOperator.H (alias).\r\n  * Switching `tf.data functions` to use `defun`, providing an escape hatch to continue using the legacy `Defun`.\r\n  * Expose CriticalSection in core as `tf.CriticalSection`.\r\n  * Enhanced graphviz output.\r\n  * The behavior of `tf.gather` is now correct when axis=None and batch_dims<0.\r\n  * Add `tf.linalg.tridiagonal_solve` op.\r\n  * Add opkernel templates for common table operations.\r\n  * Fix callbacks do not log values in eager mode when a deferred build model is used.\r\n  * SignatureDef util functions have been deprecated.\r\n  * Update Fingerprint64Map to use aliases\r\n  * Add legacy string flat hash map op kernels\r\n  * Add support for passing list of lists to the `metrics` param in Keras `compile.\r\n  * Keras training and validation curves are shown on the same plot.\r\n  * Fix: `model.add_loss(symbolic_tensor)` should work in ambient eager.\r\n  * Adding `clear_losses` API to be able to clear losses at the end of forward pass in a custom training loop in eager.\r\n  * Add support for `add_metric` in the graph function mode.\r\n  * Adding `clear_losses` API to be able to clear losses at the end of forward pass in a custom training loop in eager.\r\n  * Updating cosine similarity loss - removed the negate sign from cosine similarity.\r\n  * TF 2.0 - Update metric name to always reflect what the user has given in compile. Affects following cases 1. When name is given as 'accuracy'/'crossentropy' 2. When an aliased function name is used eg. 'mse' 3. Removing the `weighted` prefix from weighted metric names.\r\n  * Workaround for compiler bug.\r\n  * Changed default for gradient accumulation for TPU embeddings to true.\r\n  * Adds summary trace API for collecting graph and profile information.\r\n  * Support for multi-host `ncclAllReduce` in Distribution Strategy.\r\n  * Enable `tf.distribute.experimental.MultiWorkerMirroredStrategy` working in eager mode.\r\n  * `image.resize` now considers proper pixel centers and has new kernels (incl. anti-aliasing).\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\n1e100, a6802739, Abolfazl Shahbazi, Adam Weiss, Ag Ramesh, Alan Du, Albin Joy, Amit, Amit Srivastava, Andy Craze, Anshuman Tripathy, Armen Poghosov, armenpoghosov, Arpit Shah, Ashwin Ramaswami, Aurelien Geron, Aur\u00e9Lien Geron, aweers, awesomealex1, Bairen Yi, Ben Barsdell, Bhavani Subramanian, Brandon Carter, candy.dc, Chao Liu, Clayne Robison, csukuangfj, Dan Jarvis, Dan Lazewatsky, Daniel Ingram, Dave Airlie, David Norman, Dayananda V, Denis Khalikov, Deven Desai, Dheeraj Rajaram Reddy, dmitrievanthony, Drew Szurko, Duncan Riach, Fei Hu, Felix Lemke, Filip Matzner, fo40225, frreiss, Gautam, gehring, Grzegorz George Pawelczak, Grzegorz Pawelczak, HanGuo97, Hari Shankar, hehongliang, Heungsub Lee, Hoeseong Kim, I-Hong Jhuo, Ilango R, Innovimax, Jacky Ko, Jakub Lipinski, Jason Zaman, jcf94, Jeff Poznanovic, Jia Qingtong, Jiankang, Joe Q, Joe Quadrino, Jonas Rauber, Jonathan Kyl, Joppe Geluykens, Joseph Friedman, jtressle, jwu, K. Hodges, kaixih, Karl Lessard, Karl Weinmeister, Kashif Rasul, kjopek, Koan-Sin Tan, kouml, ktaebum, Laurent Le Brun, Li, Guizi, Loo Rong Jie, Lucas Hendren, Lukas Geiger, Luke Han, Mahmoud Abuzaina, manhyuk, Marco Gaido, Marek Drozdowski, Mark Ryan, mars20, Mateusz Chudyk, Matt Conley, MattConley, mbhuiyan, mdfaijul, Melissa Grueter, Michael K\u00e4Ufl, Micka\u00ebL Schoentgen, Miguel Morin, Mike Arpaia, nammbash, Natalia Gimelshein, Nayana-Ibm, neargye, Nehal J Wani, Niels Ole Salscheider, Niranjan Hasabnis, Nutti, olicht, P Sudeepam, Paige Bailey, Palmer Lao, Pariksheet Pinjari, Pavel Samolysov, Pooya Davoodi, Ryan Jiang, Samantha Andow, Sami Kama, Saurabh Deoras, Shahzad Lone, Shashi, Siju, Siju Samuel, Snease-Abq, Spencer Schaber, srinivasan.narayanamoorthy, Steve Lang, Steve Nesae, Supriya Rao, Taylor Jakobson, Taylor Thornton, ThisIsPIRI, Thomas Deegan, tomguluson92, Tongxuan Liu, Vagif, vcarpani, Vikram Tiwari, Vishwak Srinivasan, Vitor-Alves, wangsiyu, WeberXie, WeijieSun, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, Yan Facai (\u989c\u53d1\u624d), ymodak, Yong Tang, Younes Khoudli, Yuan Lin, Yves-Noel Weweler, zjjott, \u535c\u5c45, \u738b\u632f\u534e (Wang Zhenhua)", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v2.0.0-alpha0", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v2.0.0-alpha0", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0-alpha0", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/15935393"}, {"tag_name": "v1.13.1", "name": "TensorFlow 1.13.1", "author_name": "gunan", "body": "# Release 1.13.1\r\n\r\n## Major Features and Improvements\r\n\r\n* TensorFlow Lite has moved from contrib to core. This means that Python modules are under `tf.lite` and source code is now under `tensorflow/lite` rather than `tensorflow/contrib/lite`.\r\n* TensorFlow GPU binaries are now built against CUDA 10 and TensorRT 5.0.\r\n* Support for Python3.7 on all operating systems.\r\n* Moved NCCL to core.\r\n\r\n## Behavioral changes\r\n\r\n* Disallow conversion of python floating types to uint32/64 (matching behavior of other integer types) in `tf.constant`.\r\n* Make the `gain` argument of convolutional orthogonal initializers (`convolutional_delta_orthogonal`, `convolutional_orthogonal_1D`, `convolutional_orthogonal_2D`, `convolutional_orthogonal_3D`) have consistent behavior with the `tf.initializers.orthogonal` initializer, i.e. scale the output l2-norm by `gain` and NOT by `sqrt(gain)`. (Note that these functions are currently in `tf.contrib` which is not guaranteed backward compatible).\r\n\r\n## Bug Fixes and Other Changes\r\n\r\n* Documentation\r\n  * Update the doc with the details about the rounding mode used in quantize_and_dequantize_v2.\r\n  * Clarify that tensorflow::port::InitMain() _should_ be called before using the TensorFlow library.  Programs failing to do this are not portable to all platforms.\r\n* Deprecations and Symbol renames.\r\n   * Removing deprecations for the following endpoints: `tf.acos`, `tf.acosh`, `tf.add`, `tf.as_string`, `tf.asin`, `tf.asinh`, `tf.atan`, `tf.atan2`, `tf.atanh`, `tf.cos`, `tf.cosh`, `tf.equal`, `tf.exp`, `tf.floor`, `tf.greater`, `tf.greater_equal`, `tf.less`, `tf.less_equal`, `tf.log`, `tf.logp1`, `tf.logical_and`, `tf.logical_not`, `tf.logical_or`, `tf.maximum`, `tf.minimum`, `tf.not_equal`, `tf.sin`, `tf.sinh`, `tf.tan`\r\n  * Deprecate `tf.data.Dataset.shard`.\r\n  * Deprecate `saved_model.loader.load` which is replaced by `saved_model.load` and `saved_model.main_op`, which will be replaced by `saved_model.main_op` in V2.\r\n  * Deprecate tf.QUANTIZED_DTYPES. The official new symbol is tf.dtypes.QUANTIZED_DTYPES.\r\n  * Update sklearn imports for deprecated packages.\r\n  * Deprecate `Variable.count_up_to` and `tf.count_up_to` in favor of `Dataset.range`.\r\n  * Export `confusion_matrix` op as `tf.math.confusion_matrix` instead of `tf.train.confusion_matrix`.\r\n  * Add `tf.dtypes.` endpoint for every constant in dtypes.py; moving endpoints in versions.py to corresponding endpoints in `tf.sysconfig.` and `tf.version.`; moving all constants under `tf.saved_model` submodules to `tf.saved_model` module. New endpoints are added in V1 and V2 but existing endpoint removals are only applied in V2.\r\n  * Deprecates behavior where device assignment overrides collocation constraints inside a collocation context manager.\r\n* Keras & Python API\r\n  * Add to Keras functionality analogous to `tf.register_tensor_conversion_function`.\r\n  * Subclassed Keras models can now be saved through `tf.contrib.saved_model.save_keras_model`.\r\n  * `LinearOperator.matmul` now returns a new `LinearOperator`.\r\n* New ops and improved op functionality\r\n  * Add a Nearest Neighbor Resize op.\r\n  * Add an `ignore_unknown` argument to `parse_values` which suppresses ValueError for unknown hyperparameter types. Such * Add `tf.linalg.matvec` convenience function.\r\n  * `tf.einsum()`raises `ValueError` for unsupported equations like `\"ii->\"`.\r\n  * Add DCT-I and IDCT-I in `tf.signal.dct` and `tf.signal.idct`.\r\n  * Add LU decomposition op.\r\n  * Add quantile loss to gradient boosted trees in estimator.\r\n  * Add `round_mode` to `QuantizeAndDequantizeV2` op to select rounding algorithm.\r\n  * Add `unicode_encode`, `unicode_decode`, `unicode_decode_with_offsets`, `unicode_split`, `unicode_split_with_offset`, and `unicode_transcode` ops. Amongst other things, this Op adds the ability to encode, decode, and transcode a variety of input text encoding formats into the main Unicode encodings (UTF-8, UTF-16-BE, UTF-32-BE)\r\n  * Add \"unit\" attribute to the substr op, which allows obtaining the substring of a string containing unicode characters.\r\n  * Broadcasting support for Ragged Tensors.\r\n  * `SpaceToDepth` supports uint8 data type.\r\n  * Support multi-label quantile regression in estimator.\r\n  * We now use \"div\" as the default partition_strategy in `tf.nn.safe_embedding_lookup_sparse`, `tf.nn.sampled_softmax` and `tf.nn.nce_loss`.\r\n  hyperparameter are ignored.\r\n* Performance\r\n  * Improve performance of GPU cumsum/cumprod by up to 300x.\r\n  * Added support for weight decay in most TPU embedding optimizers, including AdamW and MomentumW.\r\n* TensorFlow 2.0 Development\r\n  * Add a command line tool to convert to TF2.0, tf_upgrade_v2\r\n  * Merge `tf.spectral` into `tf.signal` for TensorFlow 2.0.\r\n  * Change the default recurrent activation function for LSTM from 'hard_sigmoid' to 'sigmoid' in 2.0. Historically recurrent activation is 'hard_sigmoid' since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default LSTM will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with LSTM(recurrent_activation='hard_sigmoid') to fallback to 1.x behavior.\r\n* TensorFlow Lite\r\n  * Move from `tensorflow/contrib/lite` to `tensorflow/lite`.\r\n  * Add experimental Java API for injecting TensorFlow Lite delegates\r\n  * Add support for strings in TensorFlow Lite Java API.\r\n* `tf.contrib`:\r\n  * Add Apache Ignite Filesystem plugin to support accessing Apache IGFS.\r\n  * Dropout now takes `rate` argument, `keep_prob` is deprecated.\r\n  * Estimator occurrences references `tf.contrib.estimator` were changed to `tf.estimator`:\r\n    * `tf.contrib.estimator.BaselineEstimator` with `tf.estimator.BaselineEstimator`\r\n    * `tf.contrib.estimator.DNNLinearCombinedEstimator` with `tf.estimator.DNNLinearCombinedEstimator`\r\n    * `tf.contrib.estimator.DNNEstimator` with `tf.estimator.DNNEstimator`\r\n    * `tf.contrib.estimator.LinearEstimator` with `tf.estimator.LinearEstimator`\r\n    * `tf.contrib.estimator.InMemoryEvaluatorHook` and tf.estimator.experimental.InMemoryEvaluatorHook`.\r\n    * `tf.contrib.estimator.make_stop_at_checkpoint_step_hook` with `tf.estimator.experimental.make_stop_at_checkpoint_step_hook`.\r\n  * Expose `tf.distribute.Strategy as the new name for tf.contrib.distribute.DistributionStrategy.\r\n  * Migrate linear optimizer from contrib to core.\r\n  * Move `tf.contrib.signal` to `tf.signal` (preserving aliases in tf.contrib.signal).\r\n  * Users of `tf.contrib.estimator.export_all_saved_models` and related should switch to `tf.estimator.Estimator.experimental_export_all_saved_models`.\r\n* tf.data:\r\n  * Add `tf.data.experimental.StatsOptions()`, to configure options to collect statistics from `tf.data.Dataset` pipeline using `StatsAggregator`. Add nested option, `experimental_stats` (which takes a `tf.data.experimen tal.StatsOptions` object), to `tf.data.Options`. Deprecates `tf.data.experimental.set_stats_agregator`.\r\n  * Performance optimizations:\r\n    * Add `tf.data.experimental.OptimizationOptions()`, to configure options to enable `tf.data` performance optimizations. Add nested option, `experimental_optimization` (which takes a `tf.data.experimental.OptimizationOptions` object), to `tf.data.Options`. Remove performance optimization options from `tf.data.Options`, and add them under `tf.data.experimental.OptimizationOptions` instead.\r\n    * Enable `map_and_batch_fusion` and `noop_elimination` optimizations by default. They can be disabled by configuring `tf.data.experimental.OptimizationOptions` to set `map_and_batch = False` or `noop_elimination = False` respectively. To disable all default optimizations, set `apply_default_optimizations = False`.\r\n    * Support parallel map in `map_and_filter_fusion`.\r\n    * Disable static optimizations for input pipelines that use non-resource `tf.Variable`s.\r\n  * Add NUMA-aware MapAndBatch dataset.\r\n  * Deprecate `tf.data.Dataset.make_one_shot_iterator()` in V1, removed it from V2, and added tf.compat.v1.data.make_one_shot_iterator()`.\r\n  * Deprecate `tf.data.Dataset.make_initializable_iterator()` in V1, removed it from V2, and added `tf.compat.v1.data.make_initializable_iterator()`.\r\n  * Enable nested dataset support in core `tf.data` transformations.\r\n  * For `tf.data.Dataset` implementers: Added `tf.data.Dataset._element_structured property` to replace `Dataset.output_{types,shapes,classes}`.\r\n  * Make `num_parallel_calls` of `tf.data.Dataset.interleave` and `tf.data.Dataset.map` work in Eager mode.\r\n* Toolchains\r\n  * Fixed OpenSSL compatibility by avoiding `EVP_MD_CTX_destroy`.\r\n  * Added bounds checking to printing deprecation warnings.\r\n  * Upgraded CUDA dependency to 10.0\r\n  * To build with Android NDK r14b, add \"#include <linux/compiler.h>\" to android-ndk-r14b/platforms/android-14/arch-*/usr/include/linux/futex.h\r\n  * Removed `:android_tensorflow_lib_selective_registration*` targets, use `:android_tensorflow_lib_lite*` targets instead.\r\n* XLA\r\n  * Move `RoundToEven` function to xla/client/lib/math.h.\r\n  * A new environment variable `TF_XLA_DEBUG_OPTIONS_PASSTHROUGH` set to \"1\" or \"true\" allows the debug options passed within an XRTCompile op to be passed directly to the XLA compilation backend. If such variable is not set (service side), only a restricted set will be passed through.\r\n  * Allow the XRTCompile op to return the ProgramShape resulted form the XLA compilation as a second return argument.\r\n  * XLA HLO graphs can now be rendered as SVG/HTML.\r\n* Estimator\r\n  * Replace all occurences of `tf.contrib.estimator.BaselineEstimator` with `tf.estimator.BaselineEstimator`\r\n  * Replace all occurences of `tf.contrib.estimator.DNNLinearCombinedEstimator` with `tf.estimator.DNNLinearCombinedEstimator`\r\n  * Replace all occurrences of `tf.contrib.estimator.DNNEstimator` with `tf.estimator.DNNEstimator`\r\n  * Replace all occurrences of `tf.contrib.estimator.LinearEstimator` with `tf.estimator.LinearEstimator`\r\n  * Users of `tf.contrib.estimator.export_all_saved_models` and related should switch to `tf.estimator.Estimator.experimental_export_all_saved_models`.\r\n  * Update `regression_head` to the new Head API for Canned Estimator V2.\r\n  * Switch `multi_class_head` to Head API for Canned Estimator V2.\r\n  * Replace all occurences of `tf.contrib.estimator.InMemoryEvaluatorHook` and `tf.contrib.estimator.make_stop_at_checkpoint_step_hook` with `tf.estimator.experimental.InMemoryEvaluatorHook` and `tf.estimator.experimental.make_stop_at_checkpoint_step_hook`\r\n  * Migrate linear optimizer from contrib to core.\r\n\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\nAbhinav Upadhyay, Ag Ramesh, akikaaa, Alexis Louis, Anders Huss, Andreas Madsen, Andrew Banchich, Andy Craze, Anton Dmitriev, Artem Malykh, Avijit-Nervana, Balint Cristian, Benjamin Tan Wei Hao, Bhavani Subramanian, Brendan Finan, Brian Nemsick, Bryan Cutler, By Shen, Cao Zongyan, Castiel, Chris Antaki, Christian Goll, Cibifang, Clayne Robison, Codrut Grosu, Cong Xu, Dalmo Cirne, Daniel Hunter, Dougal J. Sutherland, Edvard Fagerholm, EFanZh, Erik Smistad, Evgeniy Polyakov, Feiyang Chen, franklin5, Fred Reiss, Gautam, gehring, Geoffrey Irving, George Sterpu, Gitea, Grzegorz George Pawelczak, Guozhong Zhuang, himkt, Hoeseong Kim, Huan Li (\u674e\u5353\u6853), HuiyangFei, hyunyoung, Isaac Burbank, jackonan, Jacky Ko, Jason Furmanek, Jason Zaman, Javier Luraschi, Jiang,Zhoulong, joaak, John Lin, Jonathan Wyatt Hoech, josephyearsley, Josh Gordon, Julian Niedermeier, Karl Lessard, Keno Fischer, lanhin, Leon Graser, leondgarse, Li, Guizi, Li, Yiqiang, lxl910915, Mahmoud Abuzaina, manhyuk, Marcela Morales Quispe, margaretmz, Matt Conley, Max Pumperla, mbhuiyan, mdfaijul, Meng, Peng, Michael, Michael Gielda, mrTsjolder, Muhammad Wildan, neargye, Nehal J Wani, NEWPLAN, Niranjan Hasabnis, Nutti, olicht, Pan Daoxin, Pedro Monreal, Peng Yu, pillarpond, Pooya Davoodi, qiezi, Rholais Lii, Richard Yu, Rin Arakaki, Roger Iyengar, sahilbadyal, Sami Kama, Sandip Giri, Scott Leishman, Serge Panev, Seunghoon Park, Shafi Dayatar, shengfuintel, Shimin Guo, Siju, silent567, Stefan Dyulgerov, steven, Tao Wei, Thor Johnsen, Tingbo Lu, tomguluson92, Tongxuan Liu, Trevor Morris, Ubuntu, Vadim Borisov, vanderliang, wangsiyu, Wen Yun, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, Xiaoming (Jason) Cui, Yan Facai (\u989c\u53d1\u624d), Yanbo Liang, Yaniv Blumenfeld, Yash Gaurkar, Yicheng Fan, Yong Tang, Yongjoon Lee, Yuan (Terry) Tang, Yuxin Wu, zldrobit", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.13.1", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.13.1", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.13.1", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/15735942"}, {"tag_name": "v1.13.0-rc2", "name": "TensorFlow 1.13.0-rc2", "author_name": "aselle", "body": "# Release 1.13.0 RC2\r\n\r\n## Major Features and Improvements\r\n\r\n* TensorFlow Lite has moved from contrib to core. This means that Python modules are under `tf.lite` and source code is now under `tensorflow/lite` rather than `tensorflow/contrib/lite`.\r\n* TensorFlow GPU binaries are now built against CUDA 10 and TensorRT 5.0.\r\n* Support for Python3.7 on all operating systems.\r\n* Moved NCCL to core.\r\n\r\n## Behavioral changes\r\n\r\n* Disallow conversion of python floating types to uint32/64 (matching behavior of other integer types) in `tf.constant`.\r\n* Make the `gain` argument of convolutional orthogonal initializers (`convolutional_delta_orthogonal`, `convolutional_orthogonal_1D`, `convolutional_orthogonal_2D`, `convolutional_orthogonal_3D`) have consistent behavior with the `tf.initializers.orthogonal` initializer, i.e. scale the output l2-norm by `gain` and NOT by `sqrt(gain)`. (Note that these functions are currently in `tf.contrib` which is not guaranteed backward compatible).\r\n\r\n## Bug Fixes and Other Changes\r\n\r\n* Documentation\r\n  * Update the doc with the details about the rounding mode used in quantize_and_dequantize_v2.\r\n  * Clarify that tensorflow::port::InitMain() _should_ be called before using the TensorFlow library.  Programs failing to do this are not portable to all platforms.\r\n* Deprecations and Symbol renames.\r\n   * Removing deprecations for the following endpoints: `tf.acos`, `tf.acosh`, `tf.add`, `tf.as_string`, `tf.asin`, `tf.asinh`, `tf.atan`, `tf.atan2`, `tf.atanh`, `tf.cos`, `tf.cosh`, `tf.equal`, `tf.exp`, `tf.floor`, `tf.greater`, `tf.greater_equal`, `tf.less`, `tf.less_equal`, `tf.log`, `tf.logp1`, `tf.logical_and`, `tf.logical_not`, `tf.logical_or`, `tf.maximum`, `tf.minimum`, `tf.not_equal`, `tf.sin`, `tf.sinh`, `tf.tan`\r\n  * Deprecate `tf.data.Dataset.shard`.\r\n  * Deprecate `saved_model.loader.load` which is replaced by `saved_model.load` and `saved_model.main_op`, which will be replaced by `saved_model.main_op` in V2.\r\n  * Deprecate tf.QUANTIZED_DTYPES. The official new symbol is tf.dtypes.QUANTIZED_DTYPES.\r\n  * Update sklearn imports for deprecated packages.\r\n  * Deprecate `Variable.count_up_to` and `tf.count_up_to` in favor of `Dataset.range`.\r\n  * Export `confusion_matrix` op as `tf.math.confusion_matrix` instead of `tf.train.confusion_matrix`.\r\n  * Add `tf.dtypes.` endpoint for every constant in dtypes.py; moving endpoints in versions.py to corresponding endpoints in `tf.sysconfig.` and `tf.version.`; moving all constants under `tf.saved_model` submodules to `tf.saved_model` module. New endpoints are added in V1 and V2 but existing endpoint removals are only applied in V2.\r\n  * Deprecates behavior where device assignment overrides collocation constraints inside a collocation context manager.\r\n* Keras & Python API\r\n  * Add to Keras functionality analogous to `tf.register_tensor_conversion_function`.\r\n  * Subclassed Keras models can now be saved through `tf.contrib.saved_model.save_keras_model`.\r\n  * `LinearOperator.matmul` now returns a new `LinearOperator`.\r\n* New ops and improved op functionality\r\n  * Add a Nearest Neighbor Resize op.\r\n  * Add an `ignore_unknown` argument to `parse_values` which suppresses ValueError for unknown hyperparameter types. Such * Add `tf.linalg.matvec` convenience function.\r\n  * `tf.einsum()`raises `ValueError` for unsupported equations like `\"ii->\"`.\r\n  * Add DCT-I and IDCT-I in `tf.signal.dct` and `tf.signal.idct`.\r\n  * Add LU decomposition op.\r\n  * Add quantile loss to gradient boosted trees in estimator.\r\n  * Add `round_mode` to `QuantizeAndDequantizeV2` op to select rounding algorithm.\r\n  * Add `unicode_encode`, `unicode_decode`, `unicode_decode_with_offsets`, `unicode_split`, `unicode_split_with_offset`, and `unicode_transcode` ops. Amongst other things, this Op adds the ability to encode, decode, and transcode a variety of input text encoding formats into the main Unicode encodings (UTF-8, UTF-16-BE, UTF-32-BE)\r\n  * Add \"unit\" attribute to the substr op, which allows obtaining the substring of a string containing unicode characters.\r\n  * Broadcasting support for Ragged Tensors.\r\n  * `SpaceToDepth` supports uint8 data type.\r\n  * Support multi-label quantile regression in estimator.\r\n  * We now use \"div\" as the default partition_strategy in `tf.nn.safe_embedding_lookup_sparse`, `tf.nn.sampled_softmax` and `tf.nn.nce_loss`.\r\n  hyperparameter are ignored.\r\n* Performance\r\n  * Improve performance of GPU cumsum/cumprod by up to 300x.\r\n  * Added support for weight decay in most TPU embedding optimizers, including AdamW and MomentumW.\r\n* TensorFlow 2.0 Development\r\n  * Add a command line tool to convert to TF2.0, tf_upgrade_v2\r\n  * Merge `tf.spectral` into `tf.signal` for TensorFlow 2.0.\r\n  * Change the default recurrent activation function for LSTM from 'hard_sigmoid' to 'sigmoid' in 2.0. Historically recurrent activation is 'hard_sigmoid' since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default LSTM will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with LSTM(recurrent_activation='hard_sigmoid') to fallback to 1.x behavior.\r\n* TensorFlow Lite\r\n  * Move from `tensorflow/contrib/lite` to `tensorflow/lite`.\r\n  * Add experimental Java API for injecting TensorFlow Lite delegates\r\n  * Add support for strings in TensorFlow Lite Java API.\r\n* `tf.contrib`:\r\n  * Add Apache Ignite Filesystem plugin to support accessing Apache IGFS.\r\n  * Dropout now takes `rate` argument, `keep_prob` is deprecated.\r\n  * Estimator occurrences references `tf.contrib.estimator` were changed to `tf.estimator`:\r\n    * `tf.contrib.estimator.BaselineEstimator` with `tf.estimator.BaselineEstimator`\r\n    * `tf.contrib.estimator.DNNLinearCombinedEstimator` with `tf.estimator.DNNLinearCombinedEstimator`\r\n    * `tf.contrib.estimator.DNNEstimator` with `tf.estimator.DNNEstimator`\r\n    * `tf.contrib.estimator.LinearEstimator` with `tf.estimator.LinearEstimator`\r\n    * `tf.contrib.estimator.InMemoryEvaluatorHook` and tf.estimator.experimental.InMemoryEvaluatorHook`.\r\n    * `tf.contrib.estimator.make_stop_at_checkpoint_step_hook` with `tf.estimator.experimental.make_stop_at_checkpoint_step_hook`.\r\n  * Expose `tf.distribute.Strategy as the new name for tf.contrib.distribute.DistributionStrategy.\r\n  * Migrate linear optimizer from contrib to core.\r\n  * Move `tf.contrib.signal` to `tf.signal` (preserving aliases in tf.contrib.signal).\r\n  * Users of `tf.contrib.estimator.export_all_saved_models` and related should switch to `tf.estimator.Estimator.experimental_export_all_saved_models`.\r\n* tf.data:\r\n  * Add `tf.data.experimental.StatsOptions()`, to configure options to collect statistics from `tf.data.Dataset` pipeline using `StatsAggregator`. Add nested option, `experimental_stats` (which takes a `tf.data.experimen tal.StatsOptions` object), to `tf.data.Options`. Deprecates `tf.data.experimental.set_stats_agregator`.\r\n  * Performance optimizations:\r\n    * Add `tf.data.experimental.OptimizationOptions()`, to configure options to enable `tf.data` performance optimizations. Add nested option, `experimental_optimization` (which takes a `tf.data.experimental.OptimizationOptions` object), to `tf.data.Options`. Remove performance optimization options from `tf.data.Options`, and add them under `tf.data.experimental.OptimizationOptions` instead.\r\n    * Enable `map_and_batch_fusion` and `noop_elimination` optimizations by default. They can be disabled by configuring `tf.data.experimental.OptimizationOptions` to set `map_and_batch = False` or `noop_elimination = False` respectively. To disable all default optimizations, set `apply_default_optimizations = False`.\r\n    * Support parallel map in `map_and_filter_fusion`.\r\n    * Disable static optimizations for input pipelines that use non-resource `tf.Variable`s.\r\n  * Add NUMA-aware MapAndBatch dataset.\r\n  * Deprecate `tf.data.Dataset.make_one_shot_iterator()` in V1, removed it from V2, and added tf.compat.v1.data.make_one_shot_iterator()`.\r\n  * Deprecate `tf.data.Dataset.make_initializable_iterator()` in V1, removed it from V2, and added `tf.compat.v1.data.make_initializable_iterator()`.\r\n  * Enable nested dataset support in core `tf.data` transformations.\r\n  * For `tf.data.Dataset` implementers: Added `tf.data.Dataset._element_structured property` to replace `Dataset.output_{types,shapes,classes}`.\r\n  * Make `num_parallel_calls` of `tf.data.Dataset.interleave` and `tf.data.Dataset.map` work in Eager mode.\r\n* Toolchains\r\n  * Fixed OpenSSL compatibility by avoiding `EVP_MD_CTX_destroy`.\r\n  * Added bounds checking to printing deprecation warnings.\r\n  * Upgraded CUDA dependency to 10.0\r\n  * To build with Android NDK r14b, add \"#include <linux/compiler.h>\" to android-ndk-r14b/platforms/android-14/arch-*/usr/include/linux/futex.h\r\n  * Removed `:android_tensorflow_lib_selective_registration*` targets, use `:android_tensorflow_lib_lite*` targets instead.\r\n* XLA\r\n  * Move `RoundToEven` function to xla/client/lib/math.h.\r\n  * A new environment variable `TF_XLA_DEBUG_OPTIONS_PASSTHROUGH` set to \"1\" or \"true\" allows the debug options passed within an XRTCompile op to be passed directly to the XLA compilation backend. If such variable is not set (service side), only a restricted set will be passed through.\r\n  * Allow the XRTCompile op to return the ProgramShape resulted form the XLA compilation as a second return argument.\r\n  * XLA HLO graphs can now be rendered as SVG/HTML.\r\n* Estimator\r\n  * Replace all occurences of `tf.contrib.estimator.BaselineEstimator` with `tf.estimator.BaselineEstimator`\r\n  * Replace all occurences of `tf.contrib.estimator.DNNLinearCombinedEstimator` with `tf.estimator.DNNLinearCombinedEstimator`\r\n  * Replace all occurrences of `tf.contrib.estimator.DNNEstimator` with `tf.estimator.DNNEstimator`\r\n  * Replace all occurrences of `tf.contrib.estimator.LinearEstimator` with `tf.estimator.LinearEstimator`\r\n  * Users of `tf.contrib.estimator.export_all_saved_models` and related should switch to `tf.estimator.Estimator.experimental_export_all_saved_models`.\r\n  * Update `regression_head` to the new Head API for Canned Estimator V2.\r\n  * Switch `multi_class_head` to Head API for Canned Estimator V2.\r\n  * Replace all occurences of `tf.contrib.estimator.InMemoryEvaluatorHook` and `tf.contrib.estimator.make_stop_at_checkpoint_step_hook` with `tf.estimator.experimental.InMemoryEvaluatorHook` and `tf.estimator.experimental.make_stop_at_checkpoint_step_hook`\r\n  * Migrate linear optimizer from contrib to core.\r\n\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\nAbhinav Upadhyay, Ag Ramesh, akikaaa, Alexis Louis, Anders Huss, Andreas Madsen, Andrew Banchich, Andy Craze, Anton Dmitriev, Artem Malykh, Avijit-Nervana, Balint Cristian, Benjamin Tan Wei Hao, Bhavani Subramanian, Brendan Finan, Brian Nemsick, Bryan Cutler, By Shen, Cao Zongyan, Castiel, Chris Antaki, Christian Goll, Cibifang, Clayne Robison, Codrut Grosu, Cong Xu, Dalmo Cirne, Daniel Hunter, Dougal J. Sutherland, Edvard Fagerholm, EFanZh, Erik Smistad, Evgeniy Polyakov, Feiyang Chen, franklin5, Fred Reiss, Gautam, gehring, Geoffrey Irving, George Sterpu, Gitea, Grzegorz George Pawelczak, Guozhong Zhuang, himkt, Hoeseong Kim, Huan Li (\u674e\u5353\u6853), HuiyangFei, hyunyoung, Isaac Burbank, jackonan, Jacky Ko, Jason Furmanek, Jason Zaman, Javier Luraschi, Jiang,Zhoulong, joaak, John Lin, Jonathan Wyatt Hoech, josephyearsley, Josh Gordon, Julian Niedermeier, Karl Lessard, Keno Fischer, lanhin, Leon Graser, leondgarse, Li, Guizi, Li, Yiqiang, lxl910915, Mahmoud Abuzaina, manhyuk, Marcela Morales Quispe, margaretmz, Matt Conley, Max Pumperla, mbhuiyan, mdfaijul, Meng, Peng, Michael, Michael Gielda, mrTsjolder, Muhammad Wildan, neargye, Nehal J Wani, NEWPLAN, Niranjan Hasabnis, Nutti, olicht, Pan Daoxin, Pedro Monreal, Peng Yu, pillarpond, Pooya Davoodi, qiezi, Rholais Lii, Richard Yu, Rin Arakaki, Roger Iyengar, sahilbadyal, Sami Kama, Sandip Giri, Scott Leishman, Serge Panev, Seunghoon Park, Shafi Dayatar, shengfuintel, Shimin Guo, Siju, silent567, Stefan Dyulgerov, steven, Tao Wei, Thor Johnsen, Tingbo Lu, tomguluson92, Tongxuan Liu, Trevor Morris, Ubuntu, Vadim Borisov, vanderliang, wangsiyu, Wen Yun, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, Xiaoming (Jason) Cui, Yan Facai (\u989c\u53d1\u624d), Yanbo Liang, Yaniv Blumenfeld, Yash Gaurkar, Yicheng Fan, Yong Tang, Yongjoon Lee, Yuan (Terry) Tang, Yuxin Wu, zldrobit\r\n", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.13.0-rc2", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.13.0-rc2", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.13.0-rc2", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/15592422"}, {"tag_name": "v1.13.0-rc1", "name": "TensorFlow 1.13.0-rc1", "author_name": "aselle", "body": "# Release 1.13.0\r\n\r\n## Major Features and Improvements\r\n\r\n* TensorFlow Lite has moved from contrib to core. This means that Python modules are under `tf.lite` and source code is now under `tensorflow/lite` rather than `tensorflow/contrib/lite`.\r\n* TensorFlow GPU binaries are now built against CUDA 10 and TensorRT 5.0.\r\n* Moved NCCL to core.\r\n\r\n## Behavioral changes\r\n\r\n* Disallow conversion of python floating types to uint32/64 (matching behavior of other integer types) in `tf.constant`.\r\n* Make the `gain` argument of convolutional orthogonal initializers (`convolutional_delta_orthogonal`, `convolutional_orthogonal_1D`, `convolutional_orthogonal_2D`, `convolutional_orthogonal_3D`) have consistent behavior with the `tf.initializers.orthogonal` initializer, i.e. scale the output l2-norm by `gain` and NOT by `sqrt(gain)`. (Note that these functions are currently in `tf.contrib` which is not guaranteed backward compatible).\r\n\r\n## Bug Fixes and Other Changes\r\n\r\n* Documentation\r\n  * Update the doc with the details about the rounding mode used in quantize_and_dequantize_v2.\r\n  * Clarify that tensorflow::port::InitMain() _should_ be called before using the TensorFlow library.  Programs failing to do this are not portable to all platforms.\r\n* Deprecations and Symbol renames.\r\n   * Removing deprecations for the following endpoints: `tf.acos`, `tf.acosh`, `tf.add`, `tf.as_string`, `tf.asin`, `tf.asinh`, `tf.atan`, `tf.atan2`, `tf.atanh`, `tf.cos`, `tf.cosh`, `tf.equal`, `tf.exp`, `tf.floor`, `tf.greater`, `tf.greater_equal`, `tf.less`, `tf.less_equal`, `tf.log`, `tf.logp1`, `tf.logical_and`, `tf.logical_not`, `tf.logical_or`, `tf.maximum`, `tf.minimum`, `tf.not_equal`, `tf.sin`, `tf.sinh`, `tf.tan`\r\n  * Deprecate `tf.data.Dataset.shard`.\r\n  * Deprecate `saved_model.loader.load` which is replaced by `saved_model.load` and `saved_model.main_op`, which will be replaced by `saved_model.main_op` in V2.\r\n  * Deprecate tf.QUANTIZED_DTYPES. The official new symbol is tf.dtypes.QUANTIZED_DTYPES.\r\n  * Update sklearn imports for deprecated packages.\r\n  * Deprecate `Variable.count_up_to` and `tf.count_up_to` in favor of `Dataset.range`.\r\n  * Export `confusion_matrix` op as `tf.math.confusion_matrix` instead of `tf.train.confusion_matrix`.\r\n  * Add `tf.dtypes.` endpoint for every constant in dtypes.py; moving endpoints in versions.py to corresponding endpoints in `tf.sysconfig.` and `tf.version.`; moving all constants under `tf.saved_model` submodules to `tf.saved_model` module. New endpoints are added in V1 and V2 but existing endpoint removals are only applied in V2.\r\n  * Deprecates behavior where device assignment overrides collocation constraints inside a collocation context manager.\r\n* Keras & Python API\r\n  * Add to Keras functionality analogous to `tf.register_tensor_conversion_function`.\r\n  * Subclassed Keras models can now be saved through `tf.contrib.saved_model.save_keras_model`.\r\n  * `LinearOperator.matmul` now returns a new `LinearOperator`.\r\n* New ops and improved op functionality\r\n  * Add a Nearest Neighbor Resize op.\r\n  * Add an `ignore_unknown` argument to `parse_values` which suppresses ValueError for unknown hyperparameter types. Such * Add `tf.linalg.matvec` convenience function.\r\n  * `tf.einsum()`raises `ValueError` for unsupported equations like `\"ii->\"`.\r\n  * Add DCT-I and IDCT-I in `tf.signal.dct` and `tf.signal.idct`.\r\n  * Add LU decomposition op.\r\n  * Add quantile loss to gradient boosted trees in estimator.\r\n  * Add `round_mode` to `QuantizeAndDequantizeV2` op to select rounding algorithm.\r\n  * Add `unicode_encode`, `unicode_decode`, `unicode_decode_with_offsets`, `unicode_split`, `unicode_split_with_offset`, and `unicode_transcode` ops. Amongst other things, this Op adds the ability to encode, decode, and transcode a variety of input text encoding formats into the main Unicode encodings (UTF-8, UTF-16-BE, UTF-32-BE)\r\n  * Add \"unit\" attribute to the substr op, which allows obtaining the substring of a string containing unicode characters.\r\n  * Broadcasting support for Ragged Tensors.\r\n  * `SpaceToDepth` supports uint8 data type.\r\n  * Support multi-label quantile regression in estimator.\r\n  * We now use \"div\" as the default partition_strategy in `tf.nn.safe_embedding_lookup_sparse`, `tf.nn.sampled_softmax` and `tf.nn.nce_loss`.\r\n  hyperparameter are ignored.\r\n* Performance\r\n  * Improve performance of GPU cumsum/cumprod by up to 300x.\r\n  * Added support for weight decay in most TPU embedding optimizers, including AdamW and MomentumW.\r\n* TensorFlow 2.0 Development\r\n  * Add a command line tool to convert to TF2.0, tf_upgrade_v2\r\n  * Merge `tf.spectral` into `tf.signal` for TensorFlow 2.0.\r\n  * Change the default recurrent activation function for LSTM from 'hard_sigmoid' to 'sigmoid' in 2.0. Historically recurrent activation is 'hard_sigmoid' since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default LSTM will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with LSTM(recurrent_activation='hard_sigmoid') to fallback to 1.x behavior.\r\n* TensorFlow Lite\r\n  * Move from `tensorflow/contrib/lite` to `tensorflow/lite`.\r\n  * Add experimental Java API for injecting TensorFlow Lite delegates\r\n  * Add support for strings in TensorFlow Lite Java API.\r\n* `tf.contrib`:\r\n  * Add Apache Ignite Filesystem plugin to support accessing Apache IGFS.\r\n  * Dropout now takes `rate` argument, `keep_prob` is deprecated.\r\n  * Estimator occurrences references `tf.contrib.estimator` were changed to `tf.estimator`:\r\n    * `tf.contrib.estimator.BaselineEstimator` with `tf.estimator.BaselineEstimator`\r\n    * `tf.contrib.estimator.DNNLinearCombinedEstimator` with `tf.estimator.DNNLinearCombinedEstimator`\r\n    * `tf.contrib.estimator.DNNEstimator` with `tf.estimator.DNNEstimator`\r\n    * `tf.contrib.estimator.LinearEstimator` with `tf.estimator.LinearEstimator`\r\n    * `tf.contrib.estimator.InMemoryEvaluatorHook` and tf.estimator.experimental.InMemoryEvaluatorHook`.\r\n    * `tf.contrib.estimator.make_stop_at_checkpoint_step_hook` with `tf.estimator.experimental.make_stop_at_checkpoint_step_hook`.\r\n  * Expose `tf.distribute.Strategy as the new name for tf.contrib.distribute.DistributionStrategy.\r\n  * Migrate linear optimizer from contrib to core.\r\n  * Move `tf.contrib.signal` to `tf.signal` (preserving aliases in tf.contrib.signal).\r\n  * Users of `tf.contrib.estimator.export_all_saved_models` and related should switch to `tf.estimator.Estimator.experimental_export_all_saved_models`.\r\n* tf.data:\r\n  * Add `tf.data.experimental.StatsOptions()`, to configure options to collect statistics from `tf.data.Dataset` pipeline using `StatsAggregator`. Add nested option, `experimental_stats` (which takes a `tf.data.experimen tal.StatsOptions` object), to `tf.data.Options`. Deprecates `tf.data.experimental.set_stats_agregator`.\r\n  * Performance optimizations:\r\n    * Add `tf.data.experimental.OptimizationOptions()`, to configure options to enable `tf.data` performance optimizations. Add nested option, `experimental_optimization` (which takes a `tf.data.experimental.OptimizationOptions` object), to `tf.data.Options`. Remove performance optimization options from `tf.data.Options`, and add them under `tf.data.experimental.OptimizationOptions` instead.\r\n    * Enable `map_and_batch_fusion` and `noop_elimination` optimizations by default. They can be disabled by configuring `tf.data.experimental.OptimizationOptions` to set `map_and_batch = False` or `noop_elimination = False` respectively. To disable all default optimizations, set `apply_default_optimizations = False`.\r\n    * Support parallel map in `map_and_filter_fusion`.\r\n    * Disable static optimizations for input pipelines that use non-resource `tf.Variable`s.\r\n  * Add NUMA-aware MapAndBatch dataset.\r\n  * Deprecate `tf.data.Dataset.make_one_shot_iterator()` in V1, removed it from V2, and added tf.compat.v1.data.make_one_shot_iterator()`.\r\n  * Deprecate `tf.data.Dataset.make_initializable_iterator()` in V1, removed it from V2, and added `tf.compat.v1.data.make_initializable_iterator()`.\r\n  * Enable nested dataset support in core `tf.data` transformations.\r\n  * For `tf.data.Dataset` implementers: Added `tf.data.Dataset._element_structured property` to replace `Dataset.output_{types,shapes,classes}`.\r\n* Toolchains\r\n  * Fixed OpenSSL compatibility by avoiding `EVP_MD_CTX_destroy`.\r\n  * Added bounds checking to printing deprecation warnings.\r\n  * Upgraded CUDA dependency to 10.0\r\n  * To build with Android NDK r14b, add \"#include <linux/compiler.h>\" to android-ndk-r14b/platforms/android-14/arch-*/usr/include/linux/futex.h\r\n  * Removed `:android_tensorflow_lib_selective_registration*` targets, use `:android_tensorflow_lib_lite*` targets instead.\r\n* XLA\r\n  * Move `RoundToEven` function to xla/client/lib/math.h.\r\n  * A new environment variable `TF_XLA_DEBUG_OPTIONS_PASSTHROUGH` set to \"1\" or \"true\" allows the debug options passed within an XRTCompile op to be passed directly to the XLA compilation backend. If such variable is not set (service side), only a restricted set will be passed through.\r\n  * Allow the XRTCompile op to return the ProgramShape resulted form the XLA compilation as a second return argument.\r\n  * XLA HLO graphs can now be rendered as SVG/HTML.\r\n* Estimator\r\n  * Replace all occurences of `tf.contrib.estimator.BaselineEstimator` with `tf.estimator.BaselineEstimator`\r\n  * Replace all occurences of `tf.contrib.estimator.DNNLinearCombinedEstimator` with `tf.estimator.DNNLinearCombinedEstimator`\r\n  * Replace all occurrences of `tf.contrib.estimator.DNNEstimator` with `tf.estimator.DNNEstimator`\r\n  * Replace all occurrences of `tf.contrib.estimator.LinearEstimator` with `tf.estimator.LinearEstimator`\r\n  * Users of `tf.contrib.estimator.export_all_saved_models` and related should switch to `tf.estimator.Estimator.experimental_export_all_saved_models`.\r\n  * Update `regression_head` to the new Head API for Canned Estimator V2.\r\n  * Switch `multi_class_head` to Head API for Canned Estimator V2.\r\n  * Replace all occurences of `tf.contrib.estimator.InMemoryEvaluatorHook` and `tf.contrib.estimator.make_stop_at_checkpoint_step_hook` with `tf.estimator.experimental.InMemoryEvaluatorHook` and `tf.estimator.experimental.make_stop_at_checkpoint_step_hook`\r\n  * Migrate linear optimizer from contrib to core.\r\n\r\n\r\n## Thanks to our Contributors\r\n\r\nThis release contains contributions from many people at Google, as well as:\r\n\r\nAbhinav Upadhyay, Ag Ramesh, akikaaa, Alexis Louis, Anders Huss, Andreas Madsen, Andrew Banchich, Andy Craze, Anton Dmitriev, Artem Malykh, Avijit-Nervana, Balint Cristian, Benjamin Tan Wei Hao, Bhavani Subramanian, Brendan Finan, Brian Nemsick, Bryan Cutler, By Shen, Cao Zongyan, Castiel, Chris Antaki, Christian Goll, Cibifang, Clayne Robison, Codrut Grosu, Cong Xu, Dalmo Cirne, Daniel Hunter, Dougal J. Sutherland, Edvard Fagerholm, EFanZh, Erik Smistad, Evgeniy Polyakov, Feiyang Chen, franklin5, Fred Reiss, Gautam, gehring, Geoffrey Irving, George Sterpu, Gitea, Grzegorz George Pawelczak, Guozhong Zhuang, himkt, Hoeseong Kim, Huan Li (\u674e\u5353\u6853), HuiyangFei, hyunyoung, Isaac Burbank, jackonan, Jacky Ko, Jason Furmanek, Jason Zaman, Javier Luraschi, Jiang,Zhoulong, joaak, John Lin, Jonathan Wyatt Hoech, josephyearsley, Josh Gordon, Julian Niedermeier, Karl Lessard, Keno Fischer, lanhin, Leon Graser, leondgarse, Li, Guizi, Li, Yiqiang, lxl910915, Mahmoud Abuzaina, manhyuk, Marcela Morales Quispe, margaretmz, Matt Conley, Max Pumperla, mbhuiyan, mdfaijul, Meng, Peng, Michael, Michael Gielda, mrTsjolder, Muhammad Wildan, neargye, Nehal J Wani, NEWPLAN, Niranjan Hasabnis, Nutti, olicht, Pan Daoxin, Pedro Monreal, Peng Yu, pillarpond, Pooya Davoodi, qiezi, Rholais Lii, Richard Yu, Rin Arakaki, Roger Iyengar, sahilbadyal, Sami Kama, Sandip Giri, Scott Leishman, Serge Panev, Seunghoon Park, Shafi Dayatar, shengfuintel, Shimin Guo, Siju, silent567, Stefan Dyulgerov, steven, Tao Wei, Thor Johnsen, Tingbo Lu, tomguluson92, Tongxuan Liu, Trevor Morris, Ubuntu, Vadim Borisov, vanderliang, wangsiyu, Wen Yun, Wen-Heng (Jack) Chung, wenxizhu, William D. Irons, Xiaoming (Jason) Cui, Yan Facai (\u989c\u53d1\u624d), Yanbo Liang, Yaniv Blumenfeld, Yash Gaurkar, Yicheng Fan, Yong Tang, Yongjoon Lee, Yuan (Terry) Tang, Yuxin Wu, zldrobit\r\n", "tarball_url": "https://api.github.com/repos/tensorflow/tensorflow/tarball/v1.13.0-rc1", "zipball_url": "https://api.github.com/repos/tensorflow/tensorflow/zipball/v1.13.0-rc1", "html_url": "https://github.com/tensorflow/tensorflow/releases/tag/v1.13.0-rc1", "url": "https://api.github.com/repos/tensorflow/tensorflow/releases/15431762"}], "confidence": [1.0]}}